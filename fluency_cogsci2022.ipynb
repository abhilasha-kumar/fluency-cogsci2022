{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fluency-cogsci2022.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPZB3IDd6Z6Ru/SygXuXiLf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhilasha-kumar/fluency-cogsci2022/blob/main/fluency_cogsci2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mouse-mole-vole: The inconspicuous benefit of phonology during retrieval from semantic memory"
      ],
      "metadata": {
        "id": "YemaL37bd2-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing drive, GPU, and packages\n",
        "\n",
        "We begin by downloading the required data files that are stored in [this public Google Drive folder]( https://drive.google.com/drive/folders/1yHVyl1npj2QhQMPBvnqMhEME0JpDtKZ9?usp=sharing)."
      ],
      "metadata": {
        "id": "MwyFlkqfflFH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggNcnk34d2O4"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()  # must authenticate\n",
        "\n",
        "'''list all ids of files directly under folder folder_id'''\n",
        "def folder_list(folder_id):\n",
        "  from googleapiclient.discovery import build\n",
        "  gdrive = build('drive', 'v3').files()\n",
        "  res = gdrive.list(q=\"'%s' in parents\" % folder_id).execute()\n",
        "  return [f['id'] for f in res['files']]\n",
        "\n",
        "'''download all files from a gdrive folder to current directory'''\n",
        "def folder_download(folder_id):\n",
        "  for fid in folder_list(folder_id):\n",
        "    !gdown -q --id $fid\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the folder ID for our public Drive. downloading may take a moment.\n",
        "folder_download('1yHVyl1npj2QhQMPBvnqMhEME0JpDtKZ9')"
      ],
      "metadata": {
        "id": "SZpftSaN7-4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also use Colab's GPU for faster computations"
      ],
      "metadata": {
        "id": "vPsQK_mD8MYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "iYlJS26seDTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download required packages"
      ],
      "metadata": {
        "id": "a_1vjSOg8Pab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import itertools\n",
        "import scipy.spatial.distance\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "from scipy.special import softmax\n",
        "from sklearn.preprocessing import MinMaxScaler, normalize\n",
        "from numpy.linalg import matrix_power\n",
        "from functools import lru_cache\n",
        "import glob\n",
        "from scipy.special import expit\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from functools import lru_cache\n",
        "from itertools import product as iterprod\n",
        "import itertools\n",
        "from nltk.metrics import *\n",
        "\n"
      ],
      "metadata": {
        "id": "7f1RA_0MeKWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reading data\n",
        "\n",
        "We read in the raw data"
      ],
      "metadata": {
        "id": "VhJ2xFIYegNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  raw_data = pd.read_csv(\"raw_data.csv\")\n",
        "  display(raw_data)"
      ],
      "metadata": {
        "id": "faY0v_dpehPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# importing norms and embeddings\n",
        "\n",
        "Here we import the Troyer norms, as well as frequency, semantic and phonological similarity matrices. See code at the end of the notebook for computing the similarity matrices."
      ],
      "metadata": {
        "id": "mp-U9_MEN8Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  norms = pd.read_csv(\"troyernorms.csv\", encoding=\"unicode-escape\")\n",
        "  freq_matrix = pd.read_csv('animals_frequencies.csv', header = None)\n",
        "  labels = list(freq_matrix[0])\n",
        "  freq_matrix = np.array(freq_matrix[1])\n",
        "  sim_matrix = pd.read_csv('word2vec_sim_matrix.csv',header = None).values\n",
        "  phon_matrix = pd.read_csv('simlabels_phon_matrix.csv',header = None).values\n",
        "  print(f\"size of vocab is {len(labels)} animals\")"
      ],
      "metadata": {
        "id": "V0G5K2xAN1jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define foraging models"
      ],
      "metadata": {
        "id": "-SF41g03OzVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## history function"
      ],
      "metadata": {
        "id": "JjW_ijRfnkIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_history_variables(fluency_list, labels, sim_matrix, freq_matrix, phon_matrix = None):\n",
        "  '''\n",
        "  inputs:\n",
        "  (1) sim_matrix: semantic similarity matrix (NxN np.array)\n",
        "  (2) phon_matrix: phonological similarity matrix (NxN np.array)\n",
        "  (3) freq_matrix: frequencies array (Nx1 array)\n",
        "  (4) labels: the space of words (list of length N)\n",
        "  (5) fluency_list: items produced by a participant (list of size L)\n",
        "  \n",
        "  outputs:\n",
        "  (1) sim_list: semantic similarities between each item in fluency_list (list of size L)\n",
        "  (2) sim_history: semantic similarities of each word with all items in labels (list of L arrays of size N)\n",
        "  (1) phon_list: phonological similarities between each item in fluency_list (list of size L)\n",
        "  (2) phon_history: phonological similarities of each word with all items in labels (list of L arrays of size N)\n",
        "  (1) freq_list: frequencies of each item in fluency_list (list of size L)\n",
        "  (2) freq_history: frequencies of all items in labels repeated L items (list of L arrays of size N)\n",
        "  \n",
        "  \n",
        "  '''\n",
        "  phon_matrix[phon_matrix <= 0] = .0001\n",
        "  sim_matrix[sim_matrix <= 0] = .0001\n",
        "\n",
        "  freq_list = []\n",
        "  freq_history = []\n",
        "\n",
        "  sim_list = []\n",
        "  sim_history = []\n",
        "\n",
        "  phon_list = []\n",
        "  phon_history = []\n",
        "\n",
        "  for i in range(0,len(fluency_list)):\n",
        "    word = fluency_list[i]\n",
        "    currentwordindex = labels.index(word)\n",
        "\n",
        "    freq_list.append(freq_matrix[currentwordindex])\n",
        "    freq_history.append(freq_matrix)\n",
        "  \n",
        "    if i > 0: # get similarity between this word and preceding word\n",
        "      prevwordindex = labels.index(fluency_list[i-1])\n",
        "      sim_list.append(sim_matrix[prevwordindex, currentwordindex] )\n",
        "      sim_history.append(sim_matrix[prevwordindex,:])\n",
        "      if phon_matrix is not None:\n",
        "        phon_list.append(phon_matrix[prevwordindex, currentwordindex] )\n",
        "        phon_history.append(phon_matrix[prevwordindex,:])\n",
        "    else: # first word\n",
        "      sim_list.append(0.0001)\n",
        "      sim_history.append(sim_matrix[currentwordindex,:])\n",
        "      if phon_matrix is not None:\n",
        "        phon_list.append(0.0001)\n",
        "        phon_history.append(phon_matrix[currentwordindex,:])\n",
        "  \n",
        "  return sim_list, sim_history, freq_list, freq_history,phon_list, phon_history"
      ],
      "metadata": {
        "id": "-ZQkMOHB8D8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## forage class"
      ],
      "metadata": {
        "id": "M0nfMSLPnm-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class forage:\n",
        "    def model_static(beta, freql, freqh, siml, simh):\n",
        "        ## beta contains the optimization parameters for frequency (beta[0]) and semantic similarity (beta[1])\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0: # if first item then its probability is based on just frequency\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else: # if not first item then its probability is based on its similarity to prev item AND frequency\n",
        "            # P of item based on frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat) # negative Log likelihood of this item: this will be minimized eventually\n",
        "        return ct\n",
        "   \n",
        "    def model_dynamic_psyrev(beta, freql, freqh, siml, simh, switchvals):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif switchvals[k]==1: ## \"dip\" based on switch type\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "\n",
        "    def model_static_plocal(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0: \n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else: \n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) * pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2])* pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "      \n",
        "    def model_dynamic_pswitchonly(beta, freql, freqh, siml, simh, phonl, phonh,switchvals):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif switchvals[k]==1:\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2]) )\n",
        "            else:\n",
        "                numrat = pow(freql[k],beta[0])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "    \n",
        "\n",
        "    def model_dynamic_plocal(beta, freql, freqh, siml, simh, phonl, phonh,switchvals):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif switchvals[k]==1: \n",
        "                numrat = pow(freql[k],beta[0]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "                numrat = pow(freql[k],beta[0])*pow(phonl[k],beta[2])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(phonh[k],beta[2])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "            \n",
        "        return ct\n",
        "\n",
        "    def model_dynamic_pglobal(beta, freql, freqh, siml, simh, phonl, phonh,switchvals):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif switchvals[k]==1:\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2]) )\n",
        "            else:\n",
        "                numrat = pow(freql[k],beta[0])*pow(phonl[k],beta[2])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(phonh[k],beta[2])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct"
      ],
      "metadata": {
        "id": "oroA2Smv8RW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## switch function"
      ],
      "metadata": {
        "id": "i8unfZF6not5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_switch_lists(fluency_list, history_vars):\n",
        "  '''\n",
        "  takes in a fluency list and history variables and creates all possible switch/cluster designations\n",
        "  current implemented methods:\n",
        "  (1) simdrop\n",
        "  (2) troyer norms\n",
        " \n",
        "  output:\n",
        "  a list of lists corresponding to \n",
        "  (1) simdrop\n",
        "  (2) troyer\n",
        "  '''\n",
        "  simdrop = []\n",
        "  troyer = []\n",
        "  \n",
        "  semantic_similarity = history_vars[0]\n",
        "  phonological_similarity = history_vars[4]\n",
        "\n",
        "  for k in range(len(fluency_list)):\n",
        "    if (k > 0 and k < (len(fluency_list)-2)): \n",
        "      # simdrop\n",
        "      if (semantic_similarity[k+1] > semantic_similarity[k]) and (semantic_similarity[k-1] > semantic_similarity[k]):\n",
        "        simdrop.append(1)\n",
        "      else:\n",
        "        simdrop.append(0)\n",
        "      \n",
        "      # troyer\n",
        "      item1 = fluency_list[k]\n",
        "      item2 = fluency_list[k-1]\n",
        "      category1 = norms[norms['Animal'] == item1]['Category'].values.tolist()\n",
        "      category2 = norms[norms['Animal'] == item2]['Category'].values.tolist()\n",
        "      if len(list(set(category1) & set(category2)))== 0:\n",
        "          troyer.append(1)\n",
        "      else:\n",
        "          troyer.append(0)\n",
        "    else:\n",
        "        simdrop.append(2)\n",
        "        troyer.append(2)\n",
        "  \n",
        "  finalswitches = []\n",
        "  finalswitches.append(simdrop)\n",
        "  finalswitches.append(troyer)\n",
        "\n",
        "  return finalswitches "
      ],
      "metadata": {
        "id": "iZvHocT36WX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fmin - optimal betas"
      ],
      "metadata": {
        "id": "ElqQc0F_nr8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fmin_output_models (modelcode, history_vars, switchvals, random_vars):\n",
        "  '''\n",
        "  total number of models:\n",
        "  (1) psyrev static\n",
        "  (2) psyrev dynamic (with any switch type)\n",
        "  (3) phonology static (with any switch type)\n",
        "  (4) phonology global dynamic (with any switch type)\n",
        "  (5) phonology local dynamic (with any switch type)\n",
        "  (6) phonology switch only dynamic (with any switch type)\n",
        "  '''\n",
        "  import numpy as np\n",
        "  from scipy.optimize import fmin\n",
        "  r1,r2,r3 = random_vars\n",
        "\n",
        "  siml, simh,  freql, freqh, phonl, phonh = history_vars\n",
        "\n",
        "  if modelcode == 1:\n",
        "    return fmin(forage.model_static, [r1,r2,r3], args=(freql, freqh, siml, simh), ftol = 0.001, disp=False)\n",
        "  elif modelcode == 2:\n",
        "    return fmin(forage.model_dynamic_psyrev, [r1,r2, r3], args=(freql, freqh, siml, simh, switchvals), ftol = 0.001, disp=False)\n",
        "  elif modelcode == 3:\n",
        "    return fmin(forage.model_static_plocal, [r1,r2, r3], args=(freql, freqh, siml, simh, phonl, phonh), ftol = 0.001, disp=False)\n",
        "  elif modelcode == 4:\n",
        "    return fmin(forage.model_dynamic_pglobal, [r1,r2, r3], args=(freql, freqh, siml, simh, phonl, phonh,switchvals), ftol = 0.001, disp=False)\n",
        "  elif modelcode == 5:\n",
        "    return fmin(forage.model_dynamic_plocal, [r1,r2, r3], args=(freql, freqh, siml, simh, phonl, phonh, switchvals), ftol = 0.001, disp=False)\n",
        "  else: \n",
        "    return fmin(forage.model_dynamic_pswitchonly, [r1,r2, r3], args=(freql, freqh, siml, simh, phonl, phonh, switchvals), ftol = 0.001, disp=False)"
      ],
      "metadata": {
        "id": "7WLKjqfaeNl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run models"
      ],
      "metadata": {
        "id": "D8Vm4NznnuDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(modelcode, betas, history_vars, switchvals):\n",
        "  siml, simh,  freql, freqh, phonl, phonh = history_vars\n",
        "\n",
        "  if modelcode == 1:\n",
        "    return forage.model_static(betas, freql, freqh, siml, simh)\n",
        "  elif modelcode == 2:\n",
        "    return forage.model_dynamic_psyrev(betas, freql, freqh, siml, simh, switchvals)\n",
        "  elif modelcode == 3:\n",
        "    return forage.model_static_plocal(betas, freql, freqh, siml, simh, phonl, phonh)\n",
        "  elif modelcode == 4:\n",
        "    return forage.model_dynamic_pglobal(betas, freql, freqh, siml, simh, phonl, phonh, switchvals)\n",
        "  elif modelcode == 5:\n",
        "    return forage.model_dynamic_plocal(betas, freql, freqh, siml, simh, phonl, phonh, switchvals)\n",
        "  else: \n",
        "    return forage.model_dynamic_pswitchonly(betas, freql, freqh, siml, simh, phonl, phonh, switchvals)"
      ],
      "metadata": {
        "id": "DbJFeeYnj_uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## store data"
      ],
      "metadata": {
        "id": "SBamYngjT07D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_data(betas, nLLs, fluency_list):\n",
        "  # function that formats everything correctly\n",
        "  df = pd.DataFrame()\n",
        "  modelnames = dict()\n",
        "  modelnames[str([1,0])] = \"psyrev static simdrop\"\n",
        "  modelnames[str([1,1])] = \"psyrev static troyer\"\n",
        "  modelnames[str([1,2])] = \"psyrev static participant\"\n",
        "\n",
        "  modelnames[str([2,0])] = \"psyrev dynamic simdrop\"\n",
        "  modelnames[str([2,1])] = \"psyrev dynamic troyer\"\n",
        "  modelnames[str([2,2])] = \"psyrev dynamic participant\"\n",
        "\n",
        "  modelnames[str([3,0])] = \"static plocal simdrop\"\n",
        "  modelnames[str([3,1])] = \"static plocal troyer\"\n",
        "  modelnames[str([3,2])] = \"static plocal participant\"\n",
        "\n",
        "  modelnames[str([4,0])] = \"dynamic pglobal simdrop\"\n",
        "  modelnames[str([4,1])] = \"dynamic pglobal troyer\"\n",
        "  modelnames[str([4,2])] = \"dynamic pglobal participant\"\n",
        "\n",
        "  modelnames[str([5,0])] = \"dynamic plocal simdrop\"\n",
        "  modelnames[str([5,1])] = \"dynamic plocal troyer\"\n",
        "  modelnames[str([5,2])] = \"dynamic plocal participant\"\n",
        "\n",
        "  modelnames[str([6,0])] = \"dynamic pswitchonly simdrop\"\n",
        "  modelnames[str([6,1])] = \"dynamic pswitchonly troyer\"\n",
        "  modelnames[str([6,2])] = \"dynamic pswitchonly participant\"\n",
        "  \n",
        "  df[\"beta_labels\"] = betas.keys()\n",
        "  df[\"beta_frequency_semantic_phonology\"] = betas.values()\n",
        "  df[\"model_names\"] = df.apply(lambda x: modelnames[x['beta_labels']], axis=1)\n",
        "  df[\"optimal_nLLs\"] = nLLs[0]\n",
        "  df[\"random_nLLs\"] = nLLs[1]\n",
        "\n",
        "  df[\"N\"] = len(fluency_list)\n",
        "  \n",
        "  return df"
      ],
      "metadata": {
        "id": "nKOV_vQImZeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create fit df"
      ],
      "metadata": {
        "id": "9yOcsOBrnvuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def modelFits(path, delimiter, model_list):\n",
        "    ### LOAD BEHAVIORAL DATA ###\n",
        "    df = pd.read_csv(path, header=None, names=['SID', 'entry', 'participantswitch'], delimiter=delimiter)\n",
        "\n",
        "    r1 = np.random.rand()\n",
        "    r2 = np.random.rand()\n",
        "    r3 = np.random.rand()\n",
        "\n",
        "    sidlist = df[\"SID\"].unique()\n",
        "    ct = 0\n",
        "    allfit_df = pd.DataFrame()\n",
        "    allmetrics = pd.DataFrame()\n",
        "    for sid in sidlist[:1]:\n",
        "      ct+=1\n",
        "      print( \"SUBJECT \" + str(ct) + '/' + str(len(sidlist)) + \" \" + str(sid))\n",
        "      # for each subject, run the models specified in model_list\n",
        "      newdata_main = df[df[\"SID\"] == sid]\n",
        "      sub_list = list(newdata_main[\"entry\"])\n",
        "      participant_switch = list(newdata_main[\"participantswitch\"])\n",
        "\n",
        "      history_vars = create_history_variables(sub_list, labels, sim_matrix, freq_matrix, phon_matrix )\n",
        "      switchvals = create_switch_lists(sub_list, history_vars) # contains the different switch lists\n",
        "\n",
        "      # add participant-level switch list here\n",
        "      switchvals.append(participant_switch)\n",
        "      \n",
        "      # obtain fmin output (betas) for different models and switch methods\n",
        "      optimal_betas = {str([model_i, i]):fmin_output_models(model_i, history_vars, switchvals[i], [r1,r2,r3]).tolist() for model_i in model_list for i in range(len(switchvals))}\n",
        "\n",
        "      optimal_fits = [run_model(model_i, optimal_betas[str([model_i,i])], history_vars, switchvals[i]) for model_i in model_list for i in range(len(switchvals))]\n",
        "      random_fits = [run_model(model_i, [0]*len(optimal_betas[str([model_i,i])]), history_vars, switchvals[i]) for model_i in model_list for i in range(len(switchvals))]\n",
        "\n",
        "      fit_df = store_data(optimal_betas, [optimal_fits, random_fits], sub_list)\n",
        "      fit_df[\"subject\"] = sid\n",
        "\n",
        "      allfit_df = pd.concat([allfit_df, fit_df])\n",
        "\n",
        "      # also create a df for specific switch values/history vars\n",
        "      metric_df = pd.DataFrame()\n",
        "      \n",
        "      metric_df[\"item\"] = sub_list\n",
        "      metric_df[\"subject\"] = sid\n",
        "      metric_df[\"semantic\"] = history_vars[0]\n",
        "      metric_df[\"frequency\"] = history_vars[2]\n",
        "      metric_df[\"phonology\"] = history_vars[4]\n",
        "      metric_df[\"simdrop\"] = switchvals[0]\n",
        "      metric_df[\"troyer\"] = switchvals[1]\n",
        "      metric_df[\"participant\"] = switchvals[2]\n",
        "      \n",
        "      allmetrics = pd.concat([allmetrics, metric_df])\n",
        "    \n",
        "    return allfit_df, allmetrics"
      ],
      "metadata": {
        "id": "7eeV5huEPEga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allfit, allmetric = modelFits('data-fluency.txt', delimiter = \"\\t\", model_list=[1,2,3,4,5, 6])"
      ],
      "metadata": {
        "id": "RQrULJyiOqXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(allfit.head())\n",
        "display(allmetric.head())"
      ],
      "metadata": {
        "id": "BzL-1oXxRWCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "id": "uTlleQqykh31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parentfolder = \"/content/drive/My Drive/IU-Abhilasha-Mike/Fluency/sem-phon/fluency_cogsci2022/\"\n",
        "\n",
        "allfit.to_csv(parentfolder  + 'cogsci2022-fits.csv', index=False, header=True)\n",
        "allmetric.to_csv(parentfolder  + 'cogsci2022-metrics.csv', index=False, header=True)"
      ],
      "metadata": {
        "id": "a1KkCUEbSglF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# code for obtaining semantic and phonological similarity"
      ],
      "metadata": {
        "id": "U_amfhYwekDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we compute the phonemic & semantic similarity matrices for a vocabulary of 771 animals"
      ],
      "metadata": {
        "id": "CikhMo2gNuAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create semantic similarity matrix\n",
        "embeddings for 771 animal words were obtained from: https://github.com/plasticityai/magnitude"
      ],
      "metadata": {
        "id": "-rOFw544fGad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import embeddings\n",
        "with tf.device('/device:GPU:0'):\n",
        "  embeddings = pd.read_csv(\"animals_embeddings.csv\").values\n",
        "  print(f\"embeddings are shaped:\", embeddings.shape)\n",
        "  print(f\"vocab is {len(embeddings)} words\")"
      ],
      "metadata": {
        "id": "f3aFPCCZelEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simlabels = pd.read_csv('new_animals.csv', header=None).values.reshape(-1,).tolist()\n",
        "len(simlabels)"
      ],
      "metadata": {
        "id": "T0_hYk_MDaEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sim_matrix(embeddings, simlabels):\n",
        "  N = len(simlabels)\n",
        "  # create semantic similarity matrix\n",
        "  matrix = 1-scipy.spatial.distance.cdist(embeddings, embeddings, 'cosine').reshape(-1)\n",
        "  matrix = matrix.reshape((N,N))\n",
        "  print(\"sim matrix has been created:\", matrix.shape)\n",
        "\n",
        "  w1_index = simlabels.index(\"dolphin\")\n",
        "  w2_index = simlabels.index(\"kitten\")\n",
        "  w3_index = simlabels.index(\"whale\")\n",
        "\n",
        "  print(\"dolphin-kitten:\", matrix[w1_index, w2_index])\n",
        "  print(\"dolphin-whale:\", matrix[w1_index, w3_index])\n",
        "  print(\"dolphin-dolphin:\", matrix[w1_index, w1_index])\n",
        "\n",
        "  pd.DataFrame(matrix).to_csv(parentfolder + 'word2vec_sim_matrix.csv', index=False, header=False)\n",
        "\n",
        "  print(\"sim matrix has been saved to drive!\")\n",
        "\n",
        "create_sim_matrix(embeddings, simlabels)\n"
      ],
      "metadata": {
        "id": "xvFN6weVe-vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create phonological similarity matrix"
      ],
      "metadata": {
        "id": "v5HTHGsjfI_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### phoneme function\n",
        "\n",
        "We define a phoneme extraction function, that takes an input string and returns the phonemes based on CMUDict."
      ],
      "metadata": {
        "id": "PxeaTfrGeNla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# algo to obtain phonemes for any given strng\n",
        "# obtained from: https://stackoverflow.com/questions/33666557/get-phonemes-from-any-word-in-python-nltk-or-other-modules\n",
        "try:\n",
        "    arpabet = nltk.corpus.cmudict.dict()\n",
        "except LookupError:\n",
        "    nltk.download('cmudict')\n",
        "    arpabet = nltk.corpus.cmudict.dict()\n",
        "\n",
        "@lru_cache()\n",
        "def wordbreak(s):\n",
        "    s = s.lower()\n",
        "    if s in arpabet:\n",
        "        return arpabet[s]\n",
        "    middle = len(s)/2\n",
        "    partition = sorted(list(range(len(s))), key=lambda x: (x-middle)**2-x)\n",
        "    for i in partition:\n",
        "        pre, suf = (s[:i], s[i:])\n",
        "        if pre in arpabet and wordbreak(suf) is not None:\n",
        "            return [x+y for x,y in iterprod(arpabet[pre], wordbreak(suf))]\n",
        "    return None\n",
        "\n",
        "def normalized_sim(w1, w2):\n",
        "  return 1-edit_distance(w1,w2)/(max(len(w1), len(w2)))\n"
      ],
      "metadata": {
        "id": "Yzh1x4VTeOkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_phon_matrix(vocab):\n",
        "  vocabulary = vocab.copy()\n",
        "  N = len(vocabulary)\n",
        "  # replace all underscores (_) with space (\" \") to match with glove vectors/vocab\n",
        "  import re\n",
        "  vocabulary = [re.sub('[^a-zA-Z]+', '', str(v)) for v in vocabulary]\n",
        "  # create phonemic similarity matrix for the small vocab\n",
        "  pmatrix = np.array([normalized_sim(wordbreak(w1)[0], wordbreak(w2)[0]) for w1 in vocabulary for w2 in vocabulary]).reshape((N,N))\n",
        "  print(\"pmatrix has been created:\", pmatrix.shape)\n",
        "  print(pmatrix)\n",
        "  pd.DataFrame(pmatrix).to_csv(parentfolder + 'simlabels_phon_matrix.csv', index=False, header=False)  \n",
        "  print(\"phon matrix has been saved to drive!\")\n",
        "\n",
        "create_phon_matrix(simlabels)\n"
      ],
      "metadata": {
        "id": "b2HouuHsfK4w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}