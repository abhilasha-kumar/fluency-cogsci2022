{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YemaL37bd2-w"
      },
      "source": [
        "# Mouse-mole-vole: The inconspicuous benefit of phonology during retrieval from semantic memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwyFlkqfflFH"
      },
      "source": [
        "# Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7f1RA_0MeKWo"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import itertools\n",
        "import scipy.spatial.distance\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "from scipy.special import softmax\n",
        "from sklearn.preprocessing import MinMaxScaler, normalize\n",
        "from numpy.linalg import matrix_power\n",
        "from functools import lru_cache\n",
        "import glob\n",
        "from scipy.special import expit\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from functools import lru_cache\n",
        "from itertools import product as iterprod\n",
        "import itertools\n",
        "from nltk.metrics import *\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp-U9_MEN8Rz"
      },
      "source": [
        "# importing norms and embeddings\n",
        "\n",
        "Here we import the Troyer norms, as well as frequency, semantic and phonological similarity matrices. See code at the end of the notebook for computing the similarity matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V0G5K2xAN1jo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size of vocab is 771 animals\n"
          ]
        }
      ],
      "source": [
        "norms = pd.read_csv(\"data/troyernorms.csv\", encoding=\"unicode-escape\")\n",
        "freq_matrix = pd.read_csv('data/animals_frequencies.csv', header = None)\n",
        "labels = list(freq_matrix[0])\n",
        "freq_matrix = np.array(freq_matrix[1])\n",
        "sim_matrix = pd.read_csv('data/word2vec_sim_matrix.csv',header = None).values\n",
        "phon_matrix = pd.read_csv('data/simlabels_phon_matrix.csv',header = None).values\n",
        "print(f\"size of vocab is {len(labels)} animals\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SF41g03OzVi"
      },
      "source": [
        "# define foraging models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjW_ijRfnkIF"
      },
      "source": [
        "## history function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-ZQkMOHB8D8X"
      },
      "outputs": [],
      "source": [
        "def create_history_variables(fluency_list, labels, sim_matrix, freq_matrix, phon_matrix = None):\n",
        "  '''\n",
        "  inputs:\n",
        "  (1) sim_matrix: semantic similarity matrix (NxN np.array)\n",
        "  (2) phon_matrix: phonological similarity matrix (NxN np.array)\n",
        "  (3) freq_matrix: frequencies array (Nx1 array)\n",
        "  (4) labels: the space of words (list of length N)\n",
        "  (5) fluency_list: items produced by a participant (list of size L)\n",
        "  \n",
        "  outputs:\n",
        "  (1) sim_list: semantic similarities between each item in fluency_list (list of size L)\n",
        "  (2) sim_history: semantic similarities of each word with all items in labels (list of L arrays of size N)\n",
        "  (1) phon_list: phonological similarities between each item in fluency_list (list of size L)\n",
        "  (2) phon_history: phonological similarities of each word with all items in labels (list of L arrays of size N)\n",
        "  (1) freq_list: frequencies of each item in fluency_list (list of size L)\n",
        "  (2) freq_history: frequencies of all items in labels repeated L items (list of L arrays of size N)\n",
        "  \n",
        "  \n",
        "  '''\n",
        "  phon_matrix[phon_matrix <= 0] = .0001\n",
        "  sim_matrix[sim_matrix <= 0] = .0001\n",
        "\n",
        "  freq_list = []\n",
        "  freq_history = []\n",
        "\n",
        "  sim_list = []\n",
        "  sim_history = []\n",
        "\n",
        "  phon_list = []\n",
        "  phon_history = []\n",
        "\n",
        "  for i in range(0,len(fluency_list)):\n",
        "    word = fluency_list[i]\n",
        "    currentwordindex = labels.index(word)\n",
        "\n",
        "    freq_list.append(freq_matrix[currentwordindex])\n",
        "    freq_history.append(freq_matrix)\n",
        "  \n",
        "    if i > 0: # get similarity between this word and preceding word\n",
        "      prevwordindex = labels.index(fluency_list[i-1])\n",
        "      sim_list.append(sim_matrix[prevwordindex, currentwordindex] )\n",
        "      sim_history.append(sim_matrix[prevwordindex,:])\n",
        "      if phon_matrix is not None:\n",
        "        phon_list.append(phon_matrix[prevwordindex, currentwordindex] )\n",
        "        phon_history.append(phon_matrix[prevwordindex,:])\n",
        "    else: # first word\n",
        "      sim_list.append(0.0001)\n",
        "      sim_history.append(sim_matrix[currentwordindex,:])\n",
        "      if phon_matrix is not None:\n",
        "        phon_list.append(0.0001)\n",
        "        phon_history.append(phon_matrix[currentwordindex,:])\n",
        "  \n",
        "  return sim_list, sim_history, freq_list, freq_history,phon_list, phon_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0nfMSLPnm-U"
      },
      "source": [
        "## forage class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oroA2Smv8RW5"
      },
      "outputs": [],
      "source": [
        "class forage:\n",
        "    def model_static(beta, freql, freqh, siml, simh):\n",
        "        ## beta contains the optimization parameters for frequency (beta[0]) and semantic similarity (beta[1])\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0: # if first item then its probability is based on just frequency\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else: # if not first item then its probability is based on its similarity to prev item AND frequency\n",
        "            # P of item based on frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat) # negative Log likelihood of this item: this will be minimized eventually\n",
        "        return ct\n",
        "   \n",
        "    def model_dynamic_psyrev(beta, freql, freqh, siml, simh, switchvals):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif switchvals[k]==1: ## \"dip\" based on switch type\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "\n",
        "    def model_static_plocal(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0: \n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else: \n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) * pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2])* pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "      \n",
        "    def model_dynamic_pswitchonly(beta, freql, freqh, siml, simh, phonl, phonh,switchvals):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif switchvals[k]==1:\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2]) )\n",
        "            else:\n",
        "                numrat = pow(freql[k],beta[0])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "    \n",
        "\n",
        "    def model_dynamic_plocal(beta, freql, freqh, siml, simh, phonl, phonh,switchvals):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif switchvals[k]==1: \n",
        "                numrat = pow(freql[k],beta[0]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "                numrat = pow(freql[k],beta[0])*pow(phonl[k],beta[2])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(phonh[k],beta[2])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "            \n",
        "        return ct\n",
        "\n",
        "    def model_dynamic_pglobal(beta, freql, freqh, siml, simh, phonl, phonh,switchvals):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif switchvals[k]==1:\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2]) )\n",
        "            else:\n",
        "                numrat = pow(freql[k],beta[0])*pow(phonl[k],beta[2])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(phonh[k],beta[2])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8unfZF6not5"
      },
      "source": [
        "## switch function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iZvHocT36WX-"
      },
      "outputs": [],
      "source": [
        "def create_switch_lists(fluency_list, history_vars):\n",
        "  '''\n",
        "  takes in a fluency list and history variables and creates all possible switch/cluster designations\n",
        "  current implemented methods:\n",
        "  (1) simdrop\n",
        "  (2) troyer norms\n",
        " \n",
        "  output:\n",
        "  a list of lists corresponding to \n",
        "  (1) simdrop\n",
        "  (2) troyer\n",
        "  '''\n",
        "  simdrop = []\n",
        "  troyer = []\n",
        "  \n",
        "  semantic_similarity = history_vars[0]\n",
        "  phonological_similarity = history_vars[4]\n",
        "\n",
        "  for k in range(len(fluency_list)):\n",
        "    if (k > 0 and k < (len(fluency_list)-2)): \n",
        "      # simdrop\n",
        "      if (semantic_similarity[k+1] > semantic_similarity[k]) and (semantic_similarity[k-1] > semantic_similarity[k]):\n",
        "        simdrop.append(1)\n",
        "      else:\n",
        "        simdrop.append(0)\n",
        "      \n",
        "      # troyer\n",
        "      item1 = fluency_list[k]\n",
        "      item2 = fluency_list[k-1]\n",
        "      category1 = norms[norms['Animal'] == item1]['Category'].values.tolist()\n",
        "      category2 = norms[norms['Animal'] == item2]['Category'].values.tolist()\n",
        "      if len(list(set(category1) & set(category2)))== 0:\n",
        "          troyer.append(1)\n",
        "      else:\n",
        "          troyer.append(0)\n",
        "    else:\n",
        "        simdrop.append(2)\n",
        "        troyer.append(2)\n",
        "  \n",
        "  finalswitches = []\n",
        "  finalswitches.append(simdrop)\n",
        "  finalswitches.append(troyer)\n",
        "\n",
        "  return finalswitches "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElqQc0F_nr8a"
      },
      "source": [
        "## fmin - optimal betas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7WLKjqfaeNl6"
      },
      "outputs": [],
      "source": [
        "def fmin_output_models (modelcode, history_vars, switchvals, random_vars):\n",
        "  '''\n",
        "  total number of models:\n",
        "  (1) psyrev static\n",
        "  (2) psyrev dynamic (with any switch type)\n",
        "  (3) phonology static (with any switch type)\n",
        "  (4) phonology global dynamic (with any switch type)\n",
        "  (5) phonology local dynamic (with any switch type)\n",
        "  (6) phonology switch only dynamic (with any switch type)\n",
        "  '''\n",
        "  import numpy as np\n",
        "  from scipy.optimize import fmin\n",
        "  r1,r2,r3 = random_vars\n",
        "\n",
        "  siml, simh,  freql, freqh, phonl, phonh = history_vars\n",
        "\n",
        "  if modelcode == 1:\n",
        "    return fmin(forage.model_static, [r1,r2,r3], args=(freql, freqh, siml, simh), ftol = 0.001, disp=False)\n",
        "  elif modelcode == 2:\n",
        "    return fmin(forage.model_dynamic_psyrev, [r1,r2, r3], args=(freql, freqh, siml, simh, switchvals), ftol = 0.001, disp=False)\n",
        "  elif modelcode == 3:\n",
        "    return fmin(forage.model_static_plocal, [r1,r2, r3], args=(freql, freqh, siml, simh, phonl, phonh), ftol = 0.001, disp=False)\n",
        "  elif modelcode == 4:\n",
        "    return fmin(forage.model_dynamic_pglobal, [r1,r2, r3], args=(freql, freqh, siml, simh, phonl, phonh,switchvals), ftol = 0.001, disp=False)\n",
        "  elif modelcode == 5:\n",
        "    return fmin(forage.model_dynamic_plocal, [r1,r2, r3], args=(freql, freqh, siml, simh, phonl, phonh, switchvals), ftol = 0.001, disp=False)\n",
        "  else: \n",
        "    return fmin(forage.model_dynamic_pswitchonly, [r1,r2, r3], args=(freql, freqh, siml, simh, phonl, phonh, switchvals), ftol = 0.001, disp=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8Vm4NznnuDj"
      },
      "source": [
        "## run models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DbJFeeYnj_uw"
      },
      "outputs": [],
      "source": [
        "def run_model(modelcode, betas, history_vars, switchvals):\n",
        "  siml, simh,  freql, freqh, phonl, phonh = history_vars\n",
        "\n",
        "  if modelcode == 1:\n",
        "    return forage.model_static(betas, freql, freqh, siml, simh)\n",
        "  elif modelcode == 2:\n",
        "    return forage.model_dynamic_psyrev(betas, freql, freqh, siml, simh, switchvals)\n",
        "  elif modelcode == 3:\n",
        "    return forage.model_static_plocal(betas, freql, freqh, siml, simh, phonl, phonh)\n",
        "  elif modelcode == 4:\n",
        "    return forage.model_dynamic_pglobal(betas, freql, freqh, siml, simh, phonl, phonh, switchvals)\n",
        "  elif modelcode == 5:\n",
        "    return forage.model_dynamic_plocal(betas, freql, freqh, siml, simh, phonl, phonh, switchvals)\n",
        "  else: \n",
        "    return forage.model_dynamic_pswitchonly(betas, freql, freqh, siml, simh, phonl, phonh, switchvals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBamYngjT07D"
      },
      "source": [
        "## store data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nKOV_vQImZeR"
      },
      "outputs": [],
      "source": [
        "def store_data(betas, nLLs, fluency_list):\n",
        "  # function that formats everything correctly\n",
        "  df = pd.DataFrame()\n",
        "  modelnames = dict()\n",
        "  modelnames[str([1,0])] = \"psyrev static simdrop\"\n",
        "  modelnames[str([1,1])] = \"psyrev static troyer\"\n",
        "  modelnames[str([1,2])] = \"psyrev static participant\"\n",
        "\n",
        "  modelnames[str([2,0])] = \"psyrev dynamic simdrop\"\n",
        "  modelnames[str([2,1])] = \"psyrev dynamic troyer\"\n",
        "  modelnames[str([2,2])] = \"psyrev dynamic participant\"\n",
        "\n",
        "  modelnames[str([3,0])] = \"static plocal simdrop\"\n",
        "  modelnames[str([3,1])] = \"static plocal troyer\"\n",
        "  modelnames[str([3,2])] = \"static plocal participant\"\n",
        "\n",
        "  modelnames[str([4,0])] = \"dynamic pglobal simdrop\"\n",
        "  modelnames[str([4,1])] = \"dynamic pglobal troyer\"\n",
        "  modelnames[str([4,2])] = \"dynamic pglobal participant\"\n",
        "\n",
        "  modelnames[str([5,0])] = \"dynamic plocal simdrop\"\n",
        "  modelnames[str([5,1])] = \"dynamic plocal troyer\"\n",
        "  modelnames[str([5,2])] = \"dynamic plocal participant\"\n",
        "\n",
        "  modelnames[str([6,0])] = \"dynamic pswitchonly simdrop\"\n",
        "  modelnames[str([6,1])] = \"dynamic pswitchonly troyer\"\n",
        "  modelnames[str([6,2])] = \"dynamic pswitchonly participant\"\n",
        "  \n",
        "  df[\"beta_labels\"] = betas.keys()\n",
        "  df[\"beta_frequency_semantic_phonology\"] = betas.values()\n",
        "  df[\"model_names\"] = df.apply(lambda x: modelnames[x['beta_labels']], axis=1)\n",
        "  df[\"optimal_nLLs\"] = nLLs[0]\n",
        "  df[\"random_nLLs\"] = nLLs[1]\n",
        "\n",
        "  df[\"N\"] = len(fluency_list)\n",
        "  \n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yOcsOBrnvuz"
      },
      "source": [
        "## create fit df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7eeV5huEPEga"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def modelFits(path, delimiter, model_list):\n",
        "    ### LOAD BEHAVIORAL DATA ###\n",
        "    df = pd.read_csv(path, header=None, names=['SID', 'entry', 'participantswitch'], delimiter=delimiter)\n",
        "\n",
        "    r1 = np.random.rand()\n",
        "    r2 = np.random.rand()\n",
        "    r3 = np.random.rand()\n",
        "\n",
        "    sidlist = df[\"SID\"].unique()\n",
        "    ct = 0\n",
        "    allfit_df = pd.DataFrame()\n",
        "    allmetrics = pd.DataFrame()\n",
        "    for sid in sidlist:\n",
        "      ct+=1\n",
        "      print( \"SUBJECT \" + str(ct) + '/' + str(len(sidlist)) + \" \" + str(sid))\n",
        "      # for each subject, run the models specified in model_list\n",
        "      newdata_main = df[df[\"SID\"] == sid]\n",
        "      sub_list = list(newdata_main[\"entry\"])\n",
        "      participant_switch = list(newdata_main[\"participantswitch\"])\n",
        "\n",
        "      history_vars = create_history_variables(sub_list, labels, sim_matrix, freq_matrix, phon_matrix )\n",
        "      switchvals = create_switch_lists(sub_list, history_vars) # contains the different switch lists\n",
        "\n",
        "      # add participant-level switch list here\n",
        "      switchvals.append(participant_switch)\n",
        "      \n",
        "      # obtain fmin output (betas) for different models and switch methods\n",
        "      optimal_betas = {str([model_i, i]):fmin_output_models(model_i, history_vars, switchvals[i], [r1,r2,r3]).tolist() for model_i in model_list for i in range(len(switchvals))}\n",
        "\n",
        "      optimal_fits = [run_model(model_i, optimal_betas[str([model_i,i])], history_vars, switchvals[i]) for model_i in model_list for i in range(len(switchvals))]\n",
        "      random_fits = [run_model(model_i, [0]*len(optimal_betas[str([model_i,i])]), history_vars, switchvals[i]) for model_i in model_list for i in range(len(switchvals))]\n",
        "\n",
        "      fit_df = store_data(optimal_betas, [optimal_fits, random_fits], sub_list)\n",
        "      fit_df[\"subject\"] = sid\n",
        "\n",
        "      allfit_df = pd.concat([allfit_df, fit_df])\n",
        "\n",
        "      # also create a df for specific switch values/history vars\n",
        "      metric_df = pd.DataFrame()\n",
        "      \n",
        "      metric_df[\"item\"] = sub_list\n",
        "      metric_df[\"subject\"] = sid\n",
        "      metric_df[\"semantic\"] = history_vars[0]\n",
        "      metric_df[\"frequency\"] = history_vars[2]\n",
        "      metric_df[\"phonology\"] = history_vars[4]\n",
        "      metric_df[\"simdrop\"] = switchvals[0]\n",
        "      metric_df[\"troyer\"] = switchvals[1]\n",
        "      metric_df[\"participant\"] = switchvals[2]\n",
        "      \n",
        "      allmetrics = pd.concat([allmetrics, metric_df])\n",
        "    \n",
        "    return allfit_df, allmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run models\n",
        "This cell runs the foraging models for the 171 participants (141 in HJT dataset and 30 in the LEA dataset). This process takes a few minutes (~25 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RQrULJyiOqXD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUBJECT 1/171 50001\n",
            "SUBJECT 2/171 50002\n",
            "SUBJECT 3/171 50003\n",
            "SUBJECT 4/171 50004\n",
            "SUBJECT 5/171 50006\n",
            "SUBJECT 6/171 50007\n",
            "SUBJECT 7/171 50008\n",
            "SUBJECT 8/171 50010\n",
            "SUBJECT 9/171 50011\n",
            "SUBJECT 10/171 50012\n",
            "SUBJECT 11/171 50013\n",
            "SUBJECT 12/171 50014\n",
            "SUBJECT 13/171 50016\n",
            "SUBJECT 14/171 50017\n",
            "SUBJECT 15/171 50018\n",
            "SUBJECT 16/171 50019\n",
            "SUBJECT 17/171 50020\n",
            "SUBJECT 18/171 50022\n",
            "SUBJECT 19/171 50023\n",
            "SUBJECT 20/171 50024\n",
            "SUBJECT 21/171 50025\n",
            "SUBJECT 22/171 50026\n",
            "SUBJECT 23/171 50027\n",
            "SUBJECT 24/171 50029\n",
            "SUBJECT 25/171 50030\n",
            "SUBJECT 26/171 50031\n",
            "SUBJECT 27/171 50032\n",
            "SUBJECT 28/171 50033\n",
            "SUBJECT 29/171 50034\n",
            "SUBJECT 30/171 50036\n",
            "SUBJECT 31/171 51\n",
            "SUBJECT 32/171 71\n",
            "SUBJECT 33/171 198\n",
            "SUBJECT 34/171 199\n",
            "SUBJECT 35/171 285\n",
            "SUBJECT 36/171 286\n",
            "SUBJECT 37/171 287\n",
            "SUBJECT 38/171 288\n",
            "SUBJECT 39/171 289\n",
            "SUBJECT 40/171 290\n",
            "SUBJECT 41/171 291\n",
            "SUBJECT 42/171 292\n",
            "SUBJECT 43/171 293\n",
            "SUBJECT 44/171 294\n",
            "SUBJECT 45/171 295\n",
            "SUBJECT 46/171 296\n",
            "SUBJECT 47/171 297\n",
            "SUBJECT 48/171 298\n",
            "SUBJECT 49/171 379\n",
            "SUBJECT 50/171 380\n",
            "SUBJECT 51/171 381\n",
            "SUBJECT 52/171 382\n",
            "SUBJECT 53/171 383\n",
            "SUBJECT 54/171 384\n",
            "SUBJECT 55/171 385\n",
            "SUBJECT 56/171 386\n",
            "SUBJECT 57/171 387\n",
            "SUBJECT 58/171 388\n",
            "SUBJECT 59/171 389\n",
            "SUBJECT 60/171 390\n",
            "SUBJECT 61/171 391\n",
            "SUBJECT 62/171 392\n",
            "SUBJECT 63/171 393\n",
            "SUBJECT 64/171 394\n",
            "SUBJECT 65/171 395\n",
            "SUBJECT 66/171 396\n",
            "SUBJECT 67/171 397\n",
            "SUBJECT 68/171 398\n",
            "SUBJECT 69/171 474\n",
            "SUBJECT 70/171 475\n",
            "SUBJECT 71/171 476\n",
            "SUBJECT 72/171 477\n",
            "SUBJECT 73/171 478\n",
            "SUBJECT 74/171 479\n",
            "SUBJECT 75/171 480\n",
            "SUBJECT 76/171 481\n",
            "SUBJECT 77/171 482\n",
            "SUBJECT 78/171 483\n",
            "SUBJECT 79/171 484\n",
            "SUBJECT 80/171 485\n",
            "SUBJECT 81/171 486\n",
            "SUBJECT 82/171 487\n",
            "SUBJECT 83/171 488\n",
            "SUBJECT 84/171 489\n",
            "SUBJECT 85/171 490\n",
            "SUBJECT 86/171 491\n",
            "SUBJECT 87/171 492\n",
            "SUBJECT 88/171 493\n",
            "SUBJECT 89/171 494\n",
            "SUBJECT 90/171 495\n",
            "SUBJECT 91/171 496\n",
            "SUBJECT 92/171 497\n",
            "SUBJECT 93/171 498\n",
            "SUBJECT 94/171 575\n",
            "SUBJECT 95/171 576\n",
            "SUBJECT 96/171 577\n",
            "SUBJECT 97/171 578\n",
            "SUBJECT 98/171 579\n",
            "SUBJECT 99/171 580\n",
            "SUBJECT 100/171 581\n",
            "SUBJECT 101/171 582\n",
            "SUBJECT 102/171 583\n",
            "SUBJECT 103/171 584\n",
            "SUBJECT 104/171 585\n",
            "SUBJECT 105/171 586\n",
            "SUBJECT 106/171 587\n",
            "SUBJECT 107/171 588\n",
            "SUBJECT 108/171 589\n",
            "SUBJECT 109/171 590\n",
            "SUBJECT 110/171 591\n",
            "SUBJECT 111/171 592\n",
            "SUBJECT 112/171 593\n",
            "SUBJECT 113/171 594\n",
            "SUBJECT 114/171 595\n",
            "SUBJECT 115/171 596\n",
            "SUBJECT 116/171 597\n",
            "SUBJECT 117/171 598\n",
            "SUBJECT 118/171 677\n",
            "SUBJECT 119/171 678\n",
            "SUBJECT 120/171 679\n",
            "SUBJECT 121/171 680\n",
            "SUBJECT 122/171 681\n",
            "SUBJECT 123/171 682\n",
            "SUBJECT 124/171 683\n",
            "SUBJECT 125/171 684\n",
            "SUBJECT 126/171 686\n",
            "SUBJECT 127/171 687\n",
            "SUBJECT 128/171 688\n",
            "SUBJECT 129/171 689\n",
            "SUBJECT 130/171 690\n",
            "SUBJECT 131/171 691\n",
            "SUBJECT 132/171 692\n",
            "SUBJECT 133/171 693\n",
            "SUBJECT 134/171 694\n",
            "SUBJECT 135/171 695\n",
            "SUBJECT 136/171 696\n",
            "SUBJECT 137/171 697\n",
            "SUBJECT 138/171 698\n",
            "SUBJECT 139/171 779\n",
            "SUBJECT 140/171 780\n",
            "SUBJECT 141/171 781\n",
            "SUBJECT 142/171 782\n",
            "SUBJECT 143/171 783\n",
            "SUBJECT 144/171 784\n",
            "SUBJECT 145/171 785\n",
            "SUBJECT 146/171 786\n",
            "SUBJECT 147/171 787\n",
            "SUBJECT 148/171 788\n",
            "SUBJECT 149/171 789\n",
            "SUBJECT 150/171 790\n",
            "SUBJECT 151/171 791\n",
            "SUBJECT 152/171 792\n",
            "SUBJECT 153/171 793\n",
            "SUBJECT 154/171 794\n",
            "SUBJECT 155/171 795\n",
            "SUBJECT 156/171 796\n",
            "SUBJECT 157/171 797\n",
            "SUBJECT 158/171 798\n",
            "SUBJECT 159/171 893\n",
            "SUBJECT 160/171 896\n",
            "SUBJECT 161/171 897\n",
            "SUBJECT 162/171 898\n",
            "SUBJECT 163/171 899\n",
            "SUBJECT 164/171 994\n",
            "SUBJECT 165/171 995\n",
            "SUBJECT 166/171 996\n",
            "SUBJECT 167/171 997\n",
            "SUBJECT 168/171 998\n",
            "SUBJECT 169/171 999\n",
            "SUBJECT 170/171 1098\n",
            "SUBJECT 171/171 1099\n"
          ]
        }
      ],
      "source": [
        "allfit, allmetric = modelFits('data/data-fluency.txt', delimiter = \"\\t\", model_list=[1,2,3,4,5, 6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BzL-1oXxRWCo"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beta_labels</th>\n",
              "      <th>beta_frequency_semantic_phonology</th>\n",
              "      <th>model_names</th>\n",
              "      <th>optimal_nLLs</th>\n",
              "      <th>random_nLLs</th>\n",
              "      <th>N</th>\n",
              "      <th>subject</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 0]</td>\n",
              "      <td>[2.118849817862145, 3.3959017925169173, 2.4607...</td>\n",
              "      <td>psyrev static simdrop</td>\n",
              "      <td>161.889318</td>\n",
              "      <td>192.782963</td>\n",
              "      <td>29</td>\n",
              "      <td>50001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 1]</td>\n",
              "      <td>[2.118849817862145, 3.3959017925169173, 2.4607...</td>\n",
              "      <td>psyrev static troyer</td>\n",
              "      <td>161.889318</td>\n",
              "      <td>192.782963</td>\n",
              "      <td>29</td>\n",
              "      <td>50001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 2]</td>\n",
              "      <td>[2.118849817862145, 3.3959017925169173, 2.4607...</td>\n",
              "      <td>psyrev static participant</td>\n",
              "      <td>161.889318</td>\n",
              "      <td>192.782963</td>\n",
              "      <td>29</td>\n",
              "      <td>50001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[2, 0]</td>\n",
              "      <td>[2.025309820356738, 4.01097268731335, -4.55386...</td>\n",
              "      <td>psyrev dynamic simdrop</td>\n",
              "      <td>161.455482</td>\n",
              "      <td>192.782963</td>\n",
              "      <td>29</td>\n",
              "      <td>50001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[2, 1]</td>\n",
              "      <td>[2.0614933395347323, 3.635874669687796, 0.6848...</td>\n",
              "      <td>psyrev dynamic troyer</td>\n",
              "      <td>169.919394</td>\n",
              "      <td>192.782963</td>\n",
              "      <td>29</td>\n",
              "      <td>50001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  beta_labels                  beta_frequency_semantic_phonology  \\\n",
              "0      [1, 0]  [2.118849817862145, 3.3959017925169173, 2.4607...   \n",
              "1      [1, 1]  [2.118849817862145, 3.3959017925169173, 2.4607...   \n",
              "2      [1, 2]  [2.118849817862145, 3.3959017925169173, 2.4607...   \n",
              "3      [2, 0]  [2.025309820356738, 4.01097268731335, -4.55386...   \n",
              "4      [2, 1]  [2.0614933395347323, 3.635874669687796, 0.6848...   \n",
              "\n",
              "                 model_names  optimal_nLLs  random_nLLs   N  subject  \n",
              "0      psyrev static simdrop    161.889318   192.782963  29    50001  \n",
              "1       psyrev static troyer    161.889318   192.782963  29    50001  \n",
              "2  psyrev static participant    161.889318   192.782963  29    50001  \n",
              "3     psyrev dynamic simdrop    161.455482   192.782963  29    50001  \n",
              "4      psyrev dynamic troyer    169.919394   192.782963  29    50001  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item</th>\n",
              "      <th>subject</th>\n",
              "      <th>semantic</th>\n",
              "      <th>frequency</th>\n",
              "      <th>phonology</th>\n",
              "      <th>simdrop</th>\n",
              "      <th>troyer</th>\n",
              "      <th>participant</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>horse</td>\n",
              "      <td>50001</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>3.676</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pig</td>\n",
              "      <td>50001</td>\n",
              "      <td>0.469811</td>\n",
              "      <td>3.300</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bear</td>\n",
              "      <td>50001</td>\n",
              "      <td>0.297824</td>\n",
              "      <td>3.467</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cat</td>\n",
              "      <td>50001</td>\n",
              "      <td>0.383209</td>\n",
              "      <td>3.529</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dog</td>\n",
              "      <td>50001</td>\n",
              "      <td>0.760946</td>\n",
              "      <td>3.993</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    item  subject  semantic  frequency  phonology  simdrop  troyer  \\\n",
              "0  horse    50001  0.000100      3.676     0.0001        2       2   \n",
              "1    pig    50001  0.469811      3.300     0.0001        0       0   \n",
              "2   bear    50001  0.297824      3.467     0.0001        1       1   \n",
              "3    cat    50001  0.383209      3.529     0.0001        0       1   \n",
              "4    dog    50001  0.760946      3.993     0.0001        0       0   \n",
              "\n",
              "   participant  \n",
              "0          2.0  \n",
              "1          0.0  \n",
              "2          1.0  \n",
              "3          1.0  \n",
              "4          0.0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(allfit.head())\n",
        "display(allmetric.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "a1KkCUEbSglF"
      },
      "outputs": [],
      "source": [
        "allfit.to_csv('data/cogsci2022-fits.csv', index=False, header=True)\n",
        "allmetric.to_csv('data/cogsci2022-metrics.csv', index=False, header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_amfhYwekDW"
      },
      "source": [
        "# code for obtaining semantic and phonological similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CikhMo2gNuAf"
      },
      "source": [
        "Here we compute the phonemic & semantic similarity matrices for a vocabulary of 771 animals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rOFw544fGad"
      },
      "source": [
        "## create semantic similarity matrix\n",
        "word2vec embeddings for 771 animal words were obtained from: https://github.com/plasticityai/magnitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "f3aFPCCZelEd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings are shaped: (771, 300)\n",
            "vocab is 771 words\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "771"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import embeddings\n",
        "embeddings = pd.read_csv(\"data/animals_embeddings.csv\").values\n",
        "print(f\"embeddings are shaped:\", embeddings.shape)\n",
        "print(f\"vocab is {len(embeddings)} words\")\n",
        "len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvFN6weVe-vq"
      },
      "outputs": [],
      "source": [
        "def create_sim_matrix(embeddings, simlabels):\n",
        "  N = len(simlabels)\n",
        "  # create semantic similarity matrix\n",
        "  matrix = 1-scipy.spatial.distance.cdist(embeddings, embeddings, 'cosine').reshape(-1)\n",
        "  matrix = matrix.reshape((N,N))\n",
        "  print(\"sim matrix has been created:\", matrix.shape)\n",
        "\n",
        "  w1_index = simlabels.index(\"dolphin\")\n",
        "  w2_index = simlabels.index(\"kitten\")\n",
        "  w3_index = simlabels.index(\"whale\")\n",
        "\n",
        "  print(\"dolphin-kitten:\", matrix[w1_index, w2_index])\n",
        "  print(\"dolphin-whale:\", matrix[w1_index, w3_index])\n",
        "  print(\"dolphin-dolphin:\", matrix[w1_index, w1_index])\n",
        "\n",
        "  pd.DataFrame(matrix).to_csv('data/word2vec_sim_matrix.csv', index=False, header=False)\n",
        "\n",
        "  print(\"sim matrix has been saved to drive!\")\n",
        "\n",
        "create_sim_matrix(embeddings, labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5HTHGsjfI_R"
      },
      "source": [
        "## create phonological similarity matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxeaTfrGeNla"
      },
      "source": [
        "### phoneme function\n",
        "\n",
        "We define a phoneme extraction function, that takes an input string and returns the phonemes based on CMUDict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yzh1x4VTeOkJ"
      },
      "outputs": [],
      "source": [
        "# algo to obtain phonemes for any given strng\n",
        "# obtained from: https://stackoverflow.com/questions/33666557/get-phonemes-from-any-word-in-python-nltk-or-other-modules\n",
        "try:\n",
        "    arpabet = nltk.corpus.cmudict.dict()\n",
        "except LookupError:\n",
        "    nltk.download('cmudict')\n",
        "    arpabet = nltk.corpus.cmudict.dict()\n",
        "\n",
        "@lru_cache()\n",
        "def wordbreak(s):\n",
        "    s = s.lower()\n",
        "    if s in arpabet:\n",
        "        return arpabet[s]\n",
        "    middle = len(s)/2\n",
        "    partition = sorted(list(range(len(s))), key=lambda x: (x-middle)**2-x)\n",
        "    for i in partition:\n",
        "        pre, suf = (s[:i], s[i:])\n",
        "        if pre in arpabet and wordbreak(suf) is not None:\n",
        "            return [x+y for x,y in iterprod(arpabet[pre], wordbreak(suf))]\n",
        "    return None\n",
        "\n",
        "def normalized_sim(w1, w2):\n",
        "  return 1-edit_distance(w1,w2)/(max(len(w1), len(w2)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2HouuHsfK4w"
      },
      "outputs": [],
      "source": [
        "def create_phon_matrix(vocab):\n",
        "  vocabulary = vocab.copy()\n",
        "  N = len(vocabulary)\n",
        "  # replace all underscores (_) with space (\" \") to match with glove vectors/vocab\n",
        "  import re\n",
        "  vocabulary = [re.sub('[^a-zA-Z]+', '', str(v)) for v in vocabulary]\n",
        "  # create phonemic similarity matrix for the small vocab\n",
        "  pmatrix = np.array([normalized_sim(wordbreak(w1)[0], wordbreak(w2)[0]) for w1 in vocabulary for w2 in vocabulary]).reshape((N,N))\n",
        "  print(\"pmatrix has been created:\", pmatrix.shape)\n",
        "  print(pmatrix)\n",
        "  pd.DataFrame(pmatrix).to_csv('data/simlabels_phon_matrix.csv', index=False, header=False)  \n",
        "  print(\"phon matrix has been saved to drive!\")\n",
        "\n",
        "create_phon_matrix(labels)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPZB3IDd6Z6Ru/SygXuXiLf",
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "fluency-cogsci2022.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
