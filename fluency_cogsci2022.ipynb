{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fluency-cogsci2022.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOoRQTNaNM7iq/RlpqXUUKz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhilasha-kumar/fluency-cogsci2022/blob/main/fluency_cogsci2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phonological Intrusions in Semantic Memory Retrieval"
      ],
      "metadata": {
        "id": "YemaL37bd2-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing drive, GPU, and packages"
      ],
      "metadata": {
        "id": "MwyFlkqfflFH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ggNcnk34d2O4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f70a49-7a65-4f6a-ed68-76f2b95e2827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "iYlJS26seDTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10bcc578-ebab-4074-d3bc-bdd214f6d6f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "Wed Jan 19 16:18:01 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    31W / 250W |    375MiB / 16280MiB |      1%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import heapq\n",
        "import itertools\n",
        "import scipy.spatial.distance\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from numpy.random import randint\n",
        "from scipy.special import softmax\n",
        "from sklearn.preprocessing import MinMaxScaler, normalize\n",
        "from numpy.linalg import matrix_power\n",
        "from functools import lru_cache\n",
        "import glob\n",
        "from scipy.special import expit\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from functools import lru_cache\n",
        "from itertools import product as iterprod\n",
        "import itertools\n",
        "from nltk.metrics import *\n",
        "\n"
      ],
      "metadata": {
        "id": "7f1RA_0MeKWo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phoneme Function"
      ],
      "metadata": {
        "id": "PxeaTfrGeNla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# algo to obtain phonemes for any given strng\n",
        "# obtained from: https://stackoverflow.com/questions/33666557/get-phonemes-from-any-word-in-python-nltk-or-other-modules\n",
        "try:\n",
        "    arpabet = nltk.corpus.cmudict.dict()\n",
        "except LookupError:\n",
        "    nltk.download('cmudict')\n",
        "    arpabet = nltk.corpus.cmudict.dict()\n",
        "\n",
        "@lru_cache()\n",
        "def wordbreak(s):\n",
        "    s = s.lower()\n",
        "    if s in arpabet:\n",
        "        return arpabet[s]\n",
        "    middle = len(s)/2\n",
        "    partition = sorted(list(range(len(s))), key=lambda x: (x-middle)**2-x)\n",
        "    for i in partition:\n",
        "        pre, suf = (s[:i], s[i:])\n",
        "        if pre in arpabet and wordbreak(suf) is not None:\n",
        "            return [x+y for x,y in iterprod(arpabet[pre], wordbreak(suf))]\n",
        "    return None\n",
        "\n",
        "def normalized_sim(w1, w2):\n",
        "  return 1-edit_distance(w1,w2)/(max(len(w1), len(w2)))\n"
      ],
      "metadata": {
        "id": "Yzh1x4VTeOkJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee975d7a-8362-454d-b3e3-c7f901202a90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## example"
      ],
      "metadata": {
        "id": "5ncmloxIecIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = \"birds\"\n",
        "w2 = \"pigs\"\n",
        "print(\"wordbreak(w1)[0]:\",wordbreak(w1)[0])\n",
        "print(\"wordbreak(w2)[0]:\",wordbreak(w2)[0])\n",
        "\n",
        "print(\"orig phon:\", edit_distance(wordbreak(w1)[0],wordbreak(w2)[0]))\n",
        "print(\"orig orth:\", edit_distance(w1, w2))\n",
        "\n",
        "print(\"norm orth:\", normalized_sim(w1, w2))\n",
        "print(\"norm phon:\", normalized_sim(wordbreak(w1)[0],wordbreak(w2)[0]))"
      ],
      "metadata": {
        "id": "WZM8Ds6neRVU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e40ee3-13b5-4ca9-911a-8c3763c6f75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wordbreak(w1)[0]: ['B', 'ER1', 'D', 'Z']\n",
            "wordbreak(w2)[0]: ['P', 'IH1', 'G', 'Z']\n",
            "orig phon: 3\n",
            "orig orth: 3\n",
            "norm orth: 0.4\n",
            "norm phon: 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reading data"
      ],
      "metadata": {
        "id": "VhJ2xFIYegNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parentfolder = \"/content/drive/My Drive/IU-Abhilasha-Mike/Fluency/sem-phon/fluency_cogsci2022\"\n",
        "with tf.device('/device:GPU:0'):\n",
        "  semantic_files = glob.glob(parentfolder + '/*.xlsx')\n",
        "print(f\"This folder has {len(semantic_files)} files\")"
      ],
      "metadata": {
        "id": "faY0v_dpehPS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24cf4fca-6712-46aa-8f7b-c8389372c856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This folder has 1 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reading embeddings"
      ],
      "metadata": {
        "id": "U_amfhYwekDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "embeddings obtained from: http://vectors.nlpl.eu/explore/embeddings/en/models/"
      ],
      "metadata": {
        "id": "e7OeO3-OrlS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import embeddings\n",
        "with tf.device('/device:GPU:0'):\n",
        "  word2vec_model = pd.read_csv(parentfolder +\"vocab_vectors.csv\", encoding=\"unicode-escape\")\n",
        "  word2vec = word2vec_model.transpose().values\n",
        "  # new_header = word2vec.iloc[0] #grab the first row for the header\n",
        "  # word2vec = word2vec[1:] #take the data less the header row\n",
        "  # word2vec.columns = new_header\n",
        "  # word2vec = word2vec.values.transpose()\n",
        "  vocab = pd.DataFrame(word2vec_model.columns, columns=[\"vocab_word\"])\n",
        "  print(f\"embeddings are shaped:\", word2vec.shape)\n",
        "  print(f\"vocab is {len(vocab)} words\")"
      ],
      "metadata": {
        "id": "f3aFPCCZelEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e158ff-390d-463a-fa90-2c83c05efa96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddings are shaped: (4924, 300)\n",
            "vocab is 4924 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# obtaining phonemic & semantic similarity"
      ],
      "metadata": {
        "id": "ILHNvQUMemLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we compute the phonemic & semantics similarity for the data, as well the similarity drop-based switch designations based on the semantic similarities."
      ],
      "metadata": {
        "id": "CikhMo2gNuAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## now we loop through each txt file\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "phon_list = []\n",
        "\n",
        "# read in the data as a pandas dataframe\n",
        "data_file = pd.read_excel(semantic_files[0])\n",
        "import re\n",
        "for index, row in data_file.iterrows():  \n",
        "  word = str(row[\"response\"])\n",
        "  ## remove all non-alphas for phonemic similarity\n",
        "  mod_word = re.sub('[^a-zA-Z]+', '', word)\n",
        "  if(len(mod_word)>0):\n",
        "    phonemes = wordbreak(mod_word)[0]\n",
        "    phon_list.append(phonemes)\n",
        "  else:\n",
        "    phon_list.append(\"wordnotfound\")\n",
        "\n",
        "data_file[\"phonemes\"] = phon_list\n",
        "data_file[\"response_number\"] = data_file.groupby(['subject', 'domain']).cumcount()+1\n",
        "\n",
        "# exclude rows that do not have a valid phoneme\n",
        "data_file = data_file[data_file.phonemes != \"wordnotfound\"]\n",
        "data_file = data_file.reset_index(drop= True)\n",
        "\n",
        "#now we compute the levenshtein edit distance as a measure of orthographic/phonemic similarity \n",
        "\n",
        "phon_similarity = []\n",
        "orth_similarity = []\n",
        "semantic_similarity = []\n",
        "\n",
        "norm_phon = []\n",
        "norm_orth = []\n",
        "\n",
        "for index, row in data_file.iterrows():\n",
        "  current_word = re.sub('[^a-zA-Z]+', '', str(row[\"response\"]))\n",
        "  current_phoneme = row[\"phonemes\"]\n",
        "  if row[\"response_number\"] == 1:\n",
        "      sem_val = -999\n",
        "      phon_val = -999\n",
        "      orth_val = -999\n",
        "      norm_phon_val = -999\n",
        "      norm_orth_val = -999\n",
        "  else:\n",
        "    previous_word = re.sub('[^a-zA-Z]+', '', str(data_file.response[index-1]))\n",
        "    previous_phoneme = data_file.phonemes[index-1]\n",
        "\n",
        "    #calculate orthographic similarity as Levenshtein (edit) distance\n",
        "    orth_val = edit_distance(previous_word, current_word)\n",
        "    norm_orth_val = normalized_sim(previous_word, current_word)\n",
        "    \n",
        "    # can also get edit distance for the phonemes themselves (as in Siew et al. Hoosier network)\n",
        "\n",
        "    phon_val = edit_distance(previous_phoneme, current_phoneme)\n",
        "    norm_phon_val = normalized_sim(previous_phoneme, current_phoneme)\n",
        "    \n",
        "    # extract word embedding for current word\n",
        "    ## no replacements/removals here\n",
        "    current_word = str(row[\"response\"])\n",
        "    previous_word = str(data_file.response[index-1])\n",
        "    if current_word in list(vocab[\"vocab_word\"]):\n",
        "      current_word_index = list(vocab[\"vocab_word\"]).index(current_word)\n",
        "      current_word_vec = word2vec[current_word_index].reshape((1,word2vec.shape[1]))\n",
        "      # extract word embedding for current word\n",
        "      if previous_word in list(vocab[\"vocab_word\"]):\n",
        "        previous_word_index = list(vocab[\"vocab_word\"]).index(previous_word)\n",
        "        previous_word_vec = word2vec[previous_word_index].reshape((1,word2vec.shape[1]))\n",
        "        sem_val = float((1 - scipy.spatial.distance.cdist(previous_word_vec, current_word_vec, 'cosine'))[0])      \n",
        "        #print(f\"for {current_word} and {previous_word} similarity is {sem_val}\")\n",
        "      else:\n",
        "        sem_val = \"NA\"\n",
        "    else:\n",
        "      sem_val = \"NA\"\n",
        "\n",
        "  phon_similarity.append(phon_val)\n",
        "  orth_similarity.append(orth_val)\n",
        "  norm_phon.append(norm_phon_val)\n",
        "  norm_orth.append(norm_orth_val)\n",
        "  semantic_similarity.append(sem_val)\n",
        "  \n",
        "data_file[\"phon_similarity\"] = phon_similarity\n",
        "data_file[\"orth_similarity\"] = orth_similarity\n",
        "data_file[\"norm_phon\"] = norm_phon\n",
        "data_file[\"norm_orth\"] = norm_orth\n",
        "data_file[\"word2vec_similarity\"] = semantic_similarity \n",
        "\n",
        "data_file.to_csv(parentfolder + f'precomputed_data.csv')"
      ],
      "metadata": {
        "id": "WqZLn2_Beljf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# similarity drop & troyer"
      ],
      "metadata": {
        "id": "Zx_Vo8TYP7cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parentfolder = \"/content/drive/My Drive/IU-Abhilasha-Mike/Fluency/sem-phon/fluency_cogsci2022/\"\n",
        "with tf.device('/device:GPU:0'):\n",
        "  precomputed_data = pd.read_csv(parentfolder +\"precomputed_data.csv\", encoding=\"unicode-escape\")\n",
        "  norms = pd.read_csv(parentfolder +\"troyernorms.csv\", encoding=\"unicode-escape\")"
      ],
      "metadata": {
        "id": "9Fa39hPKfWI-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlNBA4OWL8SL",
        "outputId": "5cad624a-e3ce-424c-e9ba-5f7d2bee1809"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Fish']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## obtain similarity drop and troyer switch values\n",
        "\n",
        "simdrop = []\n",
        "troyer = []\n",
        "for k, row in precomputed_data.iterrows():\n",
        "  # simdrop\n",
        "  if (k > 0 and k < (len(precomputed_data)-2)):\n",
        "    if (precomputed_data[\"word2vec_similarity\"][k+1] > precomputed_data[\"word2vec_similarity\"][k]) and (precomputed_data[\"word2vec_similarity\"][k-1] > precomputed_data[\"word2vec_similarity\"][k]):\n",
        "      simdrop.append(1)\n",
        "    else:\n",
        "      simdrop.append(0)\n",
        "  else:\n",
        "      simdrop.append(0)\n",
        "  \n",
        "  ## troyer\n",
        "  if(k >0  and k < (len(precomputed_data)-2)):\n",
        "    item1 = precomputed_data[\"response\"][k]\n",
        "    item2 = precomputed_data[\"response\"][k-1]\n",
        "    category1 = norms[norms['Animal'] == item1]['Category'].values.tolist()\n",
        "    category2 = norms[norms['Animal'] == item2]['Category'].values.tolist()\n",
        "    if len(list(set(category1) & set(category2)))== 0:\n",
        "        troyer.append(1)\n",
        "    else:\n",
        "        troyer.append(0)\n",
        "  else:\n",
        "     troyer.append(0) \n",
        "\n",
        "precomputed_data[\"simdrop\"] = simdrop\n",
        "precomputed_data[\"troyer\"] = troyer\n",
        "\n",
        "precomputed_data.to_csv(parentfolder + f'precomputed_data.csv', index=False)\n",
        "precomputed_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "YfQRnFs8ONhg",
        "outputId": "67df1ab3-7c89-4af2-ab7f-d3ee71d6302e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c1d7f897-dc01-47db-af70-ec71ef98339b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>dataset</th>\n",
              "      <th>subject</th>\n",
              "      <th>domain</th>\n",
              "      <th>response_number</th>\n",
              "      <th>response</th>\n",
              "      <th>response_onset_time</th>\n",
              "      <th>IRT</th>\n",
              "      <th>participant_designated_switch</th>\n",
              "      <th>phonemes</th>\n",
              "      <th>phon_similarity</th>\n",
              "      <th>orth_similarity</th>\n",
              "      <th>norm_phon</th>\n",
              "      <th>norm_orth</th>\n",
              "      <th>word2vec_similarity</th>\n",
              "      <th>simdrop</th>\n",
              "      <th>troyer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>LEA</td>\n",
              "      <td>50001</td>\n",
              "      <td>animals</td>\n",
              "      <td>1</td>\n",
              "      <td>horse</td>\n",
              "      <td>2.594</td>\n",
              "      <td>2.594000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>['HH', 'AO1', 'R', 'S']</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>LEA</td>\n",
              "      <td>50001</td>\n",
              "      <td>animals</td>\n",
              "      <td>2</td>\n",
              "      <td>pig</td>\n",
              "      <td>3.594</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>['P', 'IH1', 'G']</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.469811</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>LEA</td>\n",
              "      <td>50001</td>\n",
              "      <td>animals</td>\n",
              "      <td>3</td>\n",
              "      <td>bear</td>\n",
              "      <td>4.894</td>\n",
              "      <td>1.300000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>['B', 'EH1', 'R']</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.297824</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>LEA</td>\n",
              "      <td>50001</td>\n",
              "      <td>animals</td>\n",
              "      <td>4</td>\n",
              "      <td>cat</td>\n",
              "      <td>6.194</td>\n",
              "      <td>1.300000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>['K', 'AE1', 'T']</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.383209</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>LEA</td>\n",
              "      <td>50001</td>\n",
              "      <td>animals</td>\n",
              "      <td>5</td>\n",
              "      <td>dog</td>\n",
              "      <td>7.394</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>['D', 'AO1', 'G']</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.760946</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32338</th>\n",
              "      <td>32338</td>\n",
              "      <td>HJT</td>\n",
              "      <td>1099</td>\n",
              "      <td>sports</td>\n",
              "      <td>24</td>\n",
              "      <td>jump_rope</td>\n",
              "      <td>130.000</td>\n",
              "      <td>126.911765</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['JH', 'AH1', 'M', 'P', 'R', 'OW1', 'P']</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.186143</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32339</th>\n",
              "      <td>32339</td>\n",
              "      <td>HJT</td>\n",
              "      <td>1099</td>\n",
              "      <td>sports</td>\n",
              "      <td>25</td>\n",
              "      <td>hockey</td>\n",
              "      <td>163.000</td>\n",
              "      <td>160.838235</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['HH', 'AA1', 'K', 'IY0']</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.172121</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32340</th>\n",
              "      <td>32340</td>\n",
              "      <td>HJT</td>\n",
              "      <td>1099</td>\n",
              "      <td>sports</td>\n",
              "      <td>26</td>\n",
              "      <td>field_hockey</td>\n",
              "      <td>169.000</td>\n",
              "      <td>164.367647</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['F', 'IY1', 'L', 'D', 'HH', 'AA1', 'K', 'IY0']</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.345995</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32341</th>\n",
              "      <td>32341</td>\n",
              "      <td>HJT</td>\n",
              "      <td>1099</td>\n",
              "      <td>sports</td>\n",
              "      <td>27</td>\n",
              "      <td>lacrosse</td>\n",
              "      <td>171.000</td>\n",
              "      <td>168.220588</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['L', 'AH0', 'K', 'R', 'AO1', 'S']</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.430308</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32342</th>\n",
              "      <td>32342</td>\n",
              "      <td>HJT</td>\n",
              "      <td>1099</td>\n",
              "      <td>sports</td>\n",
              "      <td>28</td>\n",
              "      <td>bobsled_racing</td>\n",
              "      <td>198.000</td>\n",
              "      <td>194.294118</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['B', 'AA1', 'B', 'S', 'L', 'EH2', 'D', 'R', '...</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.037383</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32343 rows Ã— 17 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1d7f897-dc01-47db-af70-ec71ef98339b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c1d7f897-dc01-47db-af70-ec71ef98339b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c1d7f897-dc01-47db-af70-ec71ef98339b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Unnamed: 0 dataset  subject  ... word2vec_similarity  simdrop troyer\n",
              "0               0     LEA    50001  ...         -999.000000        0      0\n",
              "1               1     LEA    50001  ...            0.469811        0      0\n",
              "2               2     LEA    50001  ...            0.297824        1      1\n",
              "3               3     LEA    50001  ...            0.383209        0      1\n",
              "4               4     LEA    50001  ...            0.760946        0      0\n",
              "...           ...     ...      ...  ...                 ...      ...    ...\n",
              "32338       32338     HJT     1099  ...            0.186143        0      1\n",
              "32339       32339     HJT     1099  ...            0.172121        1      1\n",
              "32340       32340     HJT     1099  ...            0.345995        0      1\n",
              "32341       32341     HJT     1099  ...            0.430308        0      0\n",
              "32342       32342     HJT     1099  ...            0.037383        0      0\n",
              "\n",
              "[32343 rows x 17 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# computational search models"
      ],
      "metadata": {
        "id": "cKvNxY3we7_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we only consider the \"animals\" domain for the computational models. we start with a predefined list of 675 animals for which we create a semantic and phonological similarity matrix and obtain word frequency estimates."
      ],
      "metadata": {
        "id": "XRE_8HRugwqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# download animals list"
      ],
      "metadata": {
        "id": "KDmZm22lDaoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simlabels = pd.read_csv(parentfolder+ f'new_animals.csv', header=None).values.reshape(-1,).tolist()\n",
        "len(simlabels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0_hYk_MDaEA",
        "outputId": "41a8e5d3-3726-4d73-98eb-1d4a0ff38b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "771"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create semantic similarity matrix"
      ],
      "metadata": {
        "id": "-rOFw544fGad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## create similarity matrix and similarity labels file from whichever corpus you're using\n",
        "\n",
        "def create_sim_matrix(simlabels):\n",
        "  ## the vocab consists of ALL possible words in corpus, but we need only the \"animals\" subset here\n",
        "  ## we use the similarity_labels file from psyrev to constrain the vocab\n",
        "  animals_index = [list(vocab.vocab_word).index(lab) if lab in list(vocab.vocab_word) else -999 for lab in simlabels ]\n",
        "  animals_index = list(filter((-999).__ne__, animals_index))\n",
        "\n",
        "  ## now we restrict our vocab and embeddings to ONLY these animals\n",
        "  embeddings_small = word2vec[animals_index, :]\n",
        "  vocab_small = vocab.iloc[animals_index]\n",
        "  N = len(vocab_small)\n",
        "  print(f\"animals vocab is {N} words\")\n",
        "\n",
        "  # create semantic similarity matrix\n",
        "  matrix = 1-scipy.spatial.distance.cdist(embeddings_small, embeddings_small, 'cosine').reshape(-1)\n",
        "  matrix = matrix.reshape((N,N))\n",
        "  print(\"sim matrix has been created:\", matrix.shape)\n",
        "\n",
        "  w1_index = list(vocab_small.vocab_word).index(\"dolphin\")\n",
        "  w2_index = list(vocab_small.vocab_word).index(\"kitten\")\n",
        "  w3_index = list(vocab_small.vocab_word).index(\"whale\")\n",
        "\n",
        "  print(\"dolphin-kitten:\", matrix[w1_index, w2_index])\n",
        "  print(\"dolphin-whale:\", matrix[w1_index, w3_index])\n",
        "  print(\"dolphin-dolphin:\", matrix[w1_index, w1_index])\n",
        "\n",
        "  pd.DataFrame(matrix).to_csv(parentfolder + 'word2vec_sim_matrix.csv', index=False, header=False)\n",
        "  vocab_small.to_csv(parentfolder + 'word2vec_sim_labels.csv', index=False, header=False)\n",
        "\n",
        "  print(\"sim matrix has been saved to drive!\")\n",
        "\n",
        "create_sim_matrix(simlabels)\n"
      ],
      "metadata": {
        "id": "xvFN6weVe-vq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e8b5f0-8769-4024-89ab-9de71a09b310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "animals vocab is 771 words\n",
            "sim matrix has been created: (771, 771)\n",
            "dolphin-kitten: 0.37598666621788657\n",
            "dolphin-whale: 0.7711714974302292\n",
            "dolphin-dolphin: 1.0\n",
            "sim matrix has been saved to drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create phonological similarity matrix"
      ],
      "metadata": {
        "id": "v5HTHGsjfI_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_phon_matrix(vocab):\n",
        "  # takes in a list of labels and computes the phonological similarity matrix\n",
        "  vocabulary = vocab.copy()\n",
        "  N = len(vocabulary)\n",
        "  print(f\"vocab is {N} words\")\n",
        "  # replace all underscores (_) with space (\" \") to match with glove vectors/vocab\n",
        "  vocabulary = [re.sub('[^a-zA-Z]+', '', str(v)) for v in vocabulary]\n",
        "  print(f\"vocab now looks like:\", vocabulary[:5])\n",
        "  # create phonemic similarity matrix for the small vocab\n",
        "  pmatrix = np.array([normalized_sim(wordbreak(w1)[0], wordbreak(w2)[0]) for w1 in vocabulary for w2 in vocabulary]).reshape((N,N))\n",
        "  print(\"pmatrix has been created:\", pmatrix.shape)\n",
        "  print(pmatrix)\n",
        "  pd.DataFrame(pmatrix).to_csv(parentfolder + 'simlabels_phon_matrix.csv', index=False, header=False)  \n",
        "  print(\"phon matrix has been saved to drive!\")\n",
        "\n",
        "simlabels = pd.read_csv(parentfolder+'word2vec_sim_labels.csv', header=None).values.reshape(-1,).tolist()\n",
        "print(f\"simlabels is {len(simlabels)} items:\", simlabels[:5])\n",
        "create_phon_matrix(simlabels)\n"
      ],
      "metadata": {
        "id": "b2HouuHsfK4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db59faf-76bb-4f32-87ac-eb96cc90d306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "simlabels is 771 items: ['horse', 'pig', 'bear', 'cat', 'dog']\n",
            "vocab is 771 words\n",
            "vocab now looks like: ['horse', 'pig', 'bear', 'cat', 'dog']\n",
            "pmatrix has been created: (771, 771)\n",
            "[[1.         0.         0.25       ... 0.         0.33333333 0.125     ]\n",
            " [0.         1.         0.         ... 0.         0.         0.125     ]\n",
            " [0.25       0.         1.         ... 0.14285714 0.16666667 0.125     ]\n",
            " ...\n",
            " [0.         0.         0.14285714 ... 1.         0.14285714 0.        ]\n",
            " [0.33333333 0.         0.16666667 ... 0.14285714 1.         0.125     ]\n",
            " [0.125      0.125      0.125      ... 0.         0.125      1.        ]]\n",
            "phon matrix has been saved to drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.log(float(404))"
      ],
      "metadata": {
        "id": "tgNalkkUItTY",
        "outputId": "20eb8063-e6ab-4364-9359-5eea031492cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.0014148779611505"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get frequencies"
      ],
      "metadata": {
        "id": "mNgomwoNMI2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frequencies = pd.read_csv(parentfolder+ f'animals_frequencies.csv', header=None)\n",
        "print(len(frequencies))"
      ],
      "metadata": {
        "id": "pIkuXkoLMKkO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0c5ffe-945b-4a8c-e5e7-04b476379f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## define foraging models"
      ],
      "metadata": {
        "id": "e_NPzt9nfQOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For these models, the behavioral data is read in as .txt files separated by a space"
      ],
      "metadata": {
        "id": "kNsBt3GyOiQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def modelFits(path, delimiter):\n",
        "\n",
        "    ### LOAD REQUIRED PACKAGES ###\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import re\n",
        "\n",
        "    ### LOAD BEHAVIORAL DATA ###\n",
        "    df = pd.read_csv(path, header=None, names=['SID', 'entry'], delimiter=delimiter)\n",
        "\n",
        "    #correct behavioral fits\n",
        "    df = forage.prepareData(df)\n",
        "\n",
        "    ### LOAD SEMANTIC SIMILARITY MATRIX ###\n",
        "\n",
        "    # (aka 'local cues', here we use cosines from word2vec)\n",
        "\n",
        "    # Similarity labels\n",
        "    simlab = []\n",
        "    ofile = open(parentfolder + 'word2vec_sim_labels.csv','r')#TODO:\n",
        "    for line in ofile:\n",
        "        labs = line.split()\n",
        "        for lab in labs:\n",
        "            simlab.append(lab)\n",
        "    ofile.close()\n",
        "\n",
        "    # Similarity values\n",
        "    simval = np.zeros((len(simlab), len(simlab)))\n",
        "    ofile = open(parentfolder + 'word2vec_sim_matrix.csv', 'r')#TODO:\n",
        "    j=0\n",
        "    for line in ofile:\n",
        "        line = re.sub(',\\n', '', line)\n",
        "        sims = line.split(',')\n",
        "        i=0\n",
        "        for sim in sims:\n",
        "            simval[i,j] = sim\n",
        "            i+=1\n",
        "        j+=1\n",
        "    ofile.close()\n",
        "\n",
        "    # Make sure similarity values are non-zero\n",
        "    for i in range(0,len(simval)):\n",
        "        for j in range(0,len(simval)):\n",
        "            if simval[i,j] <= 0:\n",
        "                simval[i,j] = 0.0001\n",
        "\n",
        "    ## PHONEMIC SIMILARTY VALUES ##\n",
        "    phonval = np.zeros((len(simlab), len(simlab)))\n",
        "    ofile = open(parentfolder + 'simlabels_phon_matrix.csv', 'r')#TODO:\n",
        "    j=0\n",
        "    for line in ofile:\n",
        "        line = re.sub(',\\n', '', line)\n",
        "        sims = line.split(',')\n",
        "        i=0\n",
        "        for sim in sims:\n",
        "            phonval[i,j] = sim\n",
        "            i+=1\n",
        "        j+=1\n",
        "    ofile.close()\n",
        "\n",
        "    # Make sure phonemic values are non-zero\n",
        "    for i in range(0,len(phonval)):\n",
        "        for j in range(0,len(phonval)):\n",
        "            if phonval[i,j] <= 0:\n",
        "                phonval[i,j] = 0.0001\n",
        "\n",
        "    ### LOAD FREQUENCY LIST ###\n",
        "    # (aka 'global cue', using NOW corpus from http://corpus.byu.edu/now/, 4.2 billion words and growing daily)\n",
        "\n",
        "    freqlab = []\n",
        "    freqval = []\n",
        "    ofile = open(parentfolder + 'animals_frequencies.csv', 'r') #TODO:\n",
        "    for line in ofile:\n",
        "        line = re.sub('\\n', '', line)\n",
        "        freqs=line.split(',')\n",
        "        freqlab.append(freqs[0])\n",
        "        ## append log of frequency if using psyrev\n",
        "        #freqval.append(np.log(float(freqs[0])))\n",
        "        freqval.append(float(freqs[1]))\n",
        "    ofile.close()\n",
        "    freqval=np.array(freqval)\n",
        "\n",
        "    sidlist = list(set(df['SID']))\n",
        "    full_entdf = pd.DataFrame()\n",
        "    full_fitlist = []\n",
        "    ct = 0\n",
        "\n",
        "    ## COMPUTE CONSECUTIVE SIMILARITY AND FREQUENCY AT SUBJECT LEVEL ##\n",
        "\n",
        "    for sid in sidlist:\n",
        "        ct+=1\n",
        "        print( \"SUBJECT \" + str(ct) + '/' + str(len(sidlist)) + \" \" + str(sid))\n",
        "\n",
        "        # My general initializations\n",
        "        myfitlist = []\n",
        "        myentries = np.array(df[df['SID']==sid]['entry'])\n",
        "        #print(\"myentries:\", myentries)\n",
        "        myenttimes = np.array(df[df['SID']==sid].index)\n",
        "        ##print(\"myenttimes:\", myenttimes)\n",
        "        myused = []\n",
        "        mytime = []\n",
        "\n",
        "        # For both frequency and similarity metrics:\n",
        "            # LIST: Metrics corresponding with my observed entries\n",
        "            # CURRENT: Full metric values, with observed entries becoming 0\n",
        "            # HISTORY: State of full metric values (ie, \"current\" during each entry)\n",
        "\n",
        "        # My frequency initializations\n",
        "        # freq current contains frequencies of ALL the words in corpus\n",
        "        freq_current = np.array(freqval)\n",
        "        #print(\"freq_current.shape:\",freq_current.shape)\n",
        "        freq_list = []\n",
        "        freq_history = []\n",
        "\n",
        "        # My similarity initializations\n",
        "        sim_current = simval.copy()\n",
        "        # sim_current contains the full NxN similarity matrix\n",
        "        #print(\"sim_current shape:\",sim_current.shape)\n",
        "        sim_list = []\n",
        "        sim_history = []\n",
        "\n",
        "        phon_current = phonval.copy()\n",
        "        phon_list = []\n",
        "        phon_history = []\n",
        "        troyer_switch = []\n",
        "\n",
        "        for i in range(0,len(myentries)):\n",
        "            word = myentries[i]\n",
        "            #if word not in myused: # use this to calculate number of correct responses w/out repeats\n",
        "            if True:   # use this line instead of former to include repeated words along w/line 110,119 comment out\n",
        "\n",
        "                # Frequency: Get frequency and update relevant lists\n",
        "                freq_list.append( float(freq_current[freqlab.index(word)]) )\n",
        "                freq_history.append( np.array(freq_current) )\n",
        "                #freq_current[freqlab.index(word)] = 0.00000001\n",
        "\n",
        "                # Get similarity between this word and preceding word\n",
        "                if i > 0:         \n",
        "                    sim_list.append( float(sim_current[simlab.index(myentries[i-1]), simlab.index(word)]) )\n",
        "                    sim_history.append( np.array(sim_current[simlab.index(myentries[i-1]),:]) )\n",
        "\n",
        "                    phon_list.append( float(phon_current[simlab.index(myentries[i-1]), simlab.index(word)]) )\n",
        "                    phon_history.append( np.array(phon_current[simlab.index(myentries[i-1]),:]) )\n",
        "\n",
        "                    category1 = norms[norms['Animal'] == myentries[i-1]]['Category'].values.tolist()\n",
        "                    category2 = norms[norms['Animal'] == myentries[i]]['Category'].values.tolist()\n",
        "                    if len(list(set(category1) & set(category2)))== 0:\n",
        "                        troyer_switch.append(1)\n",
        "                    else:\n",
        "                        troyer_switch.append(0)\n",
        "\n",
        "                else:\n",
        "                    sim_list.append(0)\n",
        "                    sim_history.append( np.array(sim_current[simlab.index(word),:]) )\n",
        "                #sim_current[:,simlab.index(word)] = 0.00000001\n",
        "                    phon_list.append(0)\n",
        "                    phon_history.append( np.array(phon_current[simlab.index(word),:]) )\n",
        "\n",
        "                    troyer_switch.append(0)\n",
        "\n",
        "                # Update lists\n",
        "                myused.append(word)\n",
        "                mytime.append(myenttimes[i])\n",
        "\n",
        "        # Calculate category switches, based on similarity-drop\n",
        "        myswitch = np.zeros(len(myused)).astype(int)\n",
        "        for i in range(1,len(myused)-1):\n",
        "            if (sim_list[i+1] > sim_list[i]) and (sim_list[i-1] > sim_list[i]):\n",
        "                myswitch[i] = 1\n",
        "\n",
        "        # Save my entries with corresponding metrics\n",
        "        mydf = pd.DataFrame({'sid':[sid]*len(myused) , 'ent':myused, 'freq':freq_list, 'sim':sim_list, 'phon': phon_list,\n",
        "                             'simdropswitch':myswitch, 'troyer': troyer_switch, 'time':mytime},\n",
        "                            columns=['sid','time','ent','freq','sim', 'phon', 'simdropswitch', 'troyerswitch'])\n",
        "        full_entdf = full_entdf.append(mydf)\n",
        "        # Get parameter fits for the different models\n",
        "        myfitlist.append(sid)\n",
        "        myfitlist.append(len(myused))\n",
        "        ## obtaining the optimal/random fits for the static and dynamic model by calling the getFits function\n",
        "        myfitlist.extend( forage.getfits(freq_list, freq_history, sim_list, sim_history, phon_list, phon_history, troyer_switch) )\n",
        "        full_fitlist.append(myfitlist)\n",
        "\n",
        "    print(\"Fits Complete.\")\n",
        "\n",
        "    # create results directory if it doesn't exist yet\n",
        "    if not os.path.exists(parentfolder):\n",
        "        os.makedirs(parentfolder)\n",
        "\n",
        "    # # Output data entries with corresponding metrics for visualization in R\n",
        "    print(full_entdf)\n",
        "    full_entdf = full_entdf.reset_index(drop=True)\n",
        "    full_entdf.to_csv(parentfolder  + 'newdata-fullmetrics.csv', index=False, header=True)\n",
        "\n",
        "    # # Output parameter & model fits\n",
        "    full_fitlist = pd.DataFrame(full_fitlist)\n",
        "    full_fitlist.columns = ['subject', 'number_of_items',\n",
        "                             \n",
        "                            'beta_static_frequency', 'beta_static_semantic', 'errors_static_optimal', 'errors_static_random',\n",
        "                            'beta_plocalstatic_frequency', 'beta_plocalstatic_semantic', 'beta_plocalstatic_phonemic','errors_plocalstatic_optimal', 'errors_plocalstatic_random',\n",
        "                            \n",
        "                            'beta_simdrop_dynamic_frequency', 'beta_simdrop_dynamic_semantic', 'errors_simdrop_dynamic_optimal', 'errors_simdrop_dynamic_random',\n",
        "                            'beta_simdrop_pswitchonlydynamic_frequency', 'beta_simdrop_pswitchonlydynamic_semantic', 'beta_simdrop_pswitchonlydynamic_phonemic','errors_simdrop_pswitchonlydynamic_optimal', 'errors_simdrop_pswitchonlydynamic_random',\n",
        "\n",
        "                            'beta_simdrop_plocaldynamic_frequency', 'beta_simdrop_plocaldynamic_semantic', 'beta_simdrop_plocaldynamic_phonemic','errors_simdrop_plocaldynamic_optimal', 'errors_simdrop_plocaldynamic_random',\n",
        "                            'beta_simdrop_pglobaldynamic_frequency', 'beta_simdrop_pglobaldynamic_semantic', 'beta_simdrop_pglobaldynamic_phonemic','errors_simdrop_pglobaldynamic_optimal', 'errors_simdrop_pglobaldynamic_random',\n",
        "\n",
        "                            'beta_troyer_dynamic_frequency', 'beta_troyer_dynamic_semantic', 'errors_troyer_dynamic_optimal', 'errors_troyer_dynamic_random',\n",
        "                            'beta_troyer_pswitchonlydynamic_frequency', 'beta_troyer_pswitchonlydynamic_semantic', 'beta_troyer_pswitchonlydynamic_phonemic','errors_troyer_pswitchonlydynamic_optimal', 'errors_troyer_pswitchonlydynamic_random',\n",
        "\n",
        "                            'beta_troyer_plocaldynamic_frequency', 'beta_troyer_plocaldynamic_semantic', 'beta_troyer_plocaldynamic_phonemic','errors_troyer_plocaldynamic_optimal', 'errors_troyer_plocaldynamic_random',\n",
        "                            'beta_troyer_pglobaldynamic_frequency', 'beta_troyer_pglobaldynamic_semantic', 'beta_troyer_pglobaldynamic_phonemic','errors_troyer_pglobaldynamic_optimal', 'errors_troyer_pglobaldynamic_random'\n",
        "                            ]\n",
        "\n",
        "    #print(\"full_fitlist:\",full_fitlist)\n",
        "    full_fitlist.to_csv(parentfolder  + 'newdata-fullfits.csv', index=False, header=True)\n",
        "\n",
        "    print(full_fitlist.head())\n",
        "    print(\"Results saved to '\" + parentfolder + \"'.\")\n",
        "\n",
        "class forage:\n",
        "\n",
        "    def prepareData(data):\n",
        "        import pandas as pd\n",
        "        import re\n",
        "        # load similarity labels\n",
        "        simlab = []\n",
        "        ofile = open(parentfolder + 'word2vec_sim_labels.csv','r')\n",
        "        for line in ofile:\n",
        "            labs = line.split()\n",
        "            for lab in labs:\n",
        "                simlab.append(lab)\n",
        "        ofile.close()\n",
        "\n",
        "        ### LOAD CORRECTIONS ###\n",
        "        # This is a look-up list that maps incorrect words onto accepted words that are in the database\n",
        "        # corrections = pd.read_csv(parentfolder + 'corrections.txt', header=None, delimiter='\\t')\n",
        "        # corrections = corrections.set_index(corrections[0].values)\n",
        "        # corrections.columns = ['_from','_to']\n",
        "\n",
        "        elist = data['entry'].values\n",
        "        newlist = []\n",
        "        notfound = []\n",
        "\n",
        "        # Use look-up table to check and correct observed entries\n",
        "        for ent in elist:\n",
        "            ent = re.sub(r'\\W+', '', ent) # Alphanumericize it\n",
        "            if ent in simlab:\n",
        "                # If this entry is appropriate, keep it\n",
        "                newlist.append(ent)\n",
        "            # elif ent[0:len(ent)-1] in simlab:\n",
        "            #     # If this entry is plural, correct to the singular verion\n",
        "            #     print(f\"found the entry {ent[0:len(ent)-1]} in simlab\")\n",
        "            #     newlist.append(ent[0:len(ent)-1])\n",
        "            # elif ent in corrections._from:\n",
        "            #     # If this entry is correctable, correct it\n",
        "            #     newlist.append(corrections.loc[ent]._to)\n",
        "            else:\n",
        "                # If this entry is not found in either list, mark for removal and warn user.\n",
        "                newlist.append('NA')\n",
        "                notfound.append(ent)\n",
        "\n",
        "        # Remove the rows with inappropriate entries\n",
        "        data.entry = newlist\n",
        "        data = data[data.entry!='NA']\n",
        "\n",
        "        # Warn the user of removed entries\n",
        "        if len(notfound) > 0:\n",
        "            print('The following items were not found in the database, and were removed: [' +\n",
        "                  str(len(notfound)) + ' entries removed] \\n')\n",
        "            print(sorted(set(notfound)))\n",
        "        else:\n",
        "            print('All items OK.')\n",
        "        return data[data.entry!='NA']\n",
        "        # TODO: return statement might not be necessary...\n",
        "\n",
        "    def model_static(beta, freql, freqh, siml, simh):\n",
        "        ## beta contains the optimization parameters for frequency (beta[0]) and semantic similarity (beta[1])\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0: # if first item then its probability is based on just frequency\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else: # if not first item then its probability is based on its similarity to prev item AND frequency\n",
        "            # P of item based on frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat) # negative Log likelihood of this item: this will be minimized eventually\n",
        "        return ct\n",
        "\n",
        "    def model_static_plocal(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "        ## beta contains the optimization parameters for frequency (beta[0]) and semantic similarity (beta[1])\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0: # if first item then its probability is based on just frequency\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else: # if not first item then its probability is based on its similarity to prev item AND frequency AND phonemic similarity\n",
        "            # P of item based on frequency and similarity and phonology\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) * pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2])* pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat) # negative Log likelihood of this item: this will be minimized eventually\n",
        "        return ct\n",
        "        \n",
        "    def model_dynamic_psyrev_simdrop(beta, freql, freqh, siml, simh):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based again on frequency\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "      \n",
        "    def model_dynamic_pswitchonly_simdrop(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based on a combination of frequency and phonemic similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2]) )\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "    \n",
        "\n",
        "    def model_dynamic_plocal_simdrop(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based on frequency \n",
        "                numrat = pow(freql[k],beta[0]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(phonl[k],beta[2])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(phonh[k],beta[2])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "\n",
        "    def model_dynamic_pglobal_simdrop(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based on a combination of frequency and phonemic similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2]) )\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(phonl[k],beta[2])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(phonh[k],beta[2])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "\n",
        "    ## TROYER fits : switch in dynamic models is based on troyer norms NOT similarity drop\n",
        "\n",
        "    def model_dynamic_psyrev_troyer(beta, freql, freqh, siml, simh, troyer):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif troyer[k] == 1:## switch based on if troyer category changes\n",
        "            # If similarity dips, P of item is based again on frequency\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "      \n",
        "    def model_dynamic_pswitchonly_troyer(beta, freql, freqh, siml, simh, phonl, phonh, troyer):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif troyer[k] == 1: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based on a combination of frequency and phonemic similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2]) )\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "    \n",
        "\n",
        "    def model_dynamic_plocal_troyer(beta, freql, freqh, siml, simh, phonl, phonh, troyer):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif troyer[k] == 1:\n",
        "            # If similarity dips, P of item is based on frequency \n",
        "                numrat = pow(freql[k],beta[0]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(phonl[k],beta[2])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(phonh[k],beta[2])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "\n",
        "    def model_dynamic_pglobal_troyer(beta, freql, freqh, siml, simh, phonl, phonh, troyer):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif troyer[k] == 1:\n",
        "            # If similarity dips, P of item is based on a combination of frequency and phonemic similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2]) )\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(phonl[k],beta[2])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(phonh[k],beta[2])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "    \n",
        "\n",
        "    def getfits( freq_l, freq_h, sim_l, sim_h, phon_l, phon_h , troyer):\n",
        "        import numpy as np\n",
        "        from scipy.optimize import fmin\n",
        "    #fmin: Uses a Nelder-Mead simplex algorithm to find the minimum of function of variables.\n",
        "        r1 = np.random.rand()\n",
        "        r2 = np.random.rand()\n",
        "        r3 = np.random.rand()\n",
        "\n",
        "    # STATIC MODEL (no dynamic switching, just focusing on two cues with some weights)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_static, [r1, r2], args=(freq_l, freq_h, sim_l, sim_h), ftol = 0.001, disp=False)\n",
        "        beta_static_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_static_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_static = forage.model_static([beta_static_freq, beta_static_semantic], freq_l, freq_h, sim_l, sim_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_static = forage.model_static([0, 0], freq_l, freq_h, sim_l, sim_h)\n",
        "    # LOCAL PHONEMIC CUE, STATIC MODEL (no dynamic switching, just focusing on two cues with some weights)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_static_plocal, [r1, r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_static_plocal_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_static_plocal_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_static_plocal_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_plocalstatic = forage.model_static_plocal([beta_static_plocal_freq, beta_static_plocal_semantic, beta_static_plocal_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_plocalstatic = forage.model_static_plocal([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "    \n",
        "  \n",
        "    # SIMDROP DYNAMIC MODEL (switches dynamically between cues)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_psyrev_simdrop, [r1,r2], args=(freq_l, freq_h, sim_l, sim_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_simdrop_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_simdrop_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_dynamic_simdrop = forage.model_dynamic_psyrev_simdrop([beta_dynamic_simdrop_freq, beta_dynamic_simdrop_semantic], freq_l, freq_h, sim_l, sim_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_dynamic_simdrop = forage.model_dynamic_psyrev_simdrop([0,0], freq_l, freq_h, sim_l, sim_h)\n",
        "\n",
        "    # SIMDROP DYNAMIC PHON SWITCH ONLY MODEL (switches dynamically between cues, phonology is a GLOBAL cue with frequency)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_pswitchonly_simdrop, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_pswitchonly_simdrop_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_pswitchonly_simdrop_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_pswitchonly_simdrop_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_pswitchonlydynamic_simdrop = forage.model_dynamic_pswitchonly_simdrop([beta_dynamic_pswitchonly_simdrop_freq, beta_dynamic_pswitchonly_simdrop_semantic, beta_dynamic_pswitchonly_simdrop_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_pswitchonlydynamic_simdrop = forage.model_dynamic_pswitchonly_simdrop([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "          \n",
        "\n",
        "      # SIMDROP DYNAMIC PHON LOCAL MODEL (switches dynamically between cues, phonology,semantic, freq is a LOCAL cue)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_plocal_simdrop, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_plocal_simdrop_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_plocal_simdrop_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_plocal_simdrop_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_plocaldynamic_simdrop = forage.model_dynamic_plocal_simdrop([beta_dynamic_plocal_simdrop_freq, beta_dynamic_plocal_simdrop_semantic, beta_dynamic_plocal_simdrop_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_plocaldynamic_simdrop = forage.model_dynamic_plocal_simdrop([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "     # SIMDROP DYNAMIC PHON GLOBAL MODEL (switches dynamically between cues, phonology is a GLOBAL cue with frequency)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_pglobal_simdrop, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_pglobal_simdrop_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_pglobal_simdrop_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_pglobal_simdrop_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_pglobaldynamic_simdrop = forage.model_dynamic_pglobal_simdrop([beta_dynamic_pglobal_simdrop_freq, beta_dynamic_pglobal_simdrop_semantic, beta_dynamic_pglobal_simdrop_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_pglobaldynamic_simdrop = forage.model_dynamic_pglobal_simdrop([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "\n",
        "        ## troyer dynamic models\n",
        "\n",
        "        # TROYER DYNAMIC MODEL (switches dynamically between cues)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_psyrev_troyer, [r1,r2], args=(freq_l, freq_h, sim_l, sim_h, troyer), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_troyer_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_troyer_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_dynamic_troyer = forage.model_dynamic_psyrev_troyer([beta_dynamic_troyer_freq, beta_dynamic_troyer_semantic], freq_l, freq_h, sim_l, sim_h, troyer)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_dynamic_troyer = forage.model_dynamic_psyrev_troyer([0,0], freq_l, freq_h, sim_l, sim_h, troyer)\n",
        "\n",
        "    # TROYER DYNAMIC PHON SWITCH ONLY MODEL (switches dynamically between cues, phonology is a GLOBAL cue with frequency)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_pswitchonly_troyer, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h, troyer), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_pswitchonly_troyer_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_pswitchonly_troyer_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_pswitchonly_troyer_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_pswitchonlydynamic_troyer = forage.model_dynamic_pswitchonly_troyer([beta_dynamic_pswitchonly_troyer_freq, beta_dynamic_pswitchonly_troyer_semantic, beta_dynamic_pswitchonly_troyer_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h, troyer)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_pswitchonlydynamic_troyer = forage.model_dynamic_pswitchonly_troyer([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h, troyer)\n",
        "          \n",
        "\n",
        "      # TROYER DYNAMIC PHON LOCAL MODEL (switches dynamically between cues, phonology,semantic, freq is a LOCAL cue)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_plocal_troyer, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h, troyer), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_plocal_troyer_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_plocal_troyer_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_plocal_troyer_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_plocaldynamic_troyer = forage.model_dynamic_plocal_troyer([beta_dynamic_plocal_troyer_freq, beta_dynamic_plocal_troyer_semantic, beta_dynamic_plocal_troyer_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h, troyer)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_plocaldynamic_troyer = forage.model_dynamic_plocal_troyer([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h, troyer)\n",
        "\n",
        "     # TROYER DYNAMIC PHON GLOBAL MODEL (switches dynamically between cues, phonology is a GLOBAL cue with frequency)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_pglobal_troyer, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h, troyer), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_pglobal_troyer_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_pglobal_troyer_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_pglobal_troyer_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_pglobaldynamic_troyer = forage.model_dynamic_pglobal_troyer([beta_dynamic_pglobal_troyer_freq, beta_dynamic_pglobal_troyer_semantic, beta_dynamic_pglobal_troyer_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h, troyer)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_pglobaldynamic_troyer = forage.model_dynamic_pglobal_troyer([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h, troyer)\n",
        "      \n",
        "\n",
        "        results = [ beta_static_freq, beta_static_semantic, float(optimal_fit_static), float(random_fit_static),\n",
        "                   beta_static_plocal_freq, beta_static_plocal_semantic, beta_static_plocal_phonemic, float(optimal_fit_plocalstatic), float(random_fit_plocalstatic),\n",
        "                  \n",
        "\n",
        "                   beta_dynamic_simdrop_freq, beta_dynamic_simdrop_semantic, float(optimal_fit_dynamic_simdrop), float(random_fit_dynamic_simdrop),\n",
        "                   beta_dynamic_pswitchonly_simdrop_freq, beta_dynamic_pswitchonly_simdrop_semantic, beta_dynamic_pswitchonly_simdrop_phonemic, float(optimal_fit_pswitchonlydynamic_simdrop), float(random_fit_pswitchonlydynamic_simdrop),\n",
        "\n",
        "                   beta_dynamic_plocal_simdrop_freq, beta_dynamic_plocal_simdrop_semantic, beta_dynamic_plocal_simdrop_phonemic, float(optimal_fit_plocaldynamic_simdrop), float(random_fit_plocaldynamic_simdrop),\n",
        "                   beta_dynamic_pglobal_simdrop_freq, beta_dynamic_pglobal_simdrop_semantic, beta_dynamic_pglobal_simdrop_phonemic, float(optimal_fit_pglobaldynamic_simdrop), float(random_fit_pglobaldynamic_simdrop),\n",
        "\n",
        "                   beta_dynamic_troyer_freq, beta_dynamic_troyer_semantic, float(optimal_fit_dynamic_troyer), float(random_fit_dynamic_troyer),\n",
        "                   beta_dynamic_pswitchonly_troyer_freq, beta_dynamic_pswitchonly_troyer_semantic, beta_dynamic_pswitchonly_troyer_phonemic, float(optimal_fit_pswitchonlydynamic_troyer), float(random_fit_pswitchonlydynamic_troyer),\n",
        "\n",
        "                   beta_dynamic_plocal_troyer_freq, beta_dynamic_plocal_troyer_semantic, beta_dynamic_plocal_troyer_phonemic, float(optimal_fit_plocaldynamic_troyer), float(random_fit_plocaldynamic_troyer),\n",
        "                   beta_dynamic_pglobal_troyer_freq, beta_dynamic_pglobal_troyer_semantic, beta_dynamic_pglobal_troyer_phonemic, float(optimal_fit_pglobaldynamic_troyer), float(random_fit_pglobaldynamic_troyer)\n",
        "\n",
        "                   ]\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "lP1sdQE5fR2L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run foraging models"
      ],
      "metadata": {
        "id": "1LC8wg1yfVnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datapath = parentfolder+'data-fluency.txt'\n",
        "modelFits(datapath, delimiter = \"\\t\")"
      ],
      "metadata": {
        "id": "hV8fHr10fS3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a36a20d6-0aed-4a23-f2b3-e93c936c233a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All items OK.\n",
            "SUBJECT 1/171 51\n",
            "SUBJECT 2/171 575\n",
            "SUBJECT 3/171 576\n",
            "SUBJECT 4/171 577\n",
            "SUBJECT 5/171 578\n",
            "SUBJECT 6/171 579\n",
            "SUBJECT 7/171 580\n",
            "SUBJECT 8/171 581\n",
            "SUBJECT 9/171 582\n",
            "SUBJECT 10/171 71\n",
            "SUBJECT 11/171 583\n",
            "SUBJECT 12/171 584\n",
            "SUBJECT 13/171 585\n",
            "SUBJECT 14/171 586\n",
            "SUBJECT 15/171 587\n",
            "SUBJECT 16/171 588\n",
            "SUBJECT 17/171 589\n",
            "SUBJECT 18/171 590\n",
            "SUBJECT 19/171 591\n",
            "SUBJECT 20/171 592\n",
            "SUBJECT 21/171 593\n",
            "SUBJECT 22/171 594\n",
            "SUBJECT 23/171 595\n",
            "SUBJECT 24/171 596\n",
            "SUBJECT 25/171 597\n",
            "SUBJECT 26/171 598\n",
            "SUBJECT 27/171 677\n",
            "SUBJECT 28/171 678\n",
            "SUBJECT 29/171 679\n",
            "SUBJECT 30/171 680\n",
            "SUBJECT 31/171 681\n",
            "SUBJECT 32/171 682\n",
            "SUBJECT 33/171 683\n",
            "SUBJECT 34/171 684\n",
            "SUBJECT 35/171 686\n",
            "SUBJECT 36/171 687\n",
            "SUBJECT 37/171 688\n",
            "SUBJECT 38/171 689\n",
            "SUBJECT 39/171 690\n",
            "SUBJECT 40/171 691\n",
            "SUBJECT 41/171 692\n",
            "SUBJECT 42/171 693\n",
            "SUBJECT 43/171 694\n",
            "SUBJECT 44/171 695\n",
            "SUBJECT 45/171 696\n",
            "SUBJECT 46/171 697\n",
            "SUBJECT 47/171 698\n",
            "SUBJECT 48/171 198\n",
            "SUBJECT 49/171 199\n",
            "SUBJECT 50/171 779\n",
            "SUBJECT 51/171 780\n",
            "SUBJECT 52/171 781\n",
            "SUBJECT 53/171 782\n",
            "SUBJECT 54/171 783\n",
            "SUBJECT 55/171 784\n",
            "SUBJECT 56/171 785\n",
            "SUBJECT 57/171 786\n",
            "SUBJECT 58/171 787\n",
            "SUBJECT 59/171 788\n",
            "SUBJECT 60/171 789\n",
            "SUBJECT 61/171 790\n",
            "SUBJECT 62/171 791\n",
            "SUBJECT 63/171 792\n",
            "SUBJECT 64/171 793\n",
            "SUBJECT 65/171 794\n",
            "SUBJECT 66/171 795\n",
            "SUBJECT 67/171 796\n",
            "SUBJECT 68/171 285\n",
            "SUBJECT 69/171 286\n",
            "SUBJECT 70/171 287\n",
            "SUBJECT 71/171 288\n",
            "SUBJECT 72/171 289\n",
            "SUBJECT 73/171 290\n",
            "SUBJECT 74/171 291\n",
            "SUBJECT 75/171 292\n",
            "SUBJECT 76/171 293\n",
            "SUBJECT 77/171 294\n",
            "SUBJECT 78/171 295\n",
            "SUBJECT 79/171 296\n",
            "SUBJECT 80/171 297\n",
            "SUBJECT 81/171 298\n",
            "SUBJECT 82/171 50001\n",
            "SUBJECT 83/171 50002\n",
            "SUBJECT 84/171 50003\n",
            "SUBJECT 85/171 50004\n",
            "SUBJECT 86/171 50006\n",
            "SUBJECT 87/171 50007\n",
            "SUBJECT 88/171 50008\n",
            "SUBJECT 89/171 50010\n",
            "SUBJECT 90/171 50011\n",
            "SUBJECT 91/171 50012\n",
            "SUBJECT 92/171 50013\n",
            "SUBJECT 93/171 50014\n",
            "SUBJECT 94/171 50016\n",
            "SUBJECT 95/171 50017\n",
            "SUBJECT 96/171 50018\n",
            "SUBJECT 97/171 50019\n",
            "SUBJECT 98/171 50020\n",
            "SUBJECT 99/171 50022\n",
            "SUBJECT 100/171 50023\n",
            "SUBJECT 101/171 50024\n",
            "SUBJECT 102/171 50025\n",
            "SUBJECT 103/171 50026\n",
            "SUBJECT 104/171 50027\n",
            "SUBJECT 105/171 50029\n",
            "SUBJECT 106/171 50030\n",
            "SUBJECT 107/171 50031\n",
            "SUBJECT 108/171 50032\n",
            "SUBJECT 109/171 50033\n",
            "SUBJECT 110/171 50034\n",
            "SUBJECT 111/171 50036\n",
            "SUBJECT 112/171 379\n",
            "SUBJECT 113/171 380\n",
            "SUBJECT 114/171 381\n",
            "SUBJECT 115/171 382\n",
            "SUBJECT 116/171 383\n",
            "SUBJECT 117/171 384\n",
            "SUBJECT 118/171 385\n",
            "SUBJECT 119/171 386\n",
            "SUBJECT 120/171 387\n",
            "SUBJECT 121/171 388\n",
            "SUBJECT 122/171 389\n",
            "SUBJECT 123/171 390\n",
            "SUBJECT 124/171 391\n",
            "SUBJECT 125/171 392\n",
            "SUBJECT 126/171 393\n",
            "SUBJECT 127/171 394\n",
            "SUBJECT 128/171 395\n",
            "SUBJECT 129/171 396\n",
            "SUBJECT 130/171 397\n",
            "SUBJECT 131/171 398\n",
            "SUBJECT 132/171 893\n",
            "SUBJECT 133/171 994\n",
            "SUBJECT 134/171 995\n",
            "SUBJECT 135/171 996\n",
            "SUBJECT 136/171 1098\n",
            "SUBJECT 137/171 997\n",
            "SUBJECT 138/171 1099\n",
            "SUBJECT 139/171 896\n",
            "SUBJECT 140/171 998\n",
            "SUBJECT 141/171 897\n",
            "SUBJECT 142/171 999\n",
            "SUBJECT 143/171 898\n",
            "SUBJECT 144/171 797\n",
            "SUBJECT 145/171 899\n",
            "SUBJECT 146/171 798\n",
            "SUBJECT 147/171 474\n",
            "SUBJECT 148/171 475\n",
            "SUBJECT 149/171 476\n",
            "SUBJECT 150/171 477\n",
            "SUBJECT 151/171 478\n",
            "SUBJECT 152/171 479\n",
            "SUBJECT 153/171 480\n",
            "SUBJECT 154/171 481\n",
            "SUBJECT 155/171 482\n",
            "SUBJECT 156/171 483\n",
            "SUBJECT 157/171 484\n",
            "SUBJECT 158/171 485\n",
            "SUBJECT 159/171 486\n",
            "SUBJECT 160/171 487\n",
            "SUBJECT 161/171 488\n",
            "SUBJECT 162/171 489\n",
            "SUBJECT 163/171 490\n",
            "SUBJECT 164/171 491\n",
            "SUBJECT 165/171 492\n",
            "SUBJECT 166/171 493\n",
            "SUBJECT 167/171 494\n",
            "SUBJECT 168/171 495\n",
            "SUBJECT 169/171 496\n",
            "SUBJECT 170/171 497\n",
            "SUBJECT 171/171 498\n",
            "Fits Complete.\n",
            "    sid  time        ent   freq       sim      phon  simdropswitch troyerswitch\n",
            "0    51  1448        cat  3.529  0.000000  0.000000              0          NaN\n",
            "1    51  1449        dog  3.993  0.760946  0.000100              0          NaN\n",
            "2    51  1450      mouse  2.989  0.301437  0.000100              1          NaN\n",
            "3    51  1451        rat  3.221  0.465104  0.000100              0          NaN\n",
            "4    51  1452    giraffe  1.887  0.335645  0.250000              1          NaN\n",
            "..  ...   ...        ...    ...       ...       ...            ...          ...\n",
            "22  498  3733       deer  2.648  0.534654  0.000100              0          NaN\n",
            "23  498  3734  tarantula  1.672  0.262898  0.111111              0          NaN\n",
            "24  498  3735     Fergie  0.500  0.068397  0.000100              1          NaN\n",
            "25  498  3736   squirrel  2.447  0.148521  0.166667              0          NaN\n",
            "26  498  3737    hamster  2.041  0.451015  0.000100              0          NaN\n",
            "\n",
            "[6639 rows x 8 columns]\n",
            "   subject  ...  errors_troyer_pglobaldynamic_random\n",
            "0       51  ...                           232.669093\n",
            "1      575  ...                           146.249144\n",
            "2      576  ...                           398.861302\n",
            "3      577  ...                           319.089042\n",
            "4      578  ...                           252.612158\n",
            "\n",
            "[5 rows x 49 columns]\n",
            "Results saved to '/content/drive/My Drive/IU-Abhilasha-Mike/Fluency/sem-phon/fluency_cogsci2022/'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AAO0grZLPiSF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}