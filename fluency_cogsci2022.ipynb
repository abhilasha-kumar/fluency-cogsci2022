{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fluency-cogsci2022.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMjePpX4lHskaLduorL0em5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhilasha-kumar/fluency-cogsci2022/blob/main/fluency_cogsci2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phonological Intrusions in Semantic Memory Retrieval"
      ],
      "metadata": {
        "id": "YemaL37bd2-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing drive, GPU, and packages"
      ],
      "metadata": {
        "id": "MwyFlkqfflFH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ggNcnk34d2O4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46404b0a-539f-4108-cf90-53f8b62dc2d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "iYlJS26seDTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c5349f-b649-4d9f-b956-d4406104a028"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "Fri Jan  7 18:20:45 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    33W / 250W |    375MiB / 16280MiB |      4%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import heapq\n",
        "import itertools\n",
        "import scipy.spatial.distance\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from numpy.random import randint\n",
        "from scipy.special import softmax\n",
        "from sklearn.preprocessing import MinMaxScaler, normalize\n",
        "from numpy.linalg import matrix_power\n",
        "from functools import lru_cache\n",
        "import glob\n",
        "from scipy.special import expit\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from functools import lru_cache\n",
        "from itertools import product as iterprod\n",
        "import itertools\n",
        "from nltk.metrics import *\n",
        "\n"
      ],
      "metadata": {
        "id": "7f1RA_0MeKWo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phoneme Function"
      ],
      "metadata": {
        "id": "PxeaTfrGeNla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# algo to obtain phonemes for any given strng\n",
        "# obtained from: https://stackoverflow.com/questions/33666557/get-phonemes-from-any-word-in-python-nltk-or-other-modules\n",
        "try:\n",
        "    arpabet = nltk.corpus.cmudict.dict()\n",
        "except LookupError:\n",
        "    nltk.download('cmudict')\n",
        "    arpabet = nltk.corpus.cmudict.dict()\n",
        "\n",
        "@lru_cache()\n",
        "def wordbreak(s):\n",
        "    s = s.lower()\n",
        "    if s in arpabet:\n",
        "        return arpabet[s]\n",
        "    middle = len(s)/2\n",
        "    partition = sorted(list(range(len(s))), key=lambda x: (x-middle)**2-x)\n",
        "    for i in partition:\n",
        "        pre, suf = (s[:i], s[i:])\n",
        "        if pre in arpabet and wordbreak(suf) is not None:\n",
        "            return [x+y for x,y in iterprod(arpabet[pre], wordbreak(suf))]\n",
        "    return None\n",
        "\n",
        "def normalized_sim(w1, w2):\n",
        "  return 1-edit_distance(w1,w2)/(max(len(w1), len(w2)))\n"
      ],
      "metadata": {
        "id": "Yzh1x4VTeOkJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## example"
      ],
      "metadata": {
        "id": "5ncmloxIecIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = \"birds\"\n",
        "w2 = \"pigs\"\n",
        "print(\"wordbreak(w1)[0]:\",wordbreak(w1)[0])\n",
        "print(\"wordbreak(w2)[0]:\",wordbreak(w2)[0])\n",
        "\n",
        "print(\"orig phon:\", edit_distance(wordbreak(w1)[0],wordbreak(w2)[0]))\n",
        "print(\"orig orth:\", edit_distance(w1, w2))\n",
        "\n",
        "print(\"norm orth:\", normalized_sim(w1, w2))\n",
        "print(\"norm phon:\", normalized_sim(wordbreak(w1)[0],wordbreak(w2)[0]))"
      ],
      "metadata": {
        "id": "WZM8Ds6neRVU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce536951-2dff-4fb1-87a1-0723357cfd0f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wordbreak(w1)[0]: ['B', 'ER1', 'D', 'Z']\n",
            "wordbreak(w2)[0]: ['P', 'IH1', 'G', 'Z']\n",
            "orig phon: 3\n",
            "orig orth: 3\n",
            "norm orth: 0.4\n",
            "norm phon: 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reading data"
      ],
      "metadata": {
        "id": "VhJ2xFIYegNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parentfolder = \"/content/drive/My Drive/IU-Abhilasha-Mike/Fluency/sem-phon/fluency_cogsci2022\"\n",
        "with tf.device('/device:GPU:0'):\n",
        "  semantic_files = glob.glob(parentfolder + '/*.xlsx')\n",
        "print(f\"This folder has {len(semantic_files)} files\")"
      ],
      "metadata": {
        "id": "faY0v_dpehPS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa2fbd7-feb8-4a80-924b-f9180956c258"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This folder has 1 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reading embeddings"
      ],
      "metadata": {
        "id": "U_amfhYwekDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "embeddings obtained from: http://vectors.nlpl.eu/explore/embeddings/en/models/"
      ],
      "metadata": {
        "id": "e7OeO3-OrlS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import glove embeddings\n",
        "parentfolder = \"/content/drive/My Drive/IU-Abhilasha-Mike/Fluency/sem-phon/fluency_cogsci2022/\"\n",
        "with tf.device('/device:GPU:0'):\n",
        "  word2vec_model = pd.read_csv(parentfolder +\"wikimodel.csv\", encoding=\"unicode-escape\")\n",
        "  word2vec = word2vec_model.transpose()\n",
        "  new_header = word2vec.iloc[0] #grab the first row for the header\n",
        "  word2vec = word2vec[1:] #take the data less the header row\n",
        "  word2vec.columns = new_header\n",
        "  word2vec = word2vec.values.transpose()\n",
        "  vocab = pd.DataFrame(list(new_header), columns=[\"vocab_word\"])\n",
        "  print(f\"embeddings are shaped:\", word2vec.shape)\n",
        "  print(f\"vocab is {len(vocab)} words\")"
      ],
      "metadata": {
        "id": "f3aFPCCZelEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65187091-a09f-435b-bd2b-4516d1f269d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddings are shaped: (249211, 300)\n",
            "vocab is 0 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# obtaining phonemic & semantic similarity"
      ],
      "metadata": {
        "id": "ILHNvQUMemLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## now we loop through each txt file\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "phon_list = []\n",
        "\n",
        "# read in the data as a pandas dataframe\n",
        "data_file = pd.read_excel(semantic_files[0])\n",
        "import re\n",
        "for index, row in data_file.iterrows():  \n",
        "  word = str(row[\"response\"])\n",
        "  mod_word = re.sub('[^a-zA-Z]+', '', word)\n",
        "  if(len(mod_word)>0):\n",
        "    phonemes = wordbreak(mod_word)[0]\n",
        "    phon_list.append(phonemes)\n",
        "  else:\n",
        "    phon_list.append(\"wordnotfound\")\n",
        "\n",
        "data_file[\"phonemes\"] = phon_list\n",
        "data_file[\"response_number\"] = data_file.groupby(['subject', 'domain']).cumcount()+1\n",
        "\n",
        "# exclude rows that do not have a valid phoneme\n",
        "data_file = data_file[data_file.phonemes != \"wordnotfound\"]\n",
        "data_file = data_file.reset_index(drop= True)\n",
        "\n",
        "#now we compute the levenshtein edit distance as a measure of orthographic/phonemic similarity \n",
        "\n",
        "phon_similarity = []\n",
        "orth_similarity = []\n",
        "glove_similarity = []\n",
        "\n",
        "norm_phon = []\n",
        "norm_orth = []\n",
        "\n",
        "for index, row in data_file.iterrows():\n",
        "  current_word = re.sub('[^a-zA-Z]+', '', str(row[\"response\"]))\n",
        "  current_phoneme = row[\"phonemes\"]\n",
        "  current_word = \"FALSE\" if current_word in [\"False\", \"false\"] else current_word\n",
        "  if row[\"response_number\"] == 1:\n",
        "      sem_val = -999\n",
        "      phon_val = -999\n",
        "      orth_val = -999\n",
        "      norm_phon_val = -999\n",
        "      norm_orth_val = -999\n",
        "  else:\n",
        "    previous_word = re.sub('[^a-zA-Z]+', '', str(data_file.response[index-1]))\n",
        "    previous_phoneme = data_file.phonemes[index-1]\n",
        "\n",
        "    #calculate orthographic similarity as Levenshtein (edit) distance\n",
        "    orth_val = edit_distance(previous_word, current_word)\n",
        "    norm_orth_val = normalized_sim(previous_word, current_word)\n",
        "    \n",
        "    # can also get edit distance for the phonemes themselves (as in Siew et al. Hoosier network)\n",
        "\n",
        "    phon_val = edit_distance(previous_phoneme, current_phoneme)\n",
        "    norm_phon_val = normalized_sim(previous_phoneme, current_phoneme)\n",
        "    \n",
        "    # extract word embedding for current word\n",
        "    if current_word in list(vocab[\"vocab_word\"]):\n",
        "      current_word_index = list(vocab[\"vocab_word\"]).index(current_word)\n",
        "      current_word_vec = word2vec[current_word_index].reshape((1,word2vec.shape[1]))\n",
        "      # extract word embedding for current word\n",
        "      if previous_word in list(vocab[\"vocab_word\"]):\n",
        "        previous_word_index = list(vocab[\"vocab_word\"]).index(previous_word)\n",
        "        previous_word_vec = word2vec[previous_word_index].reshape((1,word2vec.shape[1]))\n",
        "        sem_val = float((1 - scipy.spatial.distance.cdist(previous_word_vec, current_word_vec, 'cosine'))[0])      \n",
        "        #print(f\"for {current_word} and {previous_word} similarity is {sem_val}\")\n",
        "      else:\n",
        "        sem_val = \"NA\"\n",
        "    else:\n",
        "      sem_val = \"NA\"\n",
        "\n",
        "  phon_similarity.append(phon_val)\n",
        "  orth_similarity.append(orth_val)\n",
        "  norm_phon.append(norm_phon_val)\n",
        "  norm_orth.append(norm_orth_val)\n",
        "  glove_similarity.append(sem_val)\n",
        "  \n",
        "data_file[\"phon_similarity\"] = phon_similarity\n",
        "data_file[\"orth_similarity\"] = orth_similarity\n",
        "data_file[\"norm_phon\"] = norm_phon\n",
        "data_file[\"norm_orth\"] = norm_orth\n",
        "data_file[\"word2vec_similarity\"] = glove_similarity \n",
        "data_file.to_csv(parentfolder + f'precomputed_data.csv')"
      ],
      "metadata": {
        "id": "WqZLn2_Beljf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "_xRq3eLvzLEN",
        "outputId": "bc5aa491-cfeb-46d7-c3d4-50f58c640f50"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9812047c-bbbf-4324-9ca2-58141f9c8000\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>subject</th>\n",
              "      <th>domain</th>\n",
              "      <th>response_number</th>\n",
              "      <th>response</th>\n",
              "      <th>response_onset_time</th>\n",
              "      <th>IRT</th>\n",
              "      <th>participant_designated_switch</th>\n",
              "      <th>phonemes</th>\n",
              "      <th>phon_similarity</th>\n",
              "      <th>orth_similarity</th>\n",
              "      <th>norm_phon</th>\n",
              "      <th>norm_orth</th>\n",
              "      <th>word2vec_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LEA</td>\n",
              "      <td>50001</td>\n",
              "      <td>animals</td>\n",
              "      <td>1</td>\n",
              "      <td>horse</td>\n",
              "      <td>2.594</td>\n",
              "      <td>2.594000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[HH, AO1, R, S]</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LEA</td>\n",
              "      <td>50001</td>\n",
              "      <td>animals</td>\n",
              "      <td>2</td>\n",
              "      <td>pig</td>\n",
              "      <td>3.594</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[P, IH1, G]</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.132521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LEA</td>\n",
              "      <td>50001</td>\n",
              "      <td>animals</td>\n",
              "      <td>3</td>\n",
              "      <td>bear</td>\n",
              "      <td>4.894</td>\n",
              "      <td>1.300000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[B, EH1, R]</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.196242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LEA</td>\n",
              "      <td>50001</td>\n",
              "      <td>animals</td>\n",
              "      <td>4</td>\n",
              "      <td>cat</td>\n",
              "      <td>6.194</td>\n",
              "      <td>1.300000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[K, AE1, T]</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.1052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LEA</td>\n",
              "      <td>50001</td>\n",
              "      <td>animals</td>\n",
              "      <td>5</td>\n",
              "      <td>dog</td>\n",
              "      <td>7.394</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[D, AO1, G]</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.202481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32397</th>\n",
              "      <td>HJT</td>\n",
              "      <td>1099</td>\n",
              "      <td>sports</td>\n",
              "      <td>24</td>\n",
              "      <td>jumprope</td>\n",
              "      <td>130.000</td>\n",
              "      <td>46.308824</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[JH, AH1, M, P, R, OW1, P]</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32398</th>\n",
              "      <td>HJT</td>\n",
              "      <td>1099</td>\n",
              "      <td>sports</td>\n",
              "      <td>25</td>\n",
              "      <td>hockey</td>\n",
              "      <td>163.000</td>\n",
              "      <td>33.926471</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[HH, AA1, K, IY0]</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32399</th>\n",
              "      <td>HJT</td>\n",
              "      <td>1099</td>\n",
              "      <td>sports</td>\n",
              "      <td>26</td>\n",
              "      <td>fieldhockey</td>\n",
              "      <td>169.000</td>\n",
              "      <td>3.529412</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[F, IY1, L, D, HH, AA1, K, IY0]</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32400</th>\n",
              "      <td>HJT</td>\n",
              "      <td>1099</td>\n",
              "      <td>sports</td>\n",
              "      <td>27</td>\n",
              "      <td>lacrosse</td>\n",
              "      <td>171.000</td>\n",
              "      <td>3.852941</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[L, AH0, K, R, AO1, S]</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32401</th>\n",
              "      <td>HJT</td>\n",
              "      <td>1099</td>\n",
              "      <td>sports</td>\n",
              "      <td>28</td>\n",
              "      <td>bobsledracing</td>\n",
              "      <td>198.000</td>\n",
              "      <td>26.073529</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[B, AA1, B, S, L, EH2, D, R, EY1, S, IH0, NG]</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>NA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32402 rows Ã— 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9812047c-bbbf-4324-9ca2-58141f9c8000')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9812047c-bbbf-4324-9ca2-58141f9c8000 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9812047c-bbbf-4324-9ca2-58141f9c8000');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      dataset  subject   domain  ...  norm_phon   norm_orth  word2vec_similarity\n",
              "0         LEA    50001  animals  ...   -999.000 -999.000000                 -999\n",
              "1         LEA    50001  animals  ...      0.000    0.000000             0.132521\n",
              "2         LEA    50001  animals  ...      0.000    0.000000             0.196242\n",
              "3         LEA    50001  animals  ...      0.000    0.250000               0.1052\n",
              "4         LEA    50001  animals  ...      0.000    0.000000             0.202481\n",
              "...       ...      ...      ...  ...        ...         ...                  ...\n",
              "32397     HJT     1099   sports  ...      0.125    0.222222                   NA\n",
              "32398     HJT     1099   sports  ...      0.000    0.000000                   NA\n",
              "32399     HJT     1099   sports  ...      0.500    0.545455                   NA\n",
              "32400     HJT     1099   sports  ...      0.125    0.181818                   NA\n",
              "32401     HJT     1099   sports  ...      0.250    0.153846                   NA\n",
              "\n",
              "[32402 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oMu6iDRm7q8m"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec1 = word2vec[list(vocab[\"vocab_word\"]).index(\"boy\")].reshape((1,word2vec.shape[1]))\n",
        "vec2 = word2vec[list(vocab[\"vocab_word\"]).index(\"man\")].reshape((1,word2vec.shape[1]))\n",
        "float((1 - scipy.spatial.distance.cdist(vec1, vec2, 'cosine'))[0])      "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meH-bA54zoGs",
        "outputId": "7f52932e-caf5-4f27-c2c9-a4917af65ebf"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19650142429788575"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(word1, word2):\n",
        "  vec1 = word2vec[list(vocab[\"vocab_word\"]).index(word1)].reshape((1,word2vec.shape[1]))\n",
        "  vec2 = word2vec[list(vocab[\"vocab_word\"]).index(word2)].reshape((1,word2vec.shape[1]))\n",
        "  return float((1 - scipy.spatial.distance.cdist(vec1, vec2, 'cosine'))[0])      "
      ],
      "metadata": {
        "id": "ykEurQ1NBVc1"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(\"apple\", \"apple\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atOFXEEaBiU_",
        "outputId": "f9c4928e-0603-47e3-94a8-6a31f98a53d2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999999999999"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_closest(word):\n",
        "  sims = [cosine_similarity(word,x) for x in list(vocab.vocab_word)[:1000]]\n",
        "  y = np.array(sims)\n",
        "  y_sorted = np.argsort(-y) ## gives sorted indices\n",
        "  top5_indices = y_sorted[:5]\n",
        "  w1 = [list(vocab.vocab_word)[i] for i in top5_indices]\n",
        "  return w1"
      ],
      "metadata": {
        "id": "sZz2-AHO0cCy"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_closest(\"apple\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fTzhwMwBqz0",
        "outputId": "51933d4c-6101-4c4c-89cd-16679419e1d1"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['great', 'US', 'main', 'car', 'flight']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# computational search models"
      ],
      "metadata": {
        "id": "cKvNxY3we7_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we only consider the \"animals\" domain for the computational models. we start with a predefined list of 675 animals for which we create a semantic and phonological similarity matrix and obtain word frequency estimates."
      ],
      "metadata": {
        "id": "XRE_8HRugwqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# download animals list"
      ],
      "metadata": {
        "id": "KDmZm22lDaoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simlabels = pd.read_csv(parentfolder+ f'similaritylabels.csv').values.reshape(-1,).tolist()\n",
        "len(simlabels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0_hYk_MDaEA",
        "outputId": "122b446e-a24c-401d-d72d-9727b7b0f96a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "675"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'blue jay' in list(vocab.vocab_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xEWStcxI4jQ",
        "outputId": "21d35848-afaa-41ff-fe72-e15dba1d2888"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create semantic similarity matrix"
      ],
      "metadata": {
        "id": "-rOFw544fGad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## create similarity matrix and similarity labels file from whichever corpus you're using\n",
        "\n",
        "def create_sim_matrix(simlabels):\n",
        "  ## the vocab consists of ALL possible words in corpus, but we need only the \"animals\" subset here\n",
        "  ## we use the similarity_labels file from psyrev to constrain the vocab\n",
        "  animals_index = [list(vocab.vocab_word).index(lab) if lab in list(vocab.vocab_word) else -999 for lab in simlabels ]\n",
        "  animals_index = list(filter((-999).__ne__, animals_index))\n",
        "\n",
        "  ## now we restrict our vocab and embeddings to ONLY these animals\n",
        "  embeddings_small = word2vec[animals_index, :]\n",
        "  vocab_small = vocab.iloc[animals_index]\n",
        "  N = len(vocab_small)\n",
        "  print(f\"animals vocab is {N} words\")\n",
        "\n",
        "  # create semantic similarity matrix\n",
        "  matrix = 1-scipy.spatial.distance.cdist(embeddings_small, embeddings_small, 'cosine').reshape(-1)\n",
        "  matrix = matrix.reshape((N,N))\n",
        "  print(\"sim matrix has been created:\", matrix.shape)\n",
        "\n",
        "  w1_index = list(vocab_small.vocab_word).index(\"dolphin\")\n",
        "  w2_index = list(vocab_small.vocab_word).index(\"kitten\")\n",
        "  w3_index = list(vocab_small.vocab_word).index(\"whale\")\n",
        "\n",
        "  print(\"dolphin-kitten:\", matrix[w1_index, w2_index])\n",
        "  print(\"dolphin-whale:\", matrix[w1_index, w3_index])\n",
        "  print(\"dolphin-dolphin:\", matrix[w1_index, w1_index])\n",
        "\n",
        "  pd.DataFrame(matrix).to_csv(parentfolder + 'word2vec_sim_matrix.csv', index=False, header=False)\n",
        "  vocab_small.to_csv(parentfolder + 'word2vec_sim_labels.csv', index=False, header=False)\n",
        "\n",
        "  print(\"sim matrix has been saved to drive!\")\n",
        "\n",
        "create_sim_matrix(simlabels)\n"
      ],
      "metadata": {
        "id": "xvFN6weVe-vq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd09595-18d0-4bc8-c64e-73cc27448c58"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "animals vocab is 423 words\n",
            "sim matrix has been created: (423, 423)\n",
            "dolphin-kitten: 0.0742156239385755\n",
            "dolphin-whale: 0.03625136598448808\n",
            "dolphin-dolphin: 0.9999999999999999\n",
            "sim matrix has been saved to drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create phonological similarity matrix"
      ],
      "metadata": {
        "id": "v5HTHGsjfI_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_phon_matrix(vocab):\n",
        "  # takes in a list of labels and computes the phonological similarity matrix\n",
        "  vocabulary = vocab.copy()\n",
        "  N = len(vocabulary)\n",
        "  print(f\"vocab is {N} words\")\n",
        "  # replace all underscores (_) with space (\" \") to match with glove vectors/vocab\n",
        "  vocabulary = [re.sub('[^a-zA-Z]+', '', str(v)) for v in vocabulary]\n",
        "  print(f\"vocab now looks like:\", vocabulary[:5])\n",
        "  # create phonemic similarity matrix for the small vocab\n",
        "  pmatrix = np.array([normalized_sim(wordbreak(w1)[0], wordbreak(w2)[0]) for w1 in vocabulary for w2 in vocabulary]).reshape((N,N))\n",
        "  print(\"pmatrix has been created:\", pmatrix.shape)\n",
        "  print(pmatrix)\n",
        "  pd.DataFrame(pmatrix).to_csv(parentfolder + 'simlabels_phon_matrix.csv', index=False, header=False)  \n",
        "  print(\"phon matrix has been saved to drive!\")\n",
        "\n",
        "simlabels = pd.read_csv(parentfolder+'word2vec_sim_labels.csv', header=None).values.reshape(-1,).tolist()\n",
        "print(f\"simlabels is {len(simlabels)} items:\", simlabels[:5])\n",
        "create_phon_matrix(simlabels)\n"
      ],
      "metadata": {
        "id": "b2HouuHsfK4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64d9ed9-0dc3-432b-db1c-16359b7e1dc6"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "simlabels is 423 items: ['Persian', 'Spinosaurus', 'aardvark', 'albatross', 'alligator']\n",
            "vocab is 423 words\n",
            "vocab now looks like: ['Persian', 'Spinosaurus', 'aardvark', 'albatross', 'alligator']\n",
            "pmatrix has been created: (423, 423)\n",
            "[[1.         0.1        0.         ... 0.         0.         0.125     ]\n",
            " [0.1        1.         0.1        ... 0.1        0.2        0.1       ]\n",
            " [0.         0.1        1.         ... 0.         0.14285714 0.        ]\n",
            " ...\n",
            " [0.         0.1        0.         ... 1.         0.         0.        ]\n",
            " [0.         0.2        0.14285714 ... 0.         1.         0.625     ]\n",
            " [0.125      0.1        0.         ... 0.         0.625      1.        ]]\n",
            "phon matrix has been saved to drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get frequencies"
      ],
      "metadata": {
        "id": "mNgomwoNMI2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frequencies = pd.read_csv(parentfolder+ f'frequencies.csv', header=None)\n",
        "frequencies.columns = [\"word\", \"freq\"]\n",
        "# simlabels = pd.read_csv(parentfolder+'word2vec_sim_labels.csv', header=None).values.reshape(-1,).tolist() \n",
        "freq_small = pd.DataFrame([frequencies.loc[list(frequencies.word).index(lab)].freq for lab in simlabels])\n",
        "freq_small['word'] = simlabels\n",
        "## keep only those that are part of simlabels\n",
        "freq_small.to_csv(parentfolder + 'word2vec_frequencies.csv', index=False, header=False)  "
      ],
      "metadata": {
        "id": "pIkuXkoLMKkO"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## define foraging models"
      ],
      "metadata": {
        "id": "e_NPzt9nfQOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For these models, the behavioral data is read in as .txt files separated by a space"
      ],
      "metadata": {
        "id": "kNsBt3GyOiQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def modelFits(path, delimiter):\n",
        "\n",
        "    ### LOAD REQUIRED PACKAGES ###\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import re\n",
        "\n",
        "    ### LOAD BEHAVIORAL DATA ###\n",
        "    df = pd.read_csv(path, header=None, names=['SID', 'entry'], delimiter=delimiter)\n",
        "\n",
        "    #correct behavioral fits\n",
        "    df = forage.prepareData(df)\n",
        "\n",
        "    ### LOAD SEMANTIC SIMILARITY MATRIX ###\n",
        "\n",
        "    # (aka 'local cues', here we use cosines from word2vec)\n",
        "\n",
        "    # Similarity labels\n",
        "    simlab = []\n",
        "    ofile = open(parentfolder + 'word2vec_sim_labels.csv','r')#TODO:\n",
        "    for line in ofile:\n",
        "        labs = line.split()\n",
        "        for lab in labs:\n",
        "            simlab.append(lab)\n",
        "    ofile.close()\n",
        "\n",
        "    # Similarity values\n",
        "    simval = np.zeros((len(simlab), len(simlab)))\n",
        "    ofile = open(parentfolder + 'word2vec_sim_matrix.csv', 'r')#TODO:\n",
        "    j=0\n",
        "    for line in ofile:\n",
        "        line = re.sub(',\\n', '', line)\n",
        "        sims = line.split(',')\n",
        "        i=0\n",
        "        for sim in sims:\n",
        "            simval[i,j] = sim\n",
        "            i+=1\n",
        "        j+=1\n",
        "    ofile.close()\n",
        "\n",
        "    # Make sure similarity values are non-zero\n",
        "    for i in range(0,len(simval)):\n",
        "        for j in range(0,len(simval)):\n",
        "            if simval[i,j] <= 0:\n",
        "                simval[i,j] = 0.0001\n",
        "\n",
        "    ## PHONEMIC SIMILARTY VALUES ##\n",
        "    phonval = np.zeros((len(simlab), len(simlab)))\n",
        "    ofile = open(parentfolder + 'simlabels_phon_matrix.csv', 'r')#TODO:\n",
        "    j=0\n",
        "    for line in ofile:\n",
        "        line = re.sub(',\\n', '', line)\n",
        "        sims = line.split(',')\n",
        "        i=0\n",
        "        for sim in sims:\n",
        "            phonval[i,j] = sim\n",
        "            i+=1\n",
        "        j+=1\n",
        "    ofile.close()\n",
        "\n",
        "    # Make sure phonemic values are non-zero\n",
        "    for i in range(0,len(phonval)):\n",
        "        for j in range(0,len(phonval)):\n",
        "            if phonval[i,j] <= 0:\n",
        "                phonval[i,j] = 0.0001\n",
        "\n",
        "    ### LOAD FREQUENCY LIST ###\n",
        "    # (aka 'global cue', using NOW corpus from http://corpus.byu.edu/now/, 4.2 billion words and growing daily)\n",
        "\n",
        "    freqlab = []\n",
        "    freqval = []\n",
        "    ofile = open(parentfolder + 'word2vec_frequencies.csv', 'r') #TODO:\n",
        "    for line in ofile:\n",
        "        line = re.sub('\\n', '', line)\n",
        "        freqs=line.split(',')\n",
        "        freqlab.append(freqs[1])\n",
        "        ## append log of frequency if using psyrev\n",
        "        freqval.append(np.log(float(freqs[0])))\n",
        "        #freqval.append(float(freqs[1]))\n",
        "    ofile.close()\n",
        "    freqval=np.array(freqval)\n",
        "\n",
        "    sidlist = list(set(df['SID']))\n",
        "    full_entdf = pd.DataFrame()\n",
        "    full_fitlist = []\n",
        "    ct = 0\n",
        "\n",
        "    ## COMPUTE CONSECUTIVE SIMILARITY AND FREQUENCY AT SUBJECT LEVEL ##\n",
        "\n",
        "    for sid in sidlist:\n",
        "        ct+=1\n",
        "        print( \"SUBJECT \" + str(ct) + '/' + str(len(sidlist)) + \" \" + str(sid))\n",
        "\n",
        "        # My general initializations\n",
        "        myfitlist = []\n",
        "        myentries = np.array(df[df['SID']==sid]['entry'])\n",
        "        #print(\"myentries:\", myentries)\n",
        "        myenttimes = np.array(df[df['SID']==sid].index)\n",
        "        ##print(\"myenttimes:\", myenttimes)\n",
        "        myused = []\n",
        "        mytime = []\n",
        "\n",
        "        # For both frequency and similarity metrics:\n",
        "            # LIST: Metrics corresponding with my observed entries\n",
        "            # CURRENT: Full metric values, with observed entries becoming 0\n",
        "            # HISTORY: State of full metric values (ie, \"current\" during each entry)\n",
        "\n",
        "        # My frequency initializations\n",
        "        # freq current contains frequencies of ALL the words in corpus\n",
        "        freq_current = np.array(freqval)\n",
        "        #print(\"freq_current.shape:\",freq_current.shape)\n",
        "        freq_list = []\n",
        "        freq_history = []\n",
        "\n",
        "        # My similarity initializations\n",
        "        sim_current = simval.copy()\n",
        "        # sim_current contains the full NxN similarity matrix\n",
        "        #print(\"sim_current shape:\",sim_current.shape)\n",
        "        sim_list = []\n",
        "        sim_history = []\n",
        "\n",
        "        phon_current = phonval.copy()\n",
        "        phon_list = []\n",
        "        phon_history = []\n",
        "\n",
        "        for i in range(0,len(myentries)):\n",
        "            word = myentries[i]\n",
        "            #if word not in myused: # use this to calculate number of correct responses w/out repeats\n",
        "            if True:   # use this line instead of former to include repeated words along w/line 110,119 comment out\n",
        "\n",
        "                # Frequency: Get frequency and update relevant lists\n",
        "                freq_list.append( float(freq_current[freqlab.index(word)]) )\n",
        "                freq_history.append( np.array(freq_current) )\n",
        "                #freq_current[freqlab.index(word)] = 0.00000001\n",
        "\n",
        "                # Get similarity between this word and preceding word\n",
        "                if i > 0:         \n",
        "                    sim_list.append( float(sim_current[simlab.index(myentries[i-1]), simlab.index(word)]) )\n",
        "                    sim_history.append( np.array(sim_current[simlab.index(myentries[i-1]),:]) )\n",
        "\n",
        "                    phon_list.append( float(phon_current[simlab.index(myentries[i-1]), simlab.index(word)]) )\n",
        "                    phon_history.append( np.array(phon_current[simlab.index(myentries[i-1]),:]) )\n",
        "                else:\n",
        "                    sim_list.append(0)\n",
        "                    sim_history.append( np.array(sim_current[simlab.index(word),:]) )\n",
        "                #sim_current[:,simlab.index(word)] = 0.00000001\n",
        "                    phon_list.append(0)\n",
        "                    phon_history.append( np.array(phon_current[simlab.index(word),:]) )\n",
        "\n",
        "                # Update lists\n",
        "                myused.append(word)\n",
        "                mytime.append(myenttimes[i])\n",
        "\n",
        "        # Calculate category switches, based on similarity-drop\n",
        "        myswitch = np.zeros(len(myused)).astype(int)\n",
        "        for i in range(1,len(myused)-1):\n",
        "            if (sim_list[i+1] > sim_list[i]) and (sim_list[i-1] > sim_list[i]):\n",
        "                myswitch[i] = 1\n",
        "\n",
        "        # Save my entries with corresponding metrics\n",
        "        mydf = pd.DataFrame({'sid':[sid]*len(myused) , 'ent':myused, 'freq':freq_list, 'sim':sim_list, 'phon': phon_list,\n",
        "                             'switch':myswitch, 'time':mytime},\n",
        "                            columns=['sid','time','ent','freq','sim', 'phon', 'switch'])\n",
        "        full_entdf = full_entdf.append(mydf)\n",
        "        # Get parameter fits for the different models\n",
        "        myfitlist.append(sid)\n",
        "        myfitlist.append(len(myused))\n",
        "        ## obtaining the optimal/random fits for the static and dynamic model by calling the getFits function\n",
        "        myfitlist.extend( forage.getfits(freq_list, freq_history, sim_list, sim_history, phon_list, phon_history) )\n",
        "        full_fitlist.append(myfitlist)\n",
        "\n",
        "    print(\"Fits Complete.\")\n",
        "\n",
        "    # create results directory if it doesn't exist yet\n",
        "    if not os.path.exists(parentfolder):\n",
        "        os.makedirs(parentfolder)\n",
        "\n",
        "    # # Output data entries with corresponding metrics for visualization in R\n",
        "    print(full_entdf)\n",
        "    full_entdf = full_entdf.reset_index(drop=True)\n",
        "    full_entdf.to_csv(parentfolder  + 'nancy-fullmetrics.csv', index=False, header=True)\n",
        "\n",
        "    # # Output parameter & model fits\n",
        "    full_fitlist = pd.DataFrame(full_fitlist)\n",
        "    full_fitlist.columns = ['subject', 'number_of_items', \n",
        "                            'beta_static_frequency', 'beta_static_semantic', 'errors_static_optimal', 'errors_static_random',\n",
        "                            'beta_dynamic_frequency', 'beta_dynamic_semantic', 'errors_dynamic_optimal', 'errors_dynamic_random',\n",
        "                            'beta_dynamicjack_frequency', 'beta_dynamicjack_semantic', 'errors_dynamicjack_optimal', 'errors_dynamicjack_random',\n",
        "\n",
        "                            'beta_plocalstatic_frequency', 'beta_plocalstatic_semantic', 'beta_plocalstatic_phonemic','errors_plocalstatic_optimal', 'errors_plocalstatic_random',\n",
        "\n",
        "                            'beta_plocaldynamicorig_frequency', 'beta_plocaldynamicorig_semantic', 'beta_plocaldynamicorig_phonemic','errors_plocaldynamicorig_optimal', 'errors_plocaldynamicorig_random',\n",
        "                            'beta_pglobaldynamicorig_frequency', 'beta_pglobaldynamicorig_semantic', 'beta_pglobaldynamicorig_phonemic','errors_pglobaldynamicorig_optimal', 'errors_pglobaldynamicorig_random',\n",
        "\n",
        "                            'beta_plocaldynamicjack_frequency', 'beta_plocaldynamicjack_semantic', 'beta_plocaldynamicjack_phonemic','errors_plocaldynamicjack_optimal', 'errors_plocaldynamicjack_random',\n",
        "                            'beta_pglobaldynamicjack_frequency', 'beta_pglobaldynamicjack_semantic', 'beta_pglobaldynamicjack_phonemic','errors_pglobaldynamicjack_optimal', 'errors_pglobaldynamicjack_random',\n",
        "\n",
        "                            'beta_pswitchonlydynamicjack_frequency', 'beta_pswitchonlydynamicjack_semantic', 'beta_pswitchonlydynamicjack_phonemic','errors_pswitchonlydynamicjack_optimal', 'errors_pswitchonlydynamicjack_random'\n",
        "                            ]\n",
        "\n",
        "    #print(\"full_fitlist:\",full_fitlist)\n",
        "    full_fitlist.to_csv(parentfolder  + 'nancy-fullfits.csv', index=False, header=True)\n",
        "\n",
        "    print(full_fitlist.head())\n",
        "    print(\"Results saved to '\" + parentfolder + \"'.\")\n",
        "\n",
        "class forage:\n",
        "\n",
        "    def prepareData(data):\n",
        "        import pandas as pd\n",
        "        import re\n",
        "        # load similarity labels\n",
        "        simlab = []\n",
        "        ofile = open(parentfolder + 'word2vec_sim_labels.csv','r')\n",
        "        for line in ofile:\n",
        "            labs = line.split()\n",
        "            for lab in labs:\n",
        "                simlab.append(lab)\n",
        "        ofile.close()\n",
        "\n",
        "        ### LOAD CORRECTIONS ###\n",
        "        # This is a look-up list that maps incorrect words onto accepted words that are in the database\n",
        "        corrections = pd.read_csv(parentfolder + 'corrections.txt', header=None, delimiter='\\t')\n",
        "        corrections = corrections.set_index(corrections[0].values)\n",
        "        corrections.columns = ['_from','_to']\n",
        "\n",
        "        elist = data['entry'].values\n",
        "        newlist = []\n",
        "        notfound = []\n",
        "\n",
        "        # Use look-up table to check and correct observed entries\n",
        "        for ent in elist:\n",
        "            ent = re.sub(r'\\W+', '', ent) # Alphanumericize it\n",
        "            if ent in simlab:\n",
        "                # If this entry is appropriate, keep it\n",
        "                newlist.append(ent)\n",
        "            elif ent[0:len(ent)-1] in simlab:\n",
        "                # If this entry is plural, correct to the singular verion\n",
        "                print(f\"found the entry {ent[0:len(ent)-1]} in simlab\")\n",
        "                newlist.append(ent[0:len(ent)-1])\n",
        "            # elif ent in corrections._from:\n",
        "            #     # If this entry is correctable, correct it\n",
        "            #     newlist.append(corrections.loc[ent]._to)\n",
        "            else:\n",
        "                # If this entry is not found in either list, mark for removal and warn user.\n",
        "                newlist.append('NA')\n",
        "                notfound.append(ent)\n",
        "\n",
        "        # Remove the rows with inappropriate entries\n",
        "        data.entry = newlist\n",
        "        data = data[data.entry!='NA']\n",
        "\n",
        "        # Warn the user of removed entries\n",
        "        if len(notfound) > 0:\n",
        "            print('The following items were not found in the database, and were removed: [' +\n",
        "                  str(len(notfound)) + ' entries removed] \\n')\n",
        "            print(sorted(set(notfound)))\n",
        "        else:\n",
        "            print('All items OK.')\n",
        "        return data[data.entry!='NA']\n",
        "        # TODO: return statement might not be necessary...\n",
        "\n",
        "    def model_static(beta, freql, freqh, siml, simh):\n",
        "        ## beta contains the optimization parameters for frequency (beta[0]) and semantic similarity (beta[1])\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0: # if first item then its probability is based on just frequency\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else: # if not first item then its probability is based on its similarity to prev item AND frequency\n",
        "            # P of item based on frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat) # negative Log likelihood of this item: this will be minimized eventually\n",
        "        return ct\n",
        "\n",
        "\n",
        "    def model_dynamic_original(beta, freql, freqh, siml, simh):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based again on frequency\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "        \n",
        "    def model_dynamic_jack(beta, freql, freqh, siml, simh):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based again on frequency\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "      \n",
        "    def model_static_plocal(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "        ## beta contains the optimization parameters for frequency (beta[0]) and semantic similarity (beta[1])\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0: # if first item then its probability is based on just frequency\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else: # if not first item then its probability is based on its similarity to prev item AND frequency AND phonemic similarity\n",
        "            # P of item based on frequency and similarity and phonology\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) * pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2])* pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat) # negative Log likelihood of this item: this will be minimized eventually\n",
        "        return ct\n",
        "\n",
        "    def model_dynamic_plocal_jack(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based on frequency \n",
        "                numrat = pow(freql[k],beta[0]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(phonl[k],beta[2])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(phonh[k],beta[2])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "\n",
        "    def model_dynamic_pglobal_jack(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based on a combination of frequency and phonemic similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2]) )\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(phonl[k],beta[2])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(phonh[k],beta[2])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "\n",
        "    def model_dynamic_pswitchonly_jack(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based on a combination of frequency and phonemic similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2]) )\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "    \n",
        "    def model_dynamic_plocal_original(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based on frequency \n",
        "                numrat = pow(freql[k],beta[0]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(phonl[k],beta[2])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(phonh[k],beta[2])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "\n",
        "    def model_dynamic_pglobal_original(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based on a combination of frequency and phonemic similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2]) )\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "\n",
        "    def getfits( freq_l, freq_h, sim_l, sim_h, phon_l, phon_h ):\n",
        "        import numpy as np\n",
        "        from scipy.optimize import fmin\n",
        "    #fmin: Uses a Nelder-Mead simplex algorithm to find the minimum of function of variables.\n",
        "        r1 = np.random.rand()\n",
        "        r2 = np.random.rand()\n",
        "        r3 = np.random.rand()\n",
        "\n",
        "    # STATIC MODEL (no dynamic switching, just focusing on two cues with some weights)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_static, [r1, r2], args=(freq_l, freq_h, sim_l, sim_h), ftol = 0.001, disp=False)\n",
        "        beta_static_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_static_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_static = forage.model_static([beta_static_freq, beta_static_semantic], freq_l, freq_h, sim_l, sim_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_static = forage.model_static([0, 0], freq_l, freq_h, sim_l, sim_h)\n",
        "\n",
        "    # ORIGINAL DYNAMIC MODEL (switches dynamically between cues)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_original, [r1,r2], args=(freq_l, freq_h, sim_l, sim_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_dynamic = forage.model_dynamic_original([beta_dynamic_freq, beta_dynamic_semantic], freq_l, freq_h, sim_l, sim_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_dynamic = forage.model_dynamic_original([0,0], freq_l, freq_h, sim_l, sim_h)\n",
        "\n",
        "    # JACK DYNAMIC MODEL (switches dynamically between cues)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_jack, [r1,r2], args=(freq_l, freq_h, sim_l, sim_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamicjack_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamicjack_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_dynamicjack = forage.model_dynamic_jack([beta_dynamic_freq, beta_dynamic_semantic], freq_l, freq_h, sim_l, sim_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_dynamicjack = forage.model_dynamic_jack([0,0], freq_l, freq_h, sim_l, sim_h)\n",
        "  \n",
        "\n",
        "    # LOCAL PHONEMIC CUE, STATIC MODEL (no dynamic switching, just focusing on two cues with some weights)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_static_plocal, [r1, r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_static_plocal_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_static_plocal_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_static_plocal_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_plocalstatic = forage.model_static_plocal([beta_static_plocal_freq, beta_static_plocal_semantic, beta_static_plocal_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_plocalstatic = forage.model_static_plocal([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "    \n",
        "    # ORIGINAL DYNAMIC PHON LOCAL MODEL (switches dynamically between cues, phonology,semantic, freq is a LOCAL cue)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_plocal_original, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_plocalorig_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_plocalorig_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_plocalorig_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_plocaldynamicorig = forage.model_dynamic_plocal_original([beta_dynamic_plocalorig_freq, beta_dynamic_plocalorig_semantic, beta_dynamic_plocalorig_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_plocaldynamicorig = forage.model_dynamic_plocal_original([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "     # ORIGINAL DYNAMIC PHON GLOBAL MODEL (switches dynamically between cues, phonology is a GLOBAL cue with frequency)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_pglobal_original, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_pglobalorig_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_pglobalorig_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_pglobalorig_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_pglobaldynamicorig = forage.model_dynamic_pglobal_original([beta_dynamic_pglobalorig_freq, beta_dynamic_pglobalorig_semantic, beta_dynamic_pglobalorig_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_pglobaldynamicorig = forage.model_dynamic_pglobal_original([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "      # JACK DYNAMIC PHON LOCAL MODEL (switches dynamically between cues, phonology,semantic, freq is a LOCAL cue)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_plocal_jack, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_plocaljack_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_plocaljack_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_plocaljack_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_plocaldynamicjack = forage.model_dynamic_plocal_jack([beta_dynamic_plocaljack_freq, beta_dynamic_plocaljack_semantic, beta_dynamic_plocaljack_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_plocaldynamicjack = forage.model_dynamic_plocal_jack([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "     # JACK DYNAMIC PHON GLOBAL MODEL (switches dynamically between cues, phonology is a GLOBAL cue with frequency)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_pglobal_jack, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_pglobaljack_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_pglobaljack_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_pglobaljack_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_pglobaldynamicjack = forage.model_dynamic_pglobal_jack([beta_dynamic_pglobaljack_freq, beta_dynamic_pglobaljack_semantic, beta_dynamic_pglobaljack_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_pglobaldynamicjack = forage.model_dynamic_pglobal_jack([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "      \n",
        "    # JACK DYNAMIC PHON SWITCH ONLY MODEL (switches dynamically between cues, phonology is a GLOBAL cue with frequency)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_pswitchonly_jack, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_pswitchonlyjack_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_pswitchonlyjack_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_pswitchonlyjack_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_pswitchonlydynamicjack = forage.model_dynamic_pswitchonly_jack([beta_dynamic_pswitchonlyjack_freq, beta_dynamic_pswitchonlyjack_semantic, beta_dynamic_pswitchonlyjack_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_pswitchonlydynamicjack = forage.model_dynamic_pswitchonly_jack([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "      \n",
        "      \n",
        "\n",
        "        results = [ beta_static_freq, beta_static_semantic, float(optimal_fit_static), float(random_fit_static),\n",
        "                   beta_dynamic_freq, beta_dynamic_semantic, float(optimal_fit_dynamic), float(random_fit_dynamic),\n",
        "                   beta_dynamicjack_freq, beta_dynamicjack_semantic, float(optimal_fit_dynamicjack), float(random_fit_dynamicjack),\n",
        "\n",
        "                   beta_static_plocal_freq, beta_static_plocal_semantic, beta_static_plocal_phonemic, float(optimal_fit_plocalstatic), float(random_fit_plocalstatic),\n",
        "              \n",
        "                   beta_dynamic_plocalorig_freq, beta_dynamic_plocalorig_semantic, beta_dynamic_plocalorig_phonemic, float(optimal_fit_plocaldynamicorig), float(random_fit_plocaldynamicorig),\n",
        "                   beta_dynamic_pglobalorig_freq, beta_dynamic_pglobalorig_semantic, beta_dynamic_pglobalorig_phonemic, float(optimal_fit_pglobaldynamicorig), float(random_fit_pglobaldynamicorig),\n",
        "\n",
        "                   beta_dynamic_plocaljack_freq, beta_dynamic_plocaljack_semantic, beta_dynamic_plocaljack_phonemic, float(optimal_fit_plocaldynamicjack), float(random_fit_plocaldynamicjack),\n",
        "                   beta_dynamic_pglobaljack_freq, beta_dynamic_pglobaljack_semantic, beta_dynamic_pglobaljack_phonemic, float(optimal_fit_pglobaldynamicjack), float(random_fit_pglobaldynamicjack),\n",
        "\n",
        "                   beta_dynamic_pswitchonlyjack_freq, beta_dynamic_pswitchonlyjack_semantic, beta_dynamic_pswitchonlyjack_phonemic, float(optimal_fit_pswitchonlydynamicjack), float(random_fit_pswitchonlydynamicjack)\n",
        "\n",
        "                   ]\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "lP1sdQE5fR2L"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run foraging models"
      ],
      "metadata": {
        "id": "1LC8wg1yfVnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datapath = parentfolder+'data-psyrev.txt'\n",
        "modelFits(datapath, delimiter = \" \")"
      ],
      "metadata": {
        "id": "hV8fHr10fS3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963007ec-9ede-4b39-f4b6-ecdb12f11788"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found the entry gull in simlab\n",
            "found the entry lizard in simlab\n",
            "The following items were not found in the database, and were removed: [248 entries removed] \n",
            "\n",
            "['akita', 'antlion', 'axolotl', 'bacterium', 'bengaltiger', 'billygoat', 'blackbear', 'blackmamba', 'blacktaileddeer', 'blackwidow', 'blowfish', 'bluefootedbooby', 'bluejay', 'bluewhale', 'boaconstrictor', 'brineshrimp', 'bronco', 'brownbear', 'cattle', 'dalmation', 'duckbill', 'gardensnake', 'goldenmarmot', 'greatdane', 'greatwhiteshark', 'guineapig', 'hammerheadshark', 'humpbackwhale', 'jackrabbit', 'junebug', 'killerwhale', 'kodiakbear', 'komododragon', 'labradorretriever', 'liger', 'makoshark', 'malamute', 'mantaray', 'mountainbear', 'mountaingoat', 'mountainlion', 'muledeer', 'okapi', 'pembrokewelshcorgi', 'pike', 'pillbug', 'polarbear', 'potbelliedpig', 'prairiedog', 'prayingmantis', 'puggle', 'redrumpedagouti', 'rhinoceros', 'sanddollar', 'sealion', 'seamonkey', 'shihtzu', 'siamesecat', 'siberianhusky', 'snowfox', 'snowleopard', 'snowlion', 'snowmonkey', 'spidermonkey', 'stickinsect', 'sugarglider', 'unicorn', 'watersnake', 'whaleshark', 'whitetaileddeer', 'wolverine', 'woollymammoth']\n",
            "SUBJECT 1/141 51\n",
            "SUBJECT 2/141 575\n",
            "SUBJECT 3/141 576\n",
            "SUBJECT 4/141 577\n",
            "SUBJECT 5/141 578\n",
            "SUBJECT 6/141 579\n",
            "SUBJECT 7/141 580\n",
            "SUBJECT 8/141 581\n",
            "SUBJECT 9/141 582\n",
            "SUBJECT 10/141 71\n",
            "SUBJECT 11/141 583\n",
            "SUBJECT 12/141 584\n",
            "SUBJECT 13/141 585\n",
            "SUBJECT 14/141 586\n",
            "SUBJECT 15/141 587\n",
            "SUBJECT 16/141 588\n",
            "SUBJECT 17/141 589\n",
            "SUBJECT 18/141 590\n",
            "SUBJECT 19/141 591\n",
            "SUBJECT 20/141 592\n",
            "SUBJECT 21/141 593\n",
            "SUBJECT 22/141 594\n",
            "SUBJECT 23/141 595\n",
            "SUBJECT 24/141 596\n",
            "SUBJECT 25/141 597\n",
            "SUBJECT 26/141 598\n",
            "SUBJECT 27/141 677\n",
            "SUBJECT 28/141 678\n",
            "SUBJECT 29/141 679\n",
            "SUBJECT 30/141 680\n",
            "SUBJECT 31/141 681\n",
            "SUBJECT 32/141 682\n",
            "SUBJECT 33/141 683\n",
            "SUBJECT 34/141 684\n",
            "SUBJECT 35/141 686\n",
            "SUBJECT 36/141 687\n",
            "SUBJECT 37/141 688\n",
            "SUBJECT 38/141 689\n",
            "SUBJECT 39/141 690\n",
            "SUBJECT 40/141 691\n",
            "SUBJECT 41/141 692\n",
            "SUBJECT 42/141 693\n",
            "SUBJECT 43/141 694\n",
            "SUBJECT 44/141 695\n",
            "SUBJECT 45/141 696\n",
            "SUBJECT 46/141 697\n",
            "SUBJECT 47/141 698\n",
            "SUBJECT 48/141 198\n",
            "SUBJECT 49/141 199\n",
            "SUBJECT 50/141 779\n",
            "SUBJECT 51/141 780\n",
            "SUBJECT 52/141 781\n",
            "SUBJECT 53/141 782\n",
            "SUBJECT 54/141 783\n",
            "SUBJECT 55/141 784\n",
            "SUBJECT 56/141 785\n",
            "SUBJECT 57/141 786\n",
            "SUBJECT 58/141 787\n",
            "SUBJECT 59/141 788\n",
            "SUBJECT 60/141 789\n",
            "SUBJECT 61/141 790\n",
            "SUBJECT 62/141 791\n",
            "SUBJECT 63/141 792\n",
            "SUBJECT 64/141 793\n",
            "SUBJECT 65/141 794\n",
            "SUBJECT 66/141 795\n",
            "SUBJECT 67/141 796\n",
            "SUBJECT 68/141 285\n",
            "SUBJECT 69/141 286\n",
            "SUBJECT 70/141 287\n",
            "SUBJECT 71/141 288\n",
            "SUBJECT 72/141 289\n",
            "SUBJECT 73/141 290\n",
            "SUBJECT 74/141 291\n",
            "SUBJECT 75/141 292\n",
            "SUBJECT 76/141 293\n",
            "SUBJECT 77/141 294\n",
            "SUBJECT 78/141 295\n",
            "SUBJECT 79/141 296\n",
            "SUBJECT 80/141 297\n",
            "SUBJECT 81/141 298\n",
            "SUBJECT 82/141 379\n",
            "SUBJECT 83/141 380\n",
            "SUBJECT 84/141 381\n",
            "SUBJECT 85/141 382\n",
            "SUBJECT 86/141 383\n",
            "SUBJECT 87/141 384\n",
            "SUBJECT 88/141 385\n",
            "SUBJECT 89/141 386\n",
            "SUBJECT 90/141 387\n",
            "SUBJECT 91/141 388\n",
            "SUBJECT 92/141 389\n",
            "SUBJECT 93/141 390\n",
            "SUBJECT 94/141 391\n",
            "SUBJECT 95/141 392\n",
            "SUBJECT 96/141 393\n",
            "SUBJECT 97/141 394\n",
            "SUBJECT 98/141 395\n",
            "SUBJECT 99/141 396\n",
            "SUBJECT 100/141 397\n",
            "SUBJECT 101/141 398\n",
            "SUBJECT 102/141 893\n",
            "SUBJECT 103/141 994\n",
            "SUBJECT 104/141 995\n",
            "SUBJECT 105/141 996\n",
            "SUBJECT 106/141 1098\n",
            "SUBJECT 107/141 997\n",
            "SUBJECT 108/141 1099\n",
            "SUBJECT 109/141 896\n",
            "SUBJECT 110/141 998\n",
            "SUBJECT 111/141 897\n",
            "SUBJECT 112/141 999\n",
            "SUBJECT 113/141 898\n",
            "SUBJECT 114/141 797\n",
            "SUBJECT 115/141 899\n",
            "SUBJECT 116/141 798\n",
            "SUBJECT 117/141 474\n",
            "SUBJECT 118/141 475\n",
            "SUBJECT 119/141 476\n",
            "SUBJECT 120/141 477\n",
            "SUBJECT 121/141 478\n",
            "SUBJECT 122/141 479\n",
            "SUBJECT 123/141 480\n",
            "SUBJECT 124/141 481\n",
            "SUBJECT 125/141 482\n",
            "SUBJECT 126/141 483\n",
            "SUBJECT 127/141 484\n",
            "SUBJECT 128/141 485\n",
            "SUBJECT 129/141 486\n",
            "SUBJECT 130/141 487\n",
            "SUBJECT 131/141 488\n",
            "SUBJECT 132/141 489\n",
            "SUBJECT 133/141 490\n",
            "SUBJECT 134/141 491\n",
            "SUBJECT 135/141 492\n",
            "SUBJECT 136/141 493\n",
            "SUBJECT 137/141 494\n",
            "SUBJECT 138/141 495\n",
            "SUBJECT 139/141 496\n",
            "SUBJECT 140/141 497\n",
            "SUBJECT 141/141 498\n",
            "Fits Complete.\n",
            "    sid  time        ent       freq       sim      phon  switch\n",
            "0    51     0        cat  11.476894  0.000000  0.000000       0\n",
            "1    51     1        dog  12.375790  0.202481  0.000100       0\n",
            "2    51     2      mouse  11.151769  0.082490  0.000100       0\n",
            "3    51     3        rat  10.045291  0.046041  0.000100       1\n",
            "4    51     4    giraffe   8.552946  0.158130  0.250000       0\n",
            "..  ...   ...        ...        ...       ...       ...     ...\n",
            "20  498  2236    raccoon   8.116716  0.078891  0.000100       1\n",
            "21  498  2237       deer  10.546578  0.343408  0.000100       0\n",
            "22  498  2238  tarantula   7.076654  0.151188  0.111111       1\n",
            "23  498  2239   squirrel   8.850661  0.157418  0.111111       0\n",
            "24  498  2240    hamster   7.643962  0.105035  0.000100       0\n",
            "\n",
            "[4831 rows x 7 columns]\n",
            "   subject  ...  errors_pswitchonlydynamicjack_random\n",
            "0       51  ...                            211.658026\n",
            "1      575  ...                            120.947444\n",
            "2      576  ...                            302.368609\n",
            "3      577  ...                            284.226492\n",
            "4      578  ...                            229.800143\n",
            "\n",
            "[5 rows x 44 columns]\n",
            "Results saved to '/content/drive/My Drive/IU-Abhilasha-Mike/Fluency/sem-phon/fluency_cogsci2022/'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AAO0grZLPiSF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}