{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fluency-cogsci2022.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMWCDL81RrEeLXMilMOV0YZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhilasha-kumar/fluency-cogsci2022/blob/main/fluency_cogsci2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phonological Intrusions in Semantic Memory Retrieval"
      ],
      "metadata": {
        "id": "YemaL37bd2-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing drive, GPU, and packages"
      ],
      "metadata": {
        "id": "MwyFlkqfflFH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggNcnk34d2O4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "iYlJS26seDTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import heapq\n",
        "import itertools\n",
        "import scipy.spatial.distance\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from numpy.random import randint\n",
        "from scipy.special import softmax\n",
        "from sklearn.preprocessing import MinMaxScaler, normalize\n",
        "from numpy.linalg import matrix_power\n",
        "from functools import lru_cache\n",
        "import glob\n",
        "from scipy.special import expit\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from functools import lru_cache\n",
        "from itertools import product as iterprod\n",
        "import itertools\n",
        "from nltk.metrics import *\n",
        "\n"
      ],
      "metadata": {
        "id": "7f1RA_0MeKWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phoneme Function"
      ],
      "metadata": {
        "id": "PxeaTfrGeNla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# algo to obtain phonemes for any given strng\n",
        "# obtained from: https://stackoverflow.com/questions/33666557/get-phonemes-from-any-word-in-python-nltk-or-other-modules\n",
        "try:\n",
        "    arpabet = nltk.corpus.cmudict.dict()\n",
        "except LookupError:\n",
        "    nltk.download('cmudict')\n",
        "    arpabet = nltk.corpus.cmudict.dict()\n",
        "\n",
        "@lru_cache()\n",
        "def wordbreak(s):\n",
        "    s = s.lower()\n",
        "    if s in arpabet:\n",
        "        return arpabet[s]\n",
        "    middle = len(s)/2\n",
        "    partition = sorted(list(range(len(s))), key=lambda x: (x-middle)**2-x)\n",
        "    for i in partition:\n",
        "        pre, suf = (s[:i], s[i:])\n",
        "        if pre in arpabet and wordbreak(suf) is not None:\n",
        "            return [x+y for x,y in iterprod(arpabet[pre], wordbreak(suf))]\n",
        "    return None\n",
        "\n",
        "def normalized_sim(w1, w2):\n",
        "  return 1-edit_distance(w1,w2)/(max(len(w1), len(w2)))\n"
      ],
      "metadata": {
        "id": "Yzh1x4VTeOkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## example"
      ],
      "metadata": {
        "id": "5ncmloxIecIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = \"birds\"\n",
        "w2 = \"pigs\"\n",
        "print(\"wordbreak(w1)[0]:\",wordbreak(w1)[0])\n",
        "print(\"wordbreak(w2)[0]:\",wordbreak(w2)[0])\n",
        "\n",
        "print(\"orig phon:\", edit_distance(wordbreak(w1)[0],wordbreak(w2)[0]))\n",
        "print(\"orig orth:\", edit_distance(w1, w2))\n",
        "\n",
        "print(\"norm orth:\", normalized_sim(w1, w2))\n",
        "print(\"norm phon:\", normalized_sim(wordbreak(w1)[0],wordbreak(w2)[0]))"
      ],
      "metadata": {
        "id": "WZM8Ds6neRVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reading data"
      ],
      "metadata": {
        "id": "VhJ2xFIYegNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parentfolder = \"/content/drive/My Drive/IU-Abhilasha-Mike/Fluency/sem-phon/verbal_fluency/cochlear_alldomains\"\n",
        "with tf.device('/device:GPU:0'):\n",
        "  semantic_files = glob.glob(parentfolder + '/*.xlsx')\n",
        "print(f\"This folder has {len(semantic_files)} files\")"
      ],
      "metadata": {
        "id": "faY0v_dpehPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reading embeddings"
      ],
      "metadata": {
        "id": "U_amfhYwekDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import glove embeddings\n",
        "parentfolder = \"/content/drive/My Drive/IU-Abhilasha-Mike/Fluency/sem-phon/verbal_fluency/cochlear\"\n",
        "with tf.device('/device:GPU:0'):\n",
        "  # glove = pd.read_csv(parentfolder +\"/fluency_glove.csv\").transpose().values\n",
        "  # vocab = pd.read_csv(parentfolder +\"/fluency_vocab.csv\")\n",
        "  glove = pd.read_csv(parentfolder +\"/glove_parents.csv\", encoding=\"unicode-escape\").transpose().values\n",
        "  vocab = pd.DataFrame(pd.read_csv(parentfolder +\"/glove_parents.csv\", encoding=\"unicode-escape\").columns, columns=[\"vocab_word\"])\n",
        "  print(f\"embeddings are shaped:\", glove.shape)\n",
        "  print(f\"vocab is {len(vocab)} words\")"
      ],
      "metadata": {
        "id": "f3aFPCCZelEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# obtaining phonemic & semantic similarity"
      ],
      "metadata": {
        "id": "ILHNvQUMemLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## now we loop through each txt file\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "phon_list = []\n",
        "\n",
        "# read in the data for the category as a pandas dataframe\n",
        "category_file = pd.read_csv(semantic_files)\n",
        "import re\n",
        "for index, row in category_file.iterrows():  \n",
        "  word = str(row[\"value\"])\n",
        "  mod_word = re.sub('[^a-zA-Z]+', '', word)\n",
        "  if(len(mod_word)>0):\n",
        "    phonemes = wordbreak(mod_word)[0]\n",
        "    phon_list.append(phonemes)\n",
        "  else:\n",
        "    phon_list.append(\"wordnotfound\")\n",
        "\n",
        "category_file[\"phonemes\"] = phon_list\n",
        "category_file[\"response_number\"] = category_file.groupby(['participantID', 'cue']).cumcount()+1\n",
        "\n",
        "# exclude rows that do not have a valid phoneme\n",
        "category_file = category_file[category_file.phonemes != \"wordnotfound\"]\n",
        "category_file = category_file.reset_index(drop= True)\n",
        "\n",
        "#now we compute the levenshtein edit distance as a measure of orthographic/phonemic similarity \n",
        "\n",
        "phon_similarity = []\n",
        "orth_similarity = []\n",
        "glove_similarity = []\n",
        "\n",
        "norm_phon = []\n",
        "norm_orth = []\n",
        "\n",
        "for index, row in category_file.iterrows():\n",
        "  current_word = re.sub('[^a-zA-Z]+', '', str(row[\"value\"]))\n",
        "  current_phoneme = row[\"phonemes\"]\n",
        "  current_word = \"FALSE\" if current_word in [\"False\", \"false\"] else current_word\n",
        "  if row[\"response_number\"] == 1:\n",
        "      sem_val = -999\n",
        "      phon_val = -999\n",
        "      orth_val = -999\n",
        "      norm_phon_val = -999\n",
        "      norm_orth_val = -999\n",
        "  else:\n",
        "    previous_word = re.sub('[^a-zA-Z]+', '', str(category_file.value[index-1]))\n",
        "    previous_word = \"FALSE\" if previous_word in [\"False\", \"false\"] else previous_word\n",
        "    previous_phoneme = category_file.phonemes[index-1]\n",
        "\n",
        "    #calculate orthographic similarity as Levenshtein (edit) distance\n",
        "    orth_val = edit_distance(previous_word, current_word)\n",
        "    norm_orth_val = normalized_sim(previous_word, current_word)\n",
        "    \n",
        "    # can also get edit distance for the phonemes themselves (as in Siew et al. Hoosier network)\n",
        "\n",
        "    phon_val = edit_distance(previous_phoneme, current_phoneme)\n",
        "    norm_phon_val = normalized_sim(previous_phoneme, current_phoneme)\n",
        "    \n",
        "    # extract word embedding for current word\n",
        "    if current_word in list(vocab[\"vocab_word\"]):\n",
        "      current_word_index = list(vocab[\"vocab_word\"]).index(current_word)\n",
        "      current_word_vec = glove[current_word_index].reshape((1,glove.shape[1]))\n",
        "      # extract word embedding for current word\n",
        "      if previous_word in list(vocab[\"vocab_word\"]):\n",
        "        previous_word_index = list(vocab[\"vocab_word\"]).index(previous_word)\n",
        "        previous_word_vec = glove[previous_word_index].reshape((1,glove.shape[1]))\n",
        "        sem_val = float((1 - scipy.spatial.distance.cdist(previous_word_vec, current_word_vec, 'cosine'))[0])      \n",
        "        #print(f\"for {current_word} and {previous_word} similarity is {sem_val}\")\n",
        "      else:\n",
        "        sem_val = \"NA\"\n",
        "    else:\n",
        "      sem_val = \"NA\"\n",
        "\n",
        "  phon_similarity.append(phon_val)\n",
        "  orth_similarity.append(orth_val)\n",
        "  norm_phon.append(norm_phon_val)\n",
        "  norm_orth.append(norm_orth_val)\n",
        "  glove_similarity.append(sem_val)\n",
        "  \n",
        "category_file[\"phon_similarity\"] = phon_similarity\n",
        "category_file[\"orth_similarity\"] = orth_similarity\n",
        "category_file[\"norm_phon\"] = norm_phon\n",
        "category_file[\"norm_orth\"] = norm_orth\n",
        "category_file[\"glove_childes\"] = glove_similarity "
      ],
      "metadata": {
        "id": "WqZLn2_Beljf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# computational search models"
      ],
      "metadata": {
        "id": "cKvNxY3we7_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we only consider the \"animals\" domain for the computational models. we start with a predefined list of X animals for which we create a semantic and phonological similarity matrix and obtain word frequency estimates."
      ],
      "metadata": {
        "id": "XRE_8HRugwqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create semantic similarity matrix"
      ],
      "metadata": {
        "id": "-rOFw544fGad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## create similarity matrix and similarity labels file from whichever corpus you're using\n",
        "\n",
        "def create_sim_matrix(vectorpath, corpus_dir):\n",
        "  #glove = pd.read_csv(vectorpath, encoding=\"unicode-escape\").transpose().values\n",
        "  vocab = pd.DataFrame(pd.read_csv(vectorpath, encoding=\"unicode-escape\").columns, columns=[\"vocab_word\"])\n",
        "  ## the vocab consists of ALL possible words in corpus, but we need only the \"animals\" subset here\n",
        "  ## could use the similarity_labels file from psyrev to constrain?\n",
        "  simlabels = pd.read_csv(corpus_dir+'similaritylabels.csv').values.reshape(-1,).tolist()\n",
        "  print(\"simlabels:\", simlabels)\n",
        "\n",
        "  animals_index = [list(vocab.vocab_word).index(lab) if lab in list(vocab.vocab_word) else -999 for lab in simlabels ]\n",
        "  print(\"animals_index:\", len(animals_index))\n",
        "  animals_index = list(filter((-999).__ne__, animals_index))\n",
        "  print(\"animals_index:\", len(animals_index))\n",
        "  print(\"animals_index = \", animals_index)\n",
        "  \n",
        "  ## now we restrict our vocab and embeddings to ONLY these animals\n",
        "  #glove_small = glove[animals_index, :]\n",
        "  #print(f\"embeddings are shaped:\", glove_small.shape)\n",
        "  vocab_small = vocab.iloc[animals_index]\n",
        "  print(f\"vocab is now:\", list(vocab_small.vocab_word))\n",
        "  N = len(vocab_small)\n",
        "  print(f\"vocab is {N} words\")\n",
        "\n",
        "  # create semantic similarity matrix\n",
        "  # matrix = 1-scipy.spatial.distance.cdist(glove_small, glove_small, 'cosine').reshape(-1)\n",
        "  # matrix = matrix.reshape((N,N))\n",
        "  # print(\"sim matrix has been created:\", matrix.shape)\n",
        "\n",
        "  # w1_index = list(vocab_small.vocab_word).index(\"dolphin\")\n",
        "  # w2_index = list(vocab_small.vocab_word).index(\"kitten\")\n",
        "  # w3_index = list(vocab_small.vocab_word).index(\"whale\")\n",
        "\n",
        "  # print(\"dolphin-kitten:\", matrix[w1_index, w2_index])\n",
        "  # print(\"dolphin-whale:\", matrix[w1_index, w3_index])\n",
        "\n",
        "  # pd.DataFrame(matrix).to_csv(corpus_dir + 'corpus_sim_matrix.csv', index=False, header=False)\n",
        "  # vocab_small.to_csv(corpus_dir + 'corpus_sim_labels.csv', index=False, header=False)\n",
        "\n",
        "\n",
        "vectorpath = \"/content/drive/My Drive/IU-Abhilasha-Mike/Fluency/sem-phon/verbal_fluency/cochlear/glove_parents.csv\"\n",
        "\n",
        "create_sim_matrix(vectorpath, corpus_dir)\n"
      ],
      "metadata": {
        "id": "xvFN6weVe-vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create phonological similarity matrix"
      ],
      "metadata": {
        "id": "v5HTHGsjfI_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_phon_matrix(vocab):\n",
        "  # takes in a list of labels and computes the phonological similarity matrix\n",
        "  vocabulary = vocab.copy()\n",
        "  N = len(vocabulary)\n",
        "  print(f\"vocab is {N} words\")\n",
        "  # replace all underscores (_) with space (\" \") to match with glove vectors/vocab\n",
        "  vocabulary = [re.sub('[^a-zA-Z]+', '', str(v)) for v in vocabulary]\n",
        "  print(f\"vocab now looks like:\", vocabulary[:5])\n",
        "  # create phonemic similarity matrix for the small vocab\n",
        "  pmatrix = np.array([normalized_sim(wordbreak(w1)[0], wordbreak(w2)[0]) for w1 in vocabulary for w2 in vocabulary]).reshape((N,N))\n",
        "  print(\"pmatrix has been created:\", pmatrix.shape)\n",
        "  print(pmatrix)\n",
        "  pd.DataFrame(pmatrix).to_csv(corpus_dir + 'simlabels_phon_matrix.csv', index=False, header=False)  \n",
        "  print(\"phon matrix csv created!\")\n",
        "\n",
        "simlabels = pd.read_csv(corpus_dir+'similaritylabels.csv', header=None).values.reshape(-1,).tolist()\n",
        "print(f\"simlabels is {len(simlabels)} items:\", simlabels[:5])\n",
        "create_phon_matrix(simlabels)\n"
      ],
      "metadata": {
        "id": "b2HouuHsfK4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## define foraging models"
      ],
      "metadata": {
        "id": "e_NPzt9nfQOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "results_dir = '/content/drive/My Drive/IU-Abhilasha-Mike/Fluency/optimal-foraging-model/data/results/'\n",
        "corpus_dir = '/content/drive/My Drive/IU-Abhilasha-Mike/Fluency/optimal-foraging-model/data/corpus/'\n",
        "\n",
        "def modelFits(path, delimiter):\n",
        "\n",
        "    ### LOAD REQUIRED PACKAGES ###\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import re\n",
        "\n",
        "    ### LOAD BEHAVIORAL DATA ###\n",
        "    df = pd.read_csv(path, header=None, names=['SID', 'entry'], delimiter=delimiter)\n",
        "\n",
        "    #correct behavioral fits\n",
        "    df = forage.prepareData(df)\n",
        "\n",
        "    ### LOAD SEMANTIC SIMILARITY MATRIX ###\n",
        "\n",
        "    # (aka 'local cues', here we use cosines from word2vec)\n",
        "\n",
        "    # Similarity labels\n",
        "    simlab = []\n",
        "    ofile = open(corpus_dir + 'similaritylabels.csv','r')#TODO:\n",
        "    for line in ofile:\n",
        "        labs = line.split()\n",
        "        for lab in labs:\n",
        "            simlab.append(lab)\n",
        "    ofile.close()\n",
        "\n",
        "    # Similarity values\n",
        "    simval = np.zeros((len(simlab), len(simlab)))\n",
        "    ofile = open(corpus_dir + 'similaritymatrix.csv', 'r')#TODO:\n",
        "    j=0\n",
        "    for line in ofile:\n",
        "        line = re.sub(',\\n', '', line)\n",
        "        sims = line.split(',')\n",
        "        i=0\n",
        "        for sim in sims:\n",
        "            simval[i,j] = sim\n",
        "            i+=1\n",
        "        j+=1\n",
        "    ofile.close()\n",
        "\n",
        "    # Make sure similarity values are non-zero\n",
        "    for i in range(0,len(simval)):\n",
        "        for j in range(0,len(simval)):\n",
        "            if simval[i,j] <= 0:\n",
        "                simval[i,j] = 0.0001\n",
        "\n",
        "    ## PHONEMIC SIMILARTY VALUES ##\n",
        "    phonval = np.zeros((len(simlab), len(simlab)))\n",
        "    ofile = open(corpus_dir + 'simlabels_phon_matrix.csv', 'r')#TODO:\n",
        "    j=0\n",
        "    for line in ofile:\n",
        "        line = re.sub(',\\n', '', line)\n",
        "        sims = line.split(',')\n",
        "        i=0\n",
        "        for sim in sims:\n",
        "            phonval[i,j] = sim\n",
        "            i+=1\n",
        "        j+=1\n",
        "    ofile.close()\n",
        "\n",
        "    # Make sure phonemic values are non-zero\n",
        "    for i in range(0,len(phonval)):\n",
        "        for j in range(0,len(phonval)):\n",
        "            if phonval[i,j] <= 0:\n",
        "                phonval[i,j] = 0.0001\n",
        "\n",
        "    ### LOAD FREQUENCY LIST ###\n",
        "    # (aka 'global cue', using NOW corpus from http://corpus.byu.edu/now/, 4.2 billion words and growing daily)\n",
        "\n",
        "    freqlab = []\n",
        "    freqval = []\n",
        "    ofile = open(corpus_dir + 'frequencies.csv', 'r') #TODO:\n",
        "    for line in ofile:\n",
        "        line = re.sub('\\n', '', line)\n",
        "        freqs=line.split(',')\n",
        "        freqlab.append(freqs[0])\n",
        "        ## append log of frequency if using psyrev\n",
        "        freqval.append(np.log(float(freqs[1])))\n",
        "        #freqval.append(float(freqs[1]))\n",
        "    ofile.close()\n",
        "    freqval=np.array(freqval)\n",
        "\n",
        "    sidlist = list(set(df['SID']))\n",
        "    full_entdf = pd.DataFrame()\n",
        "    full_fitlist = []\n",
        "    ct = 0\n",
        "\n",
        "    ## COMPUTE CONSECUTIVE SIMILARITY AND FREQUENCY AT SUBJECT LEVEL ##\n",
        "\n",
        "    for sid in sidlist:\n",
        "        ct+=1\n",
        "        print( \"SUBJECT \" + str(ct) + '/' + str(len(sidlist)) + \" \" + str(sid))\n",
        "\n",
        "        # My general initializations\n",
        "        myfitlist = []\n",
        "        myentries = np.array(df[df['SID']==sid]['entry'])\n",
        "        #print(\"myentries:\", myentries)\n",
        "        myenttimes = np.array(df[df['SID']==sid].index)\n",
        "        ##print(\"myenttimes:\", myenttimes)\n",
        "        myused = []\n",
        "        mytime = []\n",
        "\n",
        "        # For both frequency and similarity metrics:\n",
        "            # LIST: Metrics corresponding with my observed entries\n",
        "            # CURRENT: Full metric values, with observed entries becoming 0\n",
        "            # HISTORY: State of full metric values (ie, \"current\" during each entry)\n",
        "\n",
        "        # My frequency initializations\n",
        "        # freq current contains frequencies of ALL the words in corpus\n",
        "        freq_current = np.array(freqval)\n",
        "        #print(\"freq_current.shape:\",freq_current.shape)\n",
        "        freq_list = []\n",
        "        freq_history = []\n",
        "\n",
        "        # My similarity initializations\n",
        "        sim_current = simval.copy()\n",
        "        # sim_current contains the full NxN similarity matrix\n",
        "        #print(\"sim_current shape:\",sim_current.shape)\n",
        "        sim_list = []\n",
        "        sim_history = []\n",
        "\n",
        "        phon_current = phonval.copy()\n",
        "        phon_list = []\n",
        "        phon_history = []\n",
        "\n",
        "        for i in range(0,len(myentries)):\n",
        "            word = myentries[i]\n",
        "            #if word not in myused: # use this to calculate number of correct responses w/out repeats\n",
        "            if True:   # use this line instead of former to include repeated words along w/line 110,119 comment out\n",
        "\n",
        "                # Frequency: Get frequency and update relevant lists\n",
        "                freq_list.append( float(freq_current[freqlab.index(word)]) )\n",
        "                freq_history.append( np.array(freq_current) )\n",
        "                #freq_current[freqlab.index(word)] = 0.00000001\n",
        "\n",
        "                # Get similarity between this word and preceding word\n",
        "                if i > 0:         \n",
        "                    sim_list.append( float(sim_current[simlab.index(myentries[i-1]), simlab.index(word)]) )\n",
        "                    sim_history.append( np.array(sim_current[simlab.index(myentries[i-1]),:]) )\n",
        "\n",
        "                    phon_list.append( float(phon_current[simlab.index(myentries[i-1]), simlab.index(word)]) )\n",
        "                    phon_history.append( np.array(phon_current[simlab.index(myentries[i-1]),:]) )\n",
        "                else:\n",
        "                    sim_list.append(0)\n",
        "                    sim_history.append( np.array(sim_current[simlab.index(word),:]) )\n",
        "                #sim_current[:,simlab.index(word)] = 0.00000001\n",
        "                    phon_list.append(0)\n",
        "                    phon_history.append( np.array(phon_current[simlab.index(word),:]) )\n",
        "\n",
        "                # Update lists\n",
        "                myused.append(word)\n",
        "                mytime.append(myenttimes[i])\n",
        "\n",
        "        # Calculate category switches, based on similarity-drop\n",
        "        myswitch = np.zeros(len(myused)).astype(int)\n",
        "        for i in range(1,len(myused)-1):\n",
        "            if (sim_list[i+1] > sim_list[i]) and (sim_list[i-1] > sim_list[i]):\n",
        "                myswitch[i] = 1\n",
        "\n",
        "        # Save my entries with corresponding metrics\n",
        "        mydf = pd.DataFrame({'sid':[sid]*len(myused) , 'ent':myused, 'freq':freq_list, 'sim':sim_list, 'phon': phon_list,\n",
        "                             'switch':myswitch, 'time':mytime},\n",
        "                            columns=['sid','time','ent','freq','sim', 'phon', 'switch'])\n",
        "        full_entdf = full_entdf.append(mydf)\n",
        "        # Get parameter fits for the different models\n",
        "        myfitlist.append(sid)\n",
        "        myfitlist.append(len(myused))\n",
        "        ## obtaining the optimal/random fits for the static and dynamic model by calling the getFits function\n",
        "        myfitlist.extend( forage.getfits(freq_list, freq_history, sim_list, sim_history, phon_list, phon_history) )\n",
        "        full_fitlist.append(myfitlist)\n",
        "\n",
        "    print(\"Fits Complete.\")\n",
        "\n",
        "    # create results directory if it doesn't exist yet\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "\n",
        "    # # Output data entries with corresponding metrics for visualization in R\n",
        "    print(full_entdf)\n",
        "    full_entdf = full_entdf.reset_index(drop=True)\n",
        "    full_entdf.to_csv(results_dir  + 'nancy-fullmetrics.csv', index=False, header=True)\n",
        "\n",
        "    # # Output parameter & model fits\n",
        "    full_fitlist = pd.DataFrame(full_fitlist)\n",
        "    full_fitlist.columns = ['subject', 'number_of_items', \n",
        "                            'beta_static_frequency', 'beta_static_semantic', 'errors_static_optimal', 'errors_static_random',\n",
        "                            'beta_dynamic_frequency', 'beta_dynamic_semantic', 'errors_dynamic_optimal', 'errors_dynamic_random',\n",
        "                            'beta_dynamicjack_frequency', 'beta_dynamicjack_semantic', 'errors_dynamicjack_optimal', 'errors_dynamicjack_random',\n",
        "\n",
        "                            'beta_plocalstatic_frequency', 'beta_plocalstatic_semantic', 'beta_plocalstatic_phonemic','errors_plocalstatic_optimal', 'errors_plocalstatic_random',\n",
        "\n",
        "                            'beta_plocaldynamicorig_frequency', 'beta_plocaldynamicorig_semantic', 'beta_plocaldynamicorig_phonemic','errors_plocaldynamicorig_optimal', 'errors_plocaldynamicorig_random',\n",
        "                            'beta_pglobaldynamicorig_frequency', 'beta_pglobaldynamicorig_semantic', 'beta_pglobaldynamicorig_phonemic','errors_pglobaldynamicorig_optimal', 'errors_pglobaldynamicorig_random',\n",
        "\n",
        "                            'beta_plocaldynamicjack_frequency', 'beta_plocaldynamicjack_semantic', 'beta_plocaldynamicjack_phonemic','errors_plocaldynamicjack_optimal', 'errors_plocaldynamicjack_random',\n",
        "                            'beta_pglobaldynamicjack_frequency', 'beta_pglobaldynamicjack_semantic', 'beta_pglobaldynamicjack_phonemic','errors_pglobaldynamicjack_optimal', 'errors_pglobaldynamicjack_random',\n",
        "\n",
        "                            'beta_pswitchonlydynamicjack_frequency', 'beta_pswitchonlydynamicjack_semantic', 'beta_pswitchonlydynamicjack_phonemic','errors_pswitchonlydynamicjack_optimal', 'errors_pswitchonlydynamicjack_random'\n",
        "                            ]\n",
        "\n",
        "    #print(\"full_fitlist:\",full_fitlist)\n",
        "    full_fitlist.to_csv(results_dir  + 'nancy-fullfits.csv', index=False, header=True)\n",
        "\n",
        "    print(full_fitlist.head())\n",
        "    print(\"Results saved to '\" + results_dir + \"'.\")\n",
        "\n",
        "class forage:\n",
        "\n",
        "    def prepareData(data):\n",
        "        import pandas as pd\n",
        "        import re\n",
        "        # load similarity labels\n",
        "        simlab = []\n",
        "        ofile = open(corpus_dir + 'similaritylabels.csv','r')\n",
        "        for line in ofile:\n",
        "            labs = line.split()\n",
        "            for lab in labs:\n",
        "                simlab.append(lab)\n",
        "        ofile.close()\n",
        "\n",
        "        ### LOAD CORRECTIONS ###\n",
        "        # This is a look-up list that maps incorrect words onto accepted words that are in the database\n",
        "        corrections = pd.read_csv(corpus_dir + 'corrections.txt', header=None, delimiter='\\t')\n",
        "        corrections = corrections.set_index(corrections[0].values)\n",
        "        corrections.columns = ['_from','_to']\n",
        "\n",
        "        elist = data['entry'].values\n",
        "        newlist = []\n",
        "        notfound = []\n",
        "\n",
        "        # Use look-up table to check and correct observed entries\n",
        "        for ent in elist:\n",
        "            ent = re.sub(r'\\W+', '', ent) # Alphanumericize it\n",
        "            if ent in simlab:\n",
        "                # If this entry is appropriate, keep it\n",
        "                newlist.append(ent)\n",
        "            elif ent[0:len(ent)-1] in simlab:\n",
        "                # If this entry is plural, correct to the singular verion\n",
        "                print(f\"found the entry {ent[0:len(ent)-1]} in simlab\")\n",
        "                newlist.append(ent[0:len(ent)-1])\n",
        "            elif ent in corrections._from:\n",
        "                # If this entry is correctable, correct it\n",
        "                newlist.append(corrections.loc[ent]._to)\n",
        "            else:\n",
        "                # If this entry is not found in either list, mark for removal and warn user.\n",
        "                newlist.append('NA')\n",
        "                notfound.append(ent)\n",
        "\n",
        "        # Remove the rows with inappropriate entries\n",
        "        data.entry = newlist\n",
        "        data = data[data.entry!='NA']\n",
        "\n",
        "        # Warn the user of removed entries\n",
        "        if len(notfound) > 0:\n",
        "            print('The following items were not found in the database, and were removed: [' +\n",
        "                  str(len(notfound)) + ' entries removed] \\n')\n",
        "            print(sorted(set(notfound)))\n",
        "        else:\n",
        "            print('All items OK.')\n",
        "        return data[data.entry!='NA']\n",
        "        # TODO: return statement might not be necessary...\n",
        "\n",
        "    def model_static(beta, freql, freqh, siml, simh):\n",
        "        ## beta contains the optimization parameters for frequency (beta[0]) and semantic similarity (beta[1])\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0: # if first item then its probability is based on just frequency\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else: # if not first item then its probability is based on its similarity to prev item AND frequency\n",
        "            # P of item based on frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat) # negative Log likelihood of this item: this will be minimized eventually\n",
        "        return ct\n",
        "\n",
        "\n",
        "    def model_dynamic_original(beta, freql, freqh, siml, simh):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based again on frequency\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "        \n",
        "    def model_dynamic_jack(beta, freql, freqh, siml, simh):\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based again on frequency\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "      \n",
        "    def model_static_plocal(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "        ## beta contains the optimization parameters for frequency (beta[0]) and semantic similarity (beta[1])\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0: # if first item then its probability is based on just frequency\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else: # if not first item then its probability is based on its similarity to prev item AND frequency AND phonemic similarity\n",
        "            # P of item based on frequency and similarity and phonology\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) * pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2])* pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat) # negative Log likelihood of this item: this will be minimized eventually\n",
        "        return ct\n",
        "\n",
        "    def model_dynamic_plocal_jack(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based on frequency \n",
        "                numrat = pow(freql[k],beta[0]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(phonl[k],beta[2])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(phonh[k],beta[2])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "\n",
        "    def model_dynamic_pglobal_jack(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based on a combination of frequency and phonemic similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2]) )\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(phonl[k],beta[2])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(phonh[k],beta[2])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "\n",
        "    def model_dynamic_pswitchonly_jack(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based on a combination of frequency and phonemic similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2]) )\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(freql[k],beta[0])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(freqh[k],beta[0])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "    \n",
        "    def model_dynamic_plocal_original(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based on frequency \n",
        "                numrat = pow(freql[k],beta[0]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(phonl[k],beta[2])*pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(phonh[k],beta[2])*pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "\n",
        "    def model_dynamic_pglobal_original(beta, freql, freqh, siml, simh, phonl, phonh):\n",
        "      ## here we use phonology as a \"local\" cue with semantics\n",
        "        import numpy as np\n",
        "        ct = 0\n",
        "        for k in range(0, len(freql)):\n",
        "            if k == 0 :\n",
        "            # P of item based on frequency alone (freq of this item / freq of all items)\n",
        "                numrat = pow(freql[k],beta[0])\n",
        "                denrat = sum(pow(freqh[k],beta[0]))\n",
        "            elif k > 0 and k < (len(freql)-1) and siml[k+1] > siml[k] and siml[k-1] > siml[k]: ## \"dip\" based on sim-drop\n",
        "            # If similarity dips, P of item is based on a combination of frequency and phonemic similarity\n",
        "                numrat = pow(freql[k],beta[0]) * pow(phonl[k],beta[2]) \n",
        "                denrat = sum(pow(freqh[k],beta[0]) * pow(phonh[k],beta[2]) )\n",
        "            else:\n",
        "            # P of item based on combined frequency and similarity\n",
        "                numrat = pow(siml[k],beta[1])\n",
        "                denrat = sum(pow(simh[k],beta[1]))\n",
        "            ct += -np.log(numrat/denrat)\n",
        "        return ct\n",
        "\n",
        "    def getfits( freq_l, freq_h, sim_l, sim_h, phon_l, phon_h ):\n",
        "        import numpy as np\n",
        "        from scipy.optimize import fmin\n",
        "    #fmin: Uses a Nelder-Mead simplex algorithm to find the minimum of function of variables.\n",
        "        r1 = np.random.rand()\n",
        "        r2 = np.random.rand()\n",
        "        r3 = np.random.rand()\n",
        "\n",
        "    # STATIC MODEL (no dynamic switching, just focusing on two cues with some weights)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_static, [r1, r2], args=(freq_l, freq_h, sim_l, sim_h), ftol = 0.001, disp=False)\n",
        "        beta_static_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_static_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_static = forage.model_static([beta_static_freq, beta_static_semantic], freq_l, freq_h, sim_l, sim_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_static = forage.model_static([0, 0], freq_l, freq_h, sim_l, sim_h)\n",
        "\n",
        "    # ORIGINAL DYNAMIC MODEL (switches dynamically between cues)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_original, [r1,r2], args=(freq_l, freq_h, sim_l, sim_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_dynamic = forage.model_dynamic_original([beta_dynamic_freq, beta_dynamic_semantic], freq_l, freq_h, sim_l, sim_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_dynamic = forage.model_dynamic_original([0,0], freq_l, freq_h, sim_l, sim_h)\n",
        "\n",
        "    # JACK DYNAMIC MODEL (switches dynamically between cues)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_jack, [r1,r2], args=(freq_l, freq_h, sim_l, sim_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamicjack_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamicjack_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_dynamicjack = forage.model_dynamic_jack([beta_dynamic_freq, beta_dynamic_semantic], freq_l, freq_h, sim_l, sim_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_dynamicjack = forage.model_dynamic_jack([0,0], freq_l, freq_h, sim_l, sim_h)\n",
        "  \n",
        "\n",
        "    # LOCAL PHONEMIC CUE, STATIC MODEL (no dynamic switching, just focusing on two cues with some weights)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_static_plocal, [r1, r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_static_plocal_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_static_plocal_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_static_plocal_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_plocalstatic = forage.model_static_plocal([beta_static_plocal_freq, beta_static_plocal_semantic, beta_static_plocal_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_plocalstatic = forage.model_static_plocal([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "    \n",
        "    # ORIGINAL DYNAMIC PHON LOCAL MODEL (switches dynamically between cues, phonology,semantic, freq is a LOCAL cue)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_plocal_original, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_plocalorig_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_plocalorig_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_plocalorig_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_plocaldynamicorig = forage.model_dynamic_plocal_original([beta_dynamic_plocalorig_freq, beta_dynamic_plocalorig_semantic, beta_dynamic_plocalorig_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_plocaldynamicorig = forage.model_dynamic_plocal_original([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "     # ORIGINAL DYNAMIC PHON GLOBAL MODEL (switches dynamically between cues, phonology is a GLOBAL cue with frequency)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_pglobal_original, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_pglobalorig_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_pglobalorig_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_pglobalorig_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_pglobaldynamicorig = forage.model_dynamic_pglobal_original([beta_dynamic_pglobalorig_freq, beta_dynamic_pglobalorig_semantic, beta_dynamic_pglobalorig_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_pglobaldynamicorig = forage.model_dynamic_pglobal_original([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "      # JACK DYNAMIC PHON LOCAL MODEL (switches dynamically between cues, phonology,semantic, freq is a LOCAL cue)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_plocal_jack, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_plocaljack_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_plocaljack_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_plocaljack_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_plocaldynamicjack = forage.model_dynamic_plocal_jack([beta_dynamic_plocaljack_freq, beta_dynamic_plocaljack_semantic, beta_dynamic_plocaljack_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_plocaldynamicjack = forage.model_dynamic_plocal_jack([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "     # JACK DYNAMIC PHON GLOBAL MODEL (switches dynamically between cues, phonology is a GLOBAL cue with frequency)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_pglobal_jack, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_pglobaljack_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_pglobaljack_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_pglobaljack_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_pglobaldynamicjack = forage.model_dynamic_pglobal_jack([beta_dynamic_pglobaljack_freq, beta_dynamic_pglobaljack_semantic, beta_dynamic_pglobaljack_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_pglobaldynamicjack = forage.model_dynamic_pglobal_jack([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "      \n",
        "    # JACK DYNAMIC PHON SWITCH ONLY MODEL (switches dynamically between cues, phonology is a GLOBAL cue with frequency)\n",
        "\n",
        "        # 1.) Optimize model parameters\n",
        "        v = fmin(forage.model_dynamic_pswitchonly_jack, [r1,r2, r3], args=(freq_l, freq_h, sim_l, sim_h, phon_l, phon_h), ftol = 0.001, disp=False)\n",
        "        beta_dynamic_pswitchonlyjack_freq = float(v[0]) # Optimized weight for frequency cue\n",
        "        beta_dynamic_pswitchonlyjack_semantic = float(v[1]) # Optimized weight for similarity cue\n",
        "        beta_dynamic_pswitchonlyjack_phonemic = float(v[2]) # Optimized weight for phonemic cue\n",
        "\n",
        "        # 2.) Determine model fit (errors) at optimal parameters: will return total -LL\n",
        "        optimal_fit_pswitchonlydynamicjack = forage.model_dynamic_pswitchonly_jack([beta_dynamic_pswitchonlyjack_freq, beta_dynamic_pswitchonlyjack_semantic, beta_dynamic_pswitchonlyjack_phonemic], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "\n",
        "        # 3.) For comparison, determine model fit (errors) without parameter fits\n",
        "        random_fit_pswitchonlydynamicjack = forage.model_dynamic_pswitchonly_jack([0, 0, 0], freq_l, freq_h, sim_l, sim_h, phon_l, phon_h)\n",
        "      \n",
        "      \n",
        "\n",
        "        results = [ beta_static_freq, beta_static_semantic, float(optimal_fit_static), float(random_fit_static),\n",
        "                   beta_dynamic_freq, beta_dynamic_semantic, float(optimal_fit_dynamic), float(random_fit_dynamic),\n",
        "                   beta_dynamicjack_freq, beta_dynamicjack_semantic, float(optimal_fit_dynamicjack), float(random_fit_dynamicjack),\n",
        "\n",
        "                   beta_static_plocal_freq, beta_static_plocal_semantic, beta_static_plocal_phonemic, float(optimal_fit_plocalstatic), float(random_fit_plocalstatic),\n",
        "              \n",
        "                   beta_dynamic_plocalorig_freq, beta_dynamic_plocalorig_semantic, beta_dynamic_plocalorig_phonemic, float(optimal_fit_plocaldynamicorig), float(random_fit_plocaldynamicorig),\n",
        "                   beta_dynamic_pglobalorig_freq, beta_dynamic_pglobalorig_semantic, beta_dynamic_pglobalorig_phonemic, float(optimal_fit_pglobaldynamicorig), float(random_fit_pglobaldynamicorig),\n",
        "\n",
        "                   beta_dynamic_plocaljack_freq, beta_dynamic_plocaljack_semantic, beta_dynamic_plocaljack_phonemic, float(optimal_fit_plocaldynamicjack), float(random_fit_plocaldynamicjack),\n",
        "                   beta_dynamic_pglobaljack_freq, beta_dynamic_pglobaljack_semantic, beta_dynamic_pglobaljack_phonemic, float(optimal_fit_pglobaldynamicjack), float(random_fit_pglobaldynamicjack),\n",
        "\n",
        "                   beta_dynamic_pswitchonlyjack_freq, beta_dynamic_pswitchonlyjack_semantic, beta_dynamic_pswitchonlyjack_phonemic, float(optimal_fit_pswitchonlydynamicjack), float(random_fit_pswitchonlydynamicjack)\n",
        "\n",
        "                   ]\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "lP1sdQE5fR2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run foraging models"
      ],
      "metadata": {
        "id": "1LC8wg1yfVnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datapath = '/content/drive/My Drive/IU-Abhilasha-Mike/Fluency/optimal-foraging-model/data/corpus/nancy-animals.txt'\n",
        "modelFits(datapath, delimiter = \"\\t\")"
      ],
      "metadata": {
        "id": "hV8fHr10fS3Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}